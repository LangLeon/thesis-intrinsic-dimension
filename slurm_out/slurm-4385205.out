Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07677489025577618; val_accuracy: 0.9818869426751592 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.078548801457806; val_accuracy: 0.9806926751592356 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07858233979553174; val_accuracy: 0.9814888535031847 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07736258823305937; val_accuracy: 0.9831807324840764 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07938695414240952; val_accuracy: 0.9827826433121019 

Epoch 16 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08031099451005838; val_accuracy: 0.9825835987261147 

Epoch 17 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.080717610278327; val_accuracy: 0.9826831210191083 

Epoch 18 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08163690265671462; val_accuracy: 0.9823845541401274 

Epoch 19 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08199480778662263; val_accuracy: 0.9823845541401274 

Epoch 20 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08322503237397808; val_accuracy: 0.982484076433121 

Epoch 21 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08348223448369153; val_accuracy: 0.9826831210191083 

Epoch 22 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0841593032429932; val_accuracy: 0.9825835987261147 

Epoch 23 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08484477938929941; val_accuracy: 0.982484076433121 

Epoch 24 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08429699106391068; val_accuracy: 0.9828821656050956 

Epoch 25 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08522729305135217; val_accuracy: 0.9823845541401274 

Epoch 26 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08565623227767884; val_accuracy: 0.9826831210191083 

Epoch 27 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08652286933865516; val_accuracy: 0.9822850318471338 

Epoch 28 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08677719637846491; val_accuracy: 0.9825835987261147 

Epoch 29 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08699275887790758; val_accuracy: 0.9818869426751592 

Epoch 30 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08742470237290025; val_accuracy: 0.9826831210191083 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08795085874427656; val_accuracy: 0.982484076433121 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08794784991984154; val_accuracy: 0.9820859872611465 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08831757781611886; val_accuracy: 0.9823845541401274 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08869697884389549; val_accuracy: 0.982484076433121 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08894005168680173; val_accuracy: 0.9821855095541401 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0892530776275571; val_accuracy: 0.9825835987261147 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08917681545398798; val_accuracy: 0.9826831210191083 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0899441656270984; val_accuracy: 0.9822850318471338 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09018304493207081; val_accuracy: 0.982484076433121 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09018021549104126; val_accuracy: 0.982484076433121 

Epoch 41 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09033867313413863; val_accuracy: 0.9822850318471338 

Epoch 42 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09076094765002561; val_accuracy: 0.9825835987261147 

Epoch 43 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09115717616999984; val_accuracy: 0.9825835987261147 

Epoch 44 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0913574612064726; val_accuracy: 0.982484076433121 

Epoch 45 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09149208074067808; val_accuracy: 0.982484076433121 

Epoch 46 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09154575443856276; val_accuracy: 0.9826831210191083 

Epoch 47 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09181612822565303; val_accuracy: 0.9825835987261147 

Epoch 48 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09196685769470633; val_accuracy: 0.9825835987261147 

Epoch 49 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0919035788459383; val_accuracy: 0.982484076433121 

Epoch 50 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09222150361462003; val_accuracy: 0.9827826433121019 

plots/no_subspace_training/MLP/2020-01-19 00:40:48/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07461831519368348; val_accuracy: 0.9821855095541401 

Epoch 12 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07521994392962972; val_accuracy: 0.9819864649681529 

Epoch 13 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07705178965998304; val_accuracy: 0.9820859872611465 

Epoch 14 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07632544798076532; val_accuracy: 0.9826831210191083 

Epoch 15 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07789958349079083; val_accuracy: 0.9829816878980892 

Epoch 16 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07902450809717938; val_accuracy: 0.9826831210191083 

Epoch 17 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0794206673077717; val_accuracy: 0.9827826433121019 

Epoch 18 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08032064830326731; val_accuracy: 0.9827826433121019 

Epoch 19 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08078753518735528; val_accuracy: 0.9826831210191083 

Epoch 20 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08225789087213528; val_accuracy: 0.9828821656050956 

Epoch 21 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08215699871634222; val_accuracy: 0.9827826433121019 

Epoch 22 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08290398918139706; val_accuracy: 0.9830812101910829 

Epoch 23 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08351926372689046; val_accuracy: 0.9827826433121019 

Epoch 24 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08295831179163259; val_accuracy: 0.9826831210191083 

Epoch 25 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08380042911050425; val_accuracy: 0.982484076433121 

Epoch 26 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08417737849388912; val_accuracy: 0.9825835987261147 

Epoch 27 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08499505166794843; val_accuracy: 0.9823845541401274 

Epoch 28 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08517911565151944; val_accuracy: 0.9827826433121019 

Epoch 29 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08539155187310686; val_accuracy: 0.9823845541401274 

Epoch 30 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08585138780296228; val_accuracy: 0.9830812101910829 

Epoch 31 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08629509704602752; val_accuracy: 0.9827826433121019 

Epoch 32 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08625411531727785; val_accuracy: 0.9825835987261147 

Epoch 33 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08660164409002681; val_accuracy: 0.9827826433121019 

Epoch 34 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0869637677908703; val_accuracy: 0.982484076433121 

Epoch 35 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08724046254139038; val_accuracy: 0.9825835987261147 

Epoch 36 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08754555171557293; val_accuracy: 0.9827826433121019 

Epoch 37 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08743958115862434; val_accuracy: 0.9827826433121019 

Epoch 38 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08813584166442513; val_accuracy: 0.982484076433121 

Epoch 39 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08834617048691792; val_accuracy: 0.982484076433121 

Epoch 40 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08833916826992277; val_accuracy: 0.982484076433121 

Epoch 41 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08847884454165295; val_accuracy: 0.9822850318471338 

Epoch 42 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.088835802878354; val_accuracy: 0.9826831210191083 

Epoch 43 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08909510937371072; val_accuracy: 0.9822850318471338 

Epoch 44 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08926108021550118; val_accuracy: 0.9826831210191083 

Epoch 45 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0893721610639885; val_accuracy: 0.982484076433121 

Epoch 46 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08942583032474397; val_accuracy: 0.9825835987261147 

Epoch 47 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0896853309860275; val_accuracy: 0.9826831210191083 

Epoch 48 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0898616084722197; val_accuracy: 0.9826831210191083 

Epoch 49 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08977638740258612; val_accuracy: 0.9828821656050956 

Epoch 50 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09006978184649139; val_accuracy: 0.9825835987261147 

plots/no_subspace_training/MLP/2020-01-19 00:44:24/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1028801152945324; val_accuracy: 0.9710390127388535 

Epoch 7 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08037806335528186; val_accuracy: 0.9784036624203821 

Epoch 8 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.189130765237626; val_accuracy: 0.955812101910828 

Epoch 9 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07167025548732205; val_accuracy: 0.9819864649681529 

Epoch 10 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07623575492554409; val_accuracy: 0.9811902866242038 

Epoch 11 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07233599431006013; val_accuracy: 0.9830812101910829 

Epoch 12 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07366931822839057; val_accuracy: 0.982484076433121 

Epoch 13 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0752209352602245; val_accuracy: 0.9820859872611465 

Epoch 14 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07479701203050887; val_accuracy: 0.9826831210191083 

Epoch 15 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07601441395510534; val_accuracy: 0.9825835987261147 

Epoch 16 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07694080609614683; val_accuracy: 0.9825835987261147 

Epoch 17 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07724557432589257; val_accuracy: 0.9827826433121019 

Epoch 18 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07788316729911574; val_accuracy: 0.9825835987261147 

Epoch 19 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0782339841507043; val_accuracy: 0.982484076433121 

Epoch 20 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07944823146625689; val_accuracy: 0.9826831210191083 

Epoch 21 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07947611728101779; val_accuracy: 0.9826831210191083 

Epoch 22 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08005930371819787; val_accuracy: 0.9821855095541401 

Epoch 23 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08052426633561492; val_accuracy: 0.982484076433121 

Epoch 24 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08009038716080083; val_accuracy: 0.9822850318471338 

Epoch 25 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08073301551638136; val_accuracy: 0.9820859872611465 

Epoch 26 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08104719800554264; val_accuracy: 0.9823845541401274 

Epoch 27 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08156513270868618; val_accuracy: 0.9823845541401274 

Epoch 28 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.081836271248046; val_accuracy: 0.9823845541401274 

Epoch 29 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08192148607722513; val_accuracy: 0.9822850318471338 

Epoch 30 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08230149610691768; val_accuracy: 0.982484076433121 

Epoch 31 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08272195116254934; val_accuracy: 0.9822850318471338 

Epoch 32 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08262686009050176; val_accuracy: 0.982484076433121 

Epoch 33 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0829342187969548; val_accuracy: 0.982484076433121 

Epoch 34 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0831181289758652; val_accuracy: 0.9821855095541401 

Epoch 35 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08330401998891193; val_accuracy: 0.9825835987261147 

Epoch 36 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0835722463715608; val_accuracy: 0.982484076433121 

Epoch 37 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08348551003416632; val_accuracy: 0.9825835987261147 

Epoch 38 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08390325428858685; val_accuracy: 0.9825835987261147 

Epoch 39 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08402032348190903; val_accuracy: 0.9822850318471338 

Epoch 40 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08405346682022331; val_accuracy: 0.9823845541401274 

Epoch 41 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08410888758434612; val_accuracy: 0.982484076433121 

Epoch 42 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08441503493079713; val_accuracy: 0.9825835987261147 

Epoch 43 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08453749241248058; val_accuracy: 0.982484076433121 

Epoch 44 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08463158083569472; val_accuracy: 0.9825835987261147 

Epoch 45 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08471864903238928; val_accuracy: 0.9823845541401274 

Epoch 46 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08473870872407202; val_accuracy: 0.9826831210191083 

Epoch 47 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08488263101999168; val_accuracy: 0.9825835987261147 

Epoch 48 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08496037664212239; val_accuracy: 0.9825835987261147 

Epoch 49 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08494119569184674; val_accuracy: 0.9825835987261147 

Epoch 50 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08508898512383176; val_accuracy: 0.982484076433121 

plots/no_subspace_training/MLP/2020-01-19 00:47:56/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07677489025577618; val_accuracy: 0.9818869426751592 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.078548801457806; val_accuracy: 0.9806926751592356 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07858233979553174; val_accuracy: 0.9814888535031847 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07736258823305937; val_accuracy: 0.9831807324840764 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07938695414240952; val_accuracy: 0.9827826433121019 

Epoch 16 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07956284505261738; val_accuracy: 0.9828821656050956 

Epoch 17 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.079893122765289; val_accuracy: 0.9826831210191083 

Epoch 18 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08046322977941507; val_accuracy: 0.9825835987261147 

Epoch 19 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08065426411332598; val_accuracy: 0.9826831210191083 

Epoch 20 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0813189454044506; val_accuracy: 0.982484076433121 

Epoch 21 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.081551208570125; val_accuracy: 0.982484076433121 

Epoch 22 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08197673315265376; val_accuracy: 0.982484076433121 

Epoch 23 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08239118043024829; val_accuracy: 0.982484076433121 

Epoch 24 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08207674547555341; val_accuracy: 0.9827826433121019 

Epoch 25 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08268423577782455; val_accuracy: 0.982484076433121 

Epoch 26 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08294336030342776; val_accuracy: 0.982484076433121 

Epoch 27 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08350175533704697; val_accuracy: 0.9821855095541401 

Epoch 28 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08363155337275972; val_accuracy: 0.9821855095541401 

Epoch 29 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08377557625151744; val_accuracy: 0.9821855095541401 

Epoch 30 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08409431837736421; val_accuracy: 0.9822850318471338 

Epoch 31 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08427762970992714; val_accuracy: 0.9821855095541401 

Epoch 32 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08427740476883142; val_accuracy: 0.9821855095541401 

Epoch 33 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08442616431861166; val_accuracy: 0.9822850318471338 

Epoch 34 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08454372733831406; val_accuracy: 0.9822850318471338 

Epoch 35 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08460828491077302; val_accuracy: 0.9822850318471338 

Epoch 36 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08478016650695709; val_accuracy: 0.9821855095541401 

Epoch 37 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0847629753362601; val_accuracy: 0.9823845541401274 

Epoch 38 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08499507121979051; val_accuracy: 0.9823845541401274 

Epoch 39 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08506732004558205; val_accuracy: 0.9821855095541401 

Epoch 40 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08509268554722428; val_accuracy: 0.9821855095541401 

Epoch 41 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08515152202290334; val_accuracy: 0.9821855095541401 

Epoch 42 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08536871195219126; val_accuracy: 0.9823845541401274 

Epoch 43 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08547685338053734; val_accuracy: 0.9821855095541401 

Epoch 44 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08555498956495029; val_accuracy: 0.9821855095541401 

Epoch 45 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08564569186537889; val_accuracy: 0.9823845541401274 

Epoch 46 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08564527859543539; val_accuracy: 0.9821855095541401 

Epoch 47 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0856862425993962; val_accuracy: 0.9820859872611465 

Epoch 48 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0857209931750586; val_accuracy: 0.9820859872611465 

Epoch 49 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08572995268805012; val_accuracy: 0.9822850318471338 

Epoch 50 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08576765955443595; val_accuracy: 0.9821855095541401 

plots/no_subspace_training/MLP/2020-01-19 00:51:29/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07225284810848297; val_accuracy: 0.9823845541401274 

Epoch 12 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07245996266983118; val_accuracy: 0.9823845541401274 

Epoch 13 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07427526526390367; val_accuracy: 0.9815883757961783 

Epoch 14 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07389260325462195; val_accuracy: 0.982484076433121 

Epoch 15 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0749250210489437; val_accuracy: 0.982484076433121 

Epoch 16 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07556379168846045; val_accuracy: 0.9827826433121019 

Epoch 17 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07589792227669126; val_accuracy: 0.9825835987261147 

Epoch 18 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07645982271356946; val_accuracy: 0.9827826433121019 

Epoch 19 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07681675502069436; val_accuracy: 0.9831807324840764 

Epoch 20 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07788144612008599; val_accuracy: 0.9827826433121019 

Epoch 21 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07769673203776596; val_accuracy: 0.9826831210191083 

Epoch 22 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07795859683471121; val_accuracy: 0.9825835987261147 

Epoch 23 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0781794195152392; val_accuracy: 0.9827826433121019 

Epoch 24 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07805473072703477; val_accuracy: 0.9827826433121019 

Epoch 25 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07835398581187436; val_accuracy: 0.9826831210191083 

Epoch 26 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07854427103024379; val_accuracy: 0.9827826433121019 

Epoch 27 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07886629385553348; val_accuracy: 0.9827826433121019 

Epoch 28 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07892738757239785; val_accuracy: 0.9828821656050956 

Epoch 29 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07899999722933314; val_accuracy: 0.9827826433121019 

Epoch 30 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07922568362040125; val_accuracy: 0.9828821656050956 

Epoch 31 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07932873715640633; val_accuracy: 0.9826831210191083 

Epoch 32 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07934087225396162; val_accuracy: 0.9826831210191083 

Epoch 33 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0794219257915096; val_accuracy: 0.9827826433121019 

Epoch 34 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07949521406820625; val_accuracy: 0.9828821656050956 

Epoch 35 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07953967467235153; val_accuracy: 0.9829816878980892 

Epoch 36 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07965976360497201; val_accuracy: 0.9827826433121019 

Epoch 37 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07964786963098368; val_accuracy: 0.9826831210191083 

Epoch 38 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07978197305825106; val_accuracy: 0.9826831210191083 

Epoch 39 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07983592195305855; val_accuracy: 0.9828821656050956 

Epoch 40 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07985879832012638; val_accuracy: 0.9828821656050956 

Epoch 41 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07987545245582131; val_accuracy: 0.9828821656050956 

Epoch 42 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07992116907599625; val_accuracy: 0.9827826433121019 

Epoch 43 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07995371947622602; val_accuracy: 0.9827826433121019 

Epoch 44 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07998487210957109; val_accuracy: 0.9828821656050956 

Epoch 45 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0800146124070617; val_accuracy: 0.9827826433121019 

Epoch 46 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0800279385534821; val_accuracy: 0.9826831210191083 

Epoch 47 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08005872240681557; val_accuracy: 0.9826831210191083 

Epoch 48 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08008654733562166; val_accuracy: 0.9826831210191083 

Epoch 49 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08009633725615824; val_accuracy: 0.9827826433121019 

Epoch 50 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08011797274563723; val_accuracy: 0.9827826433121019 

plots/no_subspace_training/MLP/2020-01-19 00:55:01/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06907581167805726; val_accuracy: 0.9809912420382165 

Epoch 7 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0701591107923134; val_accuracy: 0.9806926751592356 

Epoch 8 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06901673450591458; val_accuracy: 0.9800955414012739 

Epoch 9 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0676679152307237; val_accuracy: 0.9821855095541401 

Epoch 10 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06847753162216988; val_accuracy: 0.9825835987261147 

Epoch 11 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06808830512936707; val_accuracy: 0.9822850318471338 

Epoch 12 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06845191842431475; val_accuracy: 0.9820859872611465 

Epoch 13 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06948191762729815; val_accuracy: 0.9821855095541401 

Epoch 14 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06898586435397719; val_accuracy: 0.9823845541401274 

Epoch 15 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06936092067296339; val_accuracy: 0.9821855095541401 

Epoch 16 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06948329180858698; val_accuracy: 0.982484076433121 

Epoch 17 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06961920653369016; val_accuracy: 0.9825835987261147 

Epoch 18 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06979909940225304; val_accuracy: 0.9823845541401274 

Epoch 19 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06985957073462996; val_accuracy: 0.982484076433121 

Epoch 20 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0701891860337394; val_accuracy: 0.9823845541401274 

Epoch 21 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0701224798467129; val_accuracy: 0.982484076433121 

Epoch 22 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07021240453431561; val_accuracy: 0.982484076433121 

Epoch 23 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07028768558031434; val_accuracy: 0.982484076433121 

Epoch 24 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07020255591079688; val_accuracy: 0.9823845541401274 

Epoch 25 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07029475459152726; val_accuracy: 0.9823845541401274 

Epoch 26 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07031905693802865; val_accuracy: 0.9823845541401274 

Epoch 27 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07035059465249632; val_accuracy: 0.9825835987261147 

Epoch 28 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07036555959445656; val_accuracy: 0.982484076433121 

Epoch 29 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07038336899724736; val_accuracy: 0.9825835987261147 

Epoch 30 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07039857883552078; val_accuracy: 0.9825835987261147 

Epoch 31 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07041014866179722; val_accuracy: 0.9825835987261147 

Epoch 32 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07041482409094549; val_accuracy: 0.9825835987261147 

Epoch 33 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07041974522315772; val_accuracy: 0.9825835987261147 

Epoch 34 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07042873695872393; val_accuracy: 0.9825835987261147 

Epoch 35 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0704354619856473; val_accuracy: 0.9825835987261147 

Epoch 36 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07044021298835991; val_accuracy: 0.9825835987261147 

Epoch 37 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07044278317765826; val_accuracy: 0.9825835987261147 

Epoch 38 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07044755992521147; val_accuracy: 0.9825835987261147 

Epoch 39 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07045149278773624; val_accuracy: 0.9825835987261147 

Epoch 40 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07045445363422867; val_accuracy: 0.9825835987261147 

Epoch 41 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07045444025165716; val_accuracy: 0.9825835987261147 

Epoch 42 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07045616930837084; val_accuracy: 0.9825835987261147 

Epoch 43 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07045780959877239; val_accuracy: 0.9825835987261147 

Epoch 44 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07046035826680766; val_accuracy: 0.9825835987261147 

Epoch 45 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0704620315628067; val_accuracy: 0.9825835987261147 

Epoch 46 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07046248526520031; val_accuracy: 0.9825835987261147 

Epoch 47 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07046301793425706; val_accuracy: 0.9825835987261147 

Epoch 48 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07046343156012; val_accuracy: 0.9825835987261147 

Epoch 49 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07046404051458001; val_accuracy: 0.9825835987261147 

Epoch 50 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07046454027295113; val_accuracy: 0.9825835987261147 

plots/no_subspace_training/MLP/2020-01-19 00:58:40/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07677489025577618; val_accuracy: 0.9818869426751592 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.078548801457806; val_accuracy: 0.9806926751592356 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07858233979553174; val_accuracy: 0.9814888535031847 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07736258823305937; val_accuracy: 0.9831807324840764 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07938695414240952; val_accuracy: 0.9827826433121019 

Epoch 16 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07929339290709253; val_accuracy: 0.9828821656050956 

Epoch 17 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07947580758363577; val_accuracy: 0.9830812101910829 

Epoch 18 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07977180669357063; val_accuracy: 0.9826831210191083 

Epoch 19 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07992131327083156; val_accuracy: 0.9827826433121019 

Epoch 20 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08025202171722795; val_accuracy: 0.9827826433121019 

Epoch 21 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08039496241101793; val_accuracy: 0.9825835987261147 

Epoch 22 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08064426510197342; val_accuracy: 0.9825835987261147 

Epoch 23 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08088874254541792; val_accuracy: 0.9825835987261147 

Epoch 24 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08081230172401022; val_accuracy: 0.9826831210191083 

Epoch 25 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08112177511404274; val_accuracy: 0.9826831210191083 

Epoch 26 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0813212798801577; val_accuracy: 0.982484076433121 

Epoch 27 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08159797626790727; val_accuracy: 0.9822850318471338 

Epoch 28 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08168171445845039; val_accuracy: 0.982484076433121 

Epoch 29 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08179031497543784; val_accuracy: 0.9821855095541401 

Epoch 30 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08198312238143508; val_accuracy: 0.9825835987261147 

Epoch 31 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08203545734760867; val_accuracy: 0.9823845541401274 

Epoch 32 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08206433344893395; val_accuracy: 0.9823845541401274 

Epoch 33 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08209852625135404; val_accuracy: 0.9823845541401274 

Epoch 34 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.082142951121186; val_accuracy: 0.982484076433121 

Epoch 35 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08217045396661303; val_accuracy: 0.982484076433121 

Epoch 36 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08222210998083376; val_accuracy: 0.9823845541401274 

Epoch 37 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08223805828079296; val_accuracy: 0.9823845541401274 

Epoch 38 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08229127044605601; val_accuracy: 0.9823845541401274 

Epoch 39 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08232560068084176; val_accuracy: 0.9823845541401274 

Epoch 40 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08234800588173471; val_accuracy: 0.982484076433121 

Epoch 41 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08236437335990038; val_accuracy: 0.9823845541401274 

Epoch 42 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.082417476684994; val_accuracy: 0.9823845541401274 

Epoch 43 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0824547308455607; val_accuracy: 0.9823845541401274 

Epoch 44 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0824907009673726; val_accuracy: 0.982484076433121 

Epoch 45 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0825271682612076; val_accuracy: 0.9823845541401274 

Epoch 46 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08253141537688341; val_accuracy: 0.9823845541401274 

Epoch 47 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08253634056657742; val_accuracy: 0.9823845541401274 

Epoch 48 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08254051170531351; val_accuracy: 0.9823845541401274 

Epoch 49 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0825454926557222; val_accuracy: 0.9823845541401274 

Epoch 50 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08255084022689776; val_accuracy: 0.9823845541401274 

plots/no_subspace_training/MLP/2020-01-19 01:02:10/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07168835826265584; val_accuracy: 0.9829816878980892 

Epoch 12 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07140767080768658; val_accuracy: 0.9826831210191083 

Epoch 13 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07258634648884937; val_accuracy: 0.9819864649681529 

Epoch 14 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0724221258691162; val_accuracy: 0.9820859872611465 

Epoch 15 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07302134336939284; val_accuracy: 0.982484076433121 

Epoch 16 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0733893136879441; val_accuracy: 0.9825835987261147 

Epoch 17 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07365540965537357; val_accuracy: 0.9820859872611465 

Epoch 18 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07396781786232237; val_accuracy: 0.982484076433121 

Epoch 19 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07432233893377765; val_accuracy: 0.9825835987261147 

Epoch 20 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0749478855520297; val_accuracy: 0.9829816878980892 

Epoch 21 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0748594043076418; val_accuracy: 0.9826831210191083 

Epoch 22 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07490504234080102; val_accuracy: 0.9829816878980892 

Epoch 23 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0749580595808424; val_accuracy: 0.9829816878980892 

Epoch 24 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0749590305765723; val_accuracy: 0.9828821656050956 

Epoch 25 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07502839121089619; val_accuracy: 0.9828821656050956 

Epoch 26 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07512123246861112; val_accuracy: 0.9827826433121019 

Epoch 27 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07521631454776047; val_accuracy: 0.9829816878980892 

Epoch 28 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07524545215497351; val_accuracy: 0.9829816878980892 

Epoch 29 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07529909325063608; val_accuracy: 0.9829816878980892 

Epoch 30 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07535483820992671; val_accuracy: 0.9828821656050956 

Epoch 31 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07537291104057033; val_accuracy: 0.9829816878980892 

Epoch 32 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0753869094002019; val_accuracy: 0.9829816878980892 

Epoch 33 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07539873978324757; val_accuracy: 0.9829816878980892 

Epoch 34 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0754148361219722; val_accuracy: 0.9829816878980892 

Epoch 35 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07542825722770327; val_accuracy: 0.9828821656050956 

Epoch 36 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07544482281063772; val_accuracy: 0.9828821656050956 

Epoch 37 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07545565035502622; val_accuracy: 0.9828821656050956 

Epoch 38 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07547191606395563; val_accuracy: 0.9828821656050956 

Epoch 39 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0754862185211698; val_accuracy: 0.9828821656050956 

Epoch 40 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07549795195175583; val_accuracy: 0.9828821656050956 

Epoch 41 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07549941207572913; val_accuracy: 0.9828821656050956 

Epoch 42 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07550208621723636; val_accuracy: 0.9828821656050956 

Epoch 43 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07550477962585012; val_accuracy: 0.9828821656050956 

Epoch 44 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07550779084680946; val_accuracy: 0.9828821656050956 

Epoch 45 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07551063934139385; val_accuracy: 0.9828821656050956 

Epoch 46 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07551309594493004; val_accuracy: 0.9828821656050956 

Epoch 47 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07551564072157926; val_accuracy: 0.9828821656050956 

Epoch 48 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07551810003010331; val_accuracy: 0.9828821656050956 

Epoch 49 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552075291135509; val_accuracy: 0.9828821656050956 

Epoch 50 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552322716849624; val_accuracy: 0.9828821656050956 

plots/no_subspace_training/MLP/2020-01-19 01:05:42/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06641717098510949; val_accuracy: 0.9809912420382165 

Epoch 7 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0675473737109239; val_accuracy: 0.9812898089171974 

Epoch 8 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06575990700797671; val_accuracy: 0.9811902866242038 

Epoch 9 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06592461893892591; val_accuracy: 0.9822850318471338 

Epoch 10 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06655600441584161; val_accuracy: 0.9822850318471338 

Epoch 11 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06642266704588179; val_accuracy: 0.9821855095541401 

Epoch 12 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06654601764811832; val_accuracy: 0.981687898089172 

Epoch 13 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06672510058636878; val_accuracy: 0.9814888535031847 

Epoch 14 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06661195013743297; val_accuracy: 0.9815883757961783 

Epoch 15 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06679942710384441; val_accuracy: 0.9814888535031847 

Epoch 16 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06678253704100658; val_accuracy: 0.9813893312101911 

Epoch 17 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06678128531973833; val_accuracy: 0.9815883757961783 

Epoch 18 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06680456451644563; val_accuracy: 0.9814888535031847 

Epoch 19 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06681012134453293; val_accuracy: 0.981687898089172 

Epoch 20 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.066831697324279; val_accuracy: 0.9817874203821656 

Epoch 21 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06683422131523205; val_accuracy: 0.9817874203821656 

Epoch 22 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06684031230723782; val_accuracy: 0.9817874203821656 

Epoch 23 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0668444459557913; val_accuracy: 0.9817874203821656 

Epoch 24 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0668445838864442; val_accuracy: 0.9817874203821656 

Epoch 25 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06684790406921867; val_accuracy: 0.9817874203821656 

Epoch 26 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06684820835662496; val_accuracy: 0.9817874203821656 

Epoch 27 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06684885567920223; val_accuracy: 0.9817874203821656 

Epoch 28 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0668494661522519; val_accuracy: 0.9817874203821656 

Epoch 29 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685001602408233; val_accuracy: 0.9817874203821656 

Epoch 30 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685057118724866; val_accuracy: 0.9817874203821656 

Epoch 31 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685062946312746; val_accuracy: 0.9817874203821656 

Epoch 32 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685064673708503; val_accuracy: 0.9817874203821656 

Epoch 33 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685054961852967; val_accuracy: 0.9817874203821656 

Epoch 34 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685059206785669; val_accuracy: 0.9817874203821656 

Epoch 35 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685063340196944; val_accuracy: 0.9817874203821656 

Epoch 36 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685059313561506; val_accuracy: 0.9817874203821656 

Epoch 37 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685055941817866; val_accuracy: 0.9817874203821656 

Epoch 38 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685053367333807; val_accuracy: 0.9817874203821656 

Epoch 39 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685050880643213; val_accuracy: 0.9817874203821656 

Epoch 40 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685047364158994; val_accuracy: 0.9817874203821656 

Epoch 41 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685046173015217; val_accuracy: 0.9817874203821656 

Epoch 42 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0668504501509059; val_accuracy: 0.9817874203821656 

Epoch 43 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685044500193778; val_accuracy: 0.9817874203821656 

Epoch 44 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685044659171135; val_accuracy: 0.9817874203821656 

Epoch 45 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685043823946814; val_accuracy: 0.9817874203821656 

Epoch 46 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685043861911555; val_accuracy: 0.9817874203821656 

Epoch 47 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685043683951827; val_accuracy: 0.9817874203821656 

Epoch 48 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685043757508514; val_accuracy: 0.9817874203821656 

Epoch 49 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0668504387852113; val_accuracy: 0.9817874203821656 

Epoch 50 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685043705306995; val_accuracy: 0.9817874203821656 

plots/no_subspace_training/MLP/2020-01-19 01:09:20/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07677489025577618; val_accuracy: 0.9818869426751592 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.078548801457806; val_accuracy: 0.9806926751592356 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07858233979553174; val_accuracy: 0.9814888535031847 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07736258823305937; val_accuracy: 0.9831807324840764 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07938695414240952; val_accuracy: 0.9827826433121019 

Epoch 16 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07921327968501742; val_accuracy: 0.9830812101910829 

Epoch 17 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07932663953323273; val_accuracy: 0.9831807324840764 

Epoch 18 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07951142418251675; val_accuracy: 0.9827826433121019 

Epoch 19 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07962060107547007; val_accuracy: 0.9829816878980892 

Epoch 20 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07983701514780142; val_accuracy: 0.9829816878980892 

Epoch 21 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07994577407267443; val_accuracy: 0.9829816878980892 

Epoch 22 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0801215695727403; val_accuracy: 0.9826831210191083 

Epoch 23 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0802953950348933; val_accuracy: 0.9827826433121019 

Epoch 24 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08029365843268717; val_accuracy: 0.9827826433121019 

Epoch 25 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08047888596441335; val_accuracy: 0.9826831210191083 

Epoch 26 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08063678234625773; val_accuracy: 0.982484076433121 

Epoch 27 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08080915368761227; val_accuracy: 0.982484076433121 

Epoch 28 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0808723566780804; val_accuracy: 0.9826831210191083 

Epoch 29 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08096624922695433; val_accuracy: 0.982484076433121 

Epoch 30 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08110151212116715; val_accuracy: 0.9825835987261147 

Epoch 31 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08112278469144159; val_accuracy: 0.9825835987261147 

Epoch 32 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0811399341721064; val_accuracy: 0.982484076433121 

Epoch 33 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08115406742521153; val_accuracy: 0.982484076433121 

Epoch 34 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08117457279923615; val_accuracy: 0.982484076433121 

Epoch 35 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08119100809192202; val_accuracy: 0.982484076433121 

Epoch 36 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08121296827485606; val_accuracy: 0.982484076433121 

Epoch 37 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08122623457935206; val_accuracy: 0.982484076433121 

Epoch 38 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0812478577540179; val_accuracy: 0.982484076433121 

Epoch 39 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08126660849258399; val_accuracy: 0.982484076433121 

Epoch 40 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0812820082732067; val_accuracy: 0.982484076433121 

Epoch 41 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08129197525180828; val_accuracy: 0.982484076433121 

Epoch 42 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08131327556007227; val_accuracy: 0.982484076433121 

Epoch 43 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08133279306778482; val_accuracy: 0.982484076433121 

Epoch 44 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08135186418131658; val_accuracy: 0.982484076433121 

Epoch 45 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08136939988204628; val_accuracy: 0.982484076433121 

Epoch 46 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08137123775520143; val_accuracy: 0.982484076433121 

Epoch 47 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08137314090398466; val_accuracy: 0.982484076433121 

Epoch 48 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08137465237527136; val_accuracy: 0.982484076433121 

Epoch 49 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08137664061253237; val_accuracy: 0.982484076433121 

Epoch 50 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08137850116962081; val_accuracy: 0.982484076433121 

plots/no_subspace_training/MLP/2020-01-19 01:12:49/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0716311098758582; val_accuracy: 0.9833797770700637 

Epoch 12 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07133036344104511; val_accuracy: 0.9829816878980892 

Epoch 13 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07194302311748456; val_accuracy: 0.982484076433121 

Epoch 14 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07184740075260211; val_accuracy: 0.9822850318471338 

Epoch 15 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.072274019668816; val_accuracy: 0.9826831210191083 

Epoch 16 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07254032838116785; val_accuracy: 0.982484076433121 

Epoch 17 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07274381417757386; val_accuracy: 0.9822850318471338 

Epoch 18 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07296031722976903; val_accuracy: 0.9827826433121019 

Epoch 19 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07326612768659166; val_accuracy: 0.982484076433121 

Epoch 20 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0736579478356489; val_accuracy: 0.9825835987261147 

Epoch 21 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07364196605553293; val_accuracy: 0.9825835987261147 

Epoch 22 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07365515657291291; val_accuracy: 0.9826831210191083 

Epoch 23 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07367186120171455; val_accuracy: 0.9826831210191083 

Epoch 24 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07367017869926562; val_accuracy: 0.9825835987261147 

Epoch 25 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07369452805086306; val_accuracy: 0.9825835987261147 

Epoch 26 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07373035798786552; val_accuracy: 0.9826831210191083 

Epoch 27 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07376978019620203; val_accuracy: 0.9827826433121019 

Epoch 28 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07378914397043787; val_accuracy: 0.9827826433121019 

Epoch 29 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07381728979622483; val_accuracy: 0.9826831210191083 

Epoch 30 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0738422398449509; val_accuracy: 0.9827826433121019 

Epoch 31 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07384663075208664; val_accuracy: 0.9827826433121019 

Epoch 32 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07385104367877268; val_accuracy: 0.9827826433121019 

Epoch 33 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07385394171734525; val_accuracy: 0.9827826433121019 

Epoch 34 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07385858751026689; val_accuracy: 0.9827826433121019 

Epoch 35 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0738628188705748; val_accuracy: 0.9827826433121019 

Epoch 36 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07386730701490572; val_accuracy: 0.9827826433121019 

Epoch 37 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07387137299130676; val_accuracy: 0.9827826433121019 

Epoch 38 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07387602182140776; val_accuracy: 0.9827826433121019 

Epoch 39 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388063425281245; val_accuracy: 0.9827826433121019 

Epoch 40 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388491838411161; val_accuracy: 0.9827826433121019 

Epoch 41 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388513226797626; val_accuracy: 0.9827826433121019 

Epoch 42 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388555495792133; val_accuracy: 0.9827826433121019 

Epoch 43 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388601105683928; val_accuracy: 0.9827826433121019 

Epoch 44 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388655129511645; val_accuracy: 0.9827826433121019 

Epoch 45 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388701308874568; val_accuracy: 0.9827826433121019 

Epoch 46 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388744849687928; val_accuracy: 0.9827826433121019 

Epoch 47 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388787094954473; val_accuracy: 0.9827826433121019 

Epoch 48 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0738883345464992; val_accuracy: 0.9827826433121019 

Epoch 49 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388883477942959; val_accuracy: 0.9827826433121019 

Epoch 50 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07388931864006504; val_accuracy: 0.9827826433121019 

plots/no_subspace_training/MLP/2020-01-19 01:16:26/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06576700885869136; val_accuracy: 0.9806926751592356 

Epoch 7 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06667630749333436; val_accuracy: 0.981687898089172 

Epoch 8 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06555573630389894; val_accuracy: 0.9809912420382165 

Epoch 9 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06537873032176571; val_accuracy: 0.9818869426751592 

Epoch 10 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06596264179060414; val_accuracy: 0.9820859872611465 

Epoch 11 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06590068328437532; val_accuracy: 0.9815883757961783 

Epoch 12 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06597207932715203; val_accuracy: 0.9814888535031847 

Epoch 13 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06601307021489569; val_accuracy: 0.9815883757961783 

Epoch 14 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06598591946872176; val_accuracy: 0.9815883757961783 

Epoch 15 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06607643061667491; val_accuracy: 0.9815883757961783 

Epoch 16 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06607481559655469; val_accuracy: 0.9815883757961783 

Epoch 17 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06607444062354459; val_accuracy: 0.981687898089172 

Epoch 18 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06607773591568515; val_accuracy: 0.981687898089172 

Epoch 19 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06607798629315795; val_accuracy: 0.981687898089172 

Epoch 20 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608101431351558; val_accuracy: 0.981687898089172 

Epoch 21 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608121258438013; val_accuracy: 0.981687898089172 

Epoch 22 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660819249215779; val_accuracy: 0.9815883757961783 

Epoch 23 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608231308733582; val_accuracy: 0.9815883757961783 

Epoch 24 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660823408015974; val_accuracy: 0.9815883757961783 

Epoch 25 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608260629378307; val_accuracy: 0.9815883757961783 

Epoch 26 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660825203510986; val_accuracy: 0.9815883757961783 

Epoch 27 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608251202258335; val_accuracy: 0.9815883757961783 

Epoch 28 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608248312192358; val_accuracy: 0.9815883757961783 

Epoch 29 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660824474350662; val_accuracy: 0.9815883757961783 

Epoch 30 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608239148452782; val_accuracy: 0.9815883757961783 

Epoch 31 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608236502784832; val_accuracy: 0.9815883757961783 

Epoch 32 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608234618784516; val_accuracy: 0.9815883757961783 

Epoch 33 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608232001590121; val_accuracy: 0.9815883757961783 

Epoch 34 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608230399952572; val_accuracy: 0.9815883757961783 

Epoch 35 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608228043765779; val_accuracy: 0.9815883757961783 

Epoch 36 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660822777563979; val_accuracy: 0.9815883757961783 

Epoch 37 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227471921854; val_accuracy: 0.9815883757961783 

Epoch 38 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227654627175; val_accuracy: 0.9815883757961783 

Epoch 39 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227365146017; val_accuracy: 0.9815883757961783 

Epoch 40 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227365146017; val_accuracy: 0.9815883757961783 

Epoch 41 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660822736989161; val_accuracy: 0.9815883757961783 

Epoch 42 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660822729158933; val_accuracy: 0.9815883757961783 

Epoch 43 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660822744819389; val_accuracy: 0.9815883757961783 

Epoch 44 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227270234163; val_accuracy: 0.9815883757961783 

Epoch 45 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227270234163; val_accuracy: 0.9815883757961783 

Epoch 46 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227270234163; val_accuracy: 0.9815883757961783 

Epoch 47 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227274979755; val_accuracy: 0.9815883757961783 

Epoch 48 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227274979755; val_accuracy: 0.9815883757961783 

Epoch 49 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227270234163; val_accuracy: 0.9815883757961783 

Epoch 50 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06608227274979755; val_accuracy: 0.9815883757961783 

plots/no_subspace_training/MLP/2020-01-19 01:19:58/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07677489025577618; val_accuracy: 0.9818869426751592 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.078548801457806; val_accuracy: 0.9806926751592356 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07858233979553174; val_accuracy: 0.9814888535031847 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07736258823305937; val_accuracy: 0.9831807324840764 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07938695414240952; val_accuracy: 0.9827826433121019 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07918933693580567; val_accuracy: 0.9829816878980892 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07926955306605929; val_accuracy: 0.9831807324840764 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0794020539778433; val_accuracy: 0.9828821656050956 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07948626845980146; val_accuracy: 0.9829816878980892 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07964972309341097; val_accuracy: 0.9830812101910829 

Epoch 21 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07974512490687097; val_accuracy: 0.9830812101910829 

Epoch 22 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07988602698893305; val_accuracy: 0.9827826433121019 

Epoch 23 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0800204092909576; val_accuracy: 0.9828821656050956 

Epoch 24 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08004054764083995; val_accuracy: 0.9828821656050956 

Epoch 25 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08018080866450716; val_accuracy: 0.9826831210191083 

Epoch 26 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08031804069497023; val_accuracy: 0.982484076433121 

Epoch 27 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08044698369351162; val_accuracy: 0.9825835987261147 

Epoch 28 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08050549618757455; val_accuracy: 0.9826831210191083 

Epoch 29 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08058732142968542; val_accuracy: 0.9825835987261147 

Epoch 30 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08068630697241255; val_accuracy: 0.9825835987261147 

Epoch 31 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08069915874938297; val_accuracy: 0.9825835987261147 

Epoch 32 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08071038947933039; val_accuracy: 0.9825835987261147 

Epoch 33 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08071897999875864; val_accuracy: 0.9825835987261147 

Epoch 34 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08073128392077555; val_accuracy: 0.9825835987261147 

Epoch 35 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08074192206855792; val_accuracy: 0.9825835987261147 

Epoch 36 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08075481038659242; val_accuracy: 0.982484076433121 

Epoch 37 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08076438518941023; val_accuracy: 0.982484076433121 

Epoch 38 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08077703883788388; val_accuracy: 0.982484076433121 

Epoch 39 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08078847317748768; val_accuracy: 0.982484076433121 

Epoch 40 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08079870621774607; val_accuracy: 0.982484076433121 

Epoch 41 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08080625424908984; val_accuracy: 0.982484076433121 

Epoch 42 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08081858794970118; val_accuracy: 0.982484076433121 

Epoch 43 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08083077873678723; val_accuracy: 0.982484076433121 

Epoch 44 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08084331526403214; val_accuracy: 0.982484076433121 

Epoch 45 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.080855574791029; val_accuracy: 0.982484076433121 

Epoch 46 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08085647023692252; val_accuracy: 0.982484076433121 

Epoch 47 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0808573548628646; val_accuracy: 0.982484076433121 

Epoch 48 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0808580916873209; val_accuracy: 0.982484076433121 

Epoch 49 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08085909893937931; val_accuracy: 0.982484076433121 

Epoch 50 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08086001467268178; val_accuracy: 0.982484076433121 

plots/no_subspace_training/MLP/2020-01-19 01:23:28/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08325323853997668; val_accuracy: 0.9763136942675159 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08461066226291049; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 1.78; acc: 0.86
Batch: 20; loss: 1.61; acc: 0.83
Batch: 40; loss: 2.55; acc: 0.88
Batch: 60; loss: 2.63; acc: 0.83
Batch: 80; loss: 1.1; acc: 0.89
Batch: 100; loss: 1.18; acc: 0.86
Batch: 120; loss: 2.08; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.89
Val Epoch over. val_loss: 1.358332739134503; val_accuracy: 0.866640127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.84
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07592066318081443; val_accuracy: 0.9806926751592356 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07954577869100934; val_accuracy: 0.9790007961783439 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07169858617767406; val_accuracy: 0.9831807324840764 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.071325158664755; val_accuracy: 0.9829816878980892 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07174357377989277; val_accuracy: 0.9827826433121019 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0716564660524107; val_accuracy: 0.982484076433121 

Epoch 15 start
The current lr is: 0.010000000000000002
slurmstepd: error: _is_a_lwp: open() /proc/222224/status failed: No such file or directory
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07193861934979251; val_accuracy: 0.9825835987261147 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07216036267531147; val_accuracy: 0.982484076433121 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07231456767411748; val_accuracy: 0.9823845541401274 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07248745621389645; val_accuracy: 0.9825835987261147 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07274226868038725; val_accuracy: 0.9822850318471338 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07302884705317249; val_accuracy: 0.9826831210191083 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07303288683390162; val_accuracy: 0.9826831210191083 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07304724495691858; val_accuracy: 0.9826831210191083 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07306099217978253; val_accuracy: 0.9826831210191083 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07306394816204241; val_accuracy: 0.9826831210191083 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07307721717152627; val_accuracy: 0.9826831210191083 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0730975592497048; val_accuracy: 0.9826831210191083 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07312032001413357; val_accuracy: 0.9826831210191083 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07313529629806045; val_accuracy: 0.9826831210191083 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07315297011926675; val_accuracy: 0.9826831210191083 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07316810832281781; val_accuracy: 0.9826831210191083 

Epoch 31 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07316989949934043; val_accuracy: 0.9826831210191083 

Epoch 32 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07317174192826459; val_accuracy: 0.9826831210191083 

Epoch 33 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07317287370467641; val_accuracy: 0.9826831210191083 

Epoch 34 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07317480815064376; val_accuracy: 0.9826831210191083 

Epoch 35 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07317663767155569; val_accuracy: 0.9826831210191083 

Epoch 36 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07317857140568411; val_accuracy: 0.9826831210191083 

Epoch 37 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318038873042271; val_accuracy: 0.9826831210191083 

Epoch 38 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318231012601002; val_accuracy: 0.9826831210191083 

Epoch 39 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0731842657847769; val_accuracy: 0.9826831210191083 

Epoch 40 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318603048089203; val_accuracy: 0.9826831210191083 

Epoch 41 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0731860226032081; val_accuracy: 0.9826831210191083 

Epoch 42 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318607219465219; val_accuracy: 0.9826831210191083 

Epoch 43 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318615201552203; val_accuracy: 0.9826831210191083 

Epoch 44 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318625257463213; val_accuracy: 0.9826831210191083 

Epoch 45 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318633804275732; val_accuracy: 0.9826831210191083 

Epoch 46 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318641772125936; val_accuracy: 0.9826831210191083 

Epoch 47 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318649426767021; val_accuracy: 0.9826831210191083 

Epoch 48 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318657897650056; val_accuracy: 0.9826831210191083 

Epoch 49 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318669068775359; val_accuracy: 0.9826831210191083 

Epoch 50 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07318676120726167; val_accuracy: 0.9826831210191083 

plots/no_subspace_training/MLP/2020-01-19 01:27:04/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.2600719262934794; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09992160140329105; val_accuracy: 0.9700437898089171 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08191286160308085; val_accuracy: 0.9765127388535032 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08268810170376377; val_accuracy: 0.9778065286624203 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08531157150390042; val_accuracy: 0.9756170382165605 

Epoch 6 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06566226442053819; val_accuracy: 0.9807921974522293 

Epoch 7 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06633046224333677; val_accuracy: 0.981687898089172 

Epoch 8 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06552801191047498; val_accuracy: 0.9808917197452229 

Epoch 9 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06529419176327954; val_accuracy: 0.981687898089172 

Epoch 10 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06575816454496353; val_accuracy: 0.981687898089172 

Epoch 11 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06575726563478732; val_accuracy: 0.9818869426751592 

Epoch 12 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06580386068790582; val_accuracy: 0.9819864649681529 

Epoch 13 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0658285689724099; val_accuracy: 0.9814888535031847 

Epoch 14 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06581982915663416; val_accuracy: 0.9815883757961783 

Epoch 15 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586441037001883; val_accuracy: 0.981687898089172 

Epoch 16 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0658646501173639; val_accuracy: 0.9815883757961783 

Epoch 17 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586505149959758; val_accuracy: 0.9815883757961783 

Epoch 18 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0658664485546434; val_accuracy: 0.9815883757961783 

Epoch 19 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586671032153876; val_accuracy: 0.9815883757961783 

Epoch 20 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586770281480377; val_accuracy: 0.9815883757961783 

Epoch 21 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586762610229717; val_accuracy: 0.9815883757961783 

Epoch 22 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0658677672124972; val_accuracy: 0.9815883757961783 

Epoch 23 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586774338962166; val_accuracy: 0.9815883757961783 

Epoch 24 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586766219252993; val_accuracy: 0.9815883757961783 

Epoch 25 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586763874930181; val_accuracy: 0.9815883757961783 

Epoch 26 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586757560919045; val_accuracy: 0.9815883757961783 

Epoch 27 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586754170193035; val_accuracy: 0.9815883757961783 

Epoch 28 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586749372398777; val_accuracy: 0.9815883757961783 

Epoch 29 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.065867447834106; val_accuracy: 0.9815883757961783 

Epoch 30 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586739454109958; val_accuracy: 0.9815883757961783 

Epoch 31 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0658673900802424; val_accuracy: 0.9815883757961783 

Epoch 32 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586738815827733; val_accuracy: 0.9815883757961783 

Epoch 33 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586738113480009; val_accuracy: 0.9815883757961783 

Epoch 34 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737847726816; val_accuracy: 0.9815883757961783 

Epoch 35 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737444351433; val_accuracy: 0.9815883757961783 

Epoch 36 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737460961008; val_accuracy: 0.9815883757961783 

Epoch 37 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0658673743960584; val_accuracy: 0.9815883757961783 

Epoch 38 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737377913135; val_accuracy: 0.9815883757961783 

Epoch 39 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737392149913; val_accuracy: 0.9815883757961783 

Epoch 40 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737510789732; val_accuracy: 0.9815883757961783 

Epoch 41 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737515535324; val_accuracy: 0.9815883757961783 

Epoch 42 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737510789732; val_accuracy: 0.9815883757961783 

Epoch 43 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737510789732; val_accuracy: 0.9815883757961783 

Epoch 44 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737496552954; val_accuracy: 0.9815883757961783 

Epoch 45 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737491807361; val_accuracy: 0.9815883757961783 

Epoch 46 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737491807361; val_accuracy: 0.9815883757961783 

Epoch 47 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737491807361; val_accuracy: 0.9815883757961783 

Epoch 48 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737491807361; val_accuracy: 0.9815883757961783 

Epoch 49 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737491807361; val_accuracy: 0.9815883757961783 

Epoch 50 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06586737501298547; val_accuracy: 0.9815883757961783 

plots/no_subspace_training/MLP/2020-01-19 01:30:41/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.91
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12117072576834897; val_accuracy: 0.9671576433121019 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11298768466730026; val_accuracy: 0.96875 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10762371644852267; val_accuracy: 0.9697452229299363 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10281171815790188; val_accuracy: 0.9701433121019108 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09964878603246562; val_accuracy: 0.9707404458598726 

Epoch 16 start
The current lr is: 0.008
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09777774832620742; val_accuracy: 0.9729299363057324 

Epoch 17 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09422974836579553; val_accuracy: 0.9716361464968153 

Epoch 18 start
The current lr is: 0.008
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09175917760107168; val_accuracy: 0.9734275477707006 

Epoch 19 start
The current lr is: 0.008
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09233751684237437; val_accuracy: 0.9738256369426752 

Epoch 20 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09056404574661497; val_accuracy: 0.9753184713375797 

Epoch 21 start
The current lr is: 0.008
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08686486358760269; val_accuracy: 0.9743232484076433 

Epoch 22 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08995274043861468; val_accuracy: 0.9746218152866242 

Epoch 23 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.94
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08539862154870276; val_accuracy: 0.9756170382165605 

Epoch 24 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08494771336009548; val_accuracy: 0.9746218152866242 

Epoch 25 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08367879240281263; val_accuracy: 0.9739251592356688 

Epoch 26 start
The current lr is: 0.008
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.98
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.2; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08271047344822792; val_accuracy: 0.9758160828025477 

Epoch 27 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08284396018571914; val_accuracy: 0.9768113057324841 

Epoch 28 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08049530018666748; val_accuracy: 0.9766122611464968 

Epoch 29 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08026672724136122; val_accuracy: 0.9762141719745223 

Epoch 30 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0805413330531424; val_accuracy: 0.9767117834394905 

Epoch 31 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08098536485414597; val_accuracy: 0.9767117834394905 

Epoch 32 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07894687269143998; val_accuracy: 0.9770103503184714 

Epoch 33 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07754586988193973; val_accuracy: 0.9770103503184714 

Epoch 34 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07776195074247706; val_accuracy: 0.9774084394904459 

Epoch 35 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07835294472373974; val_accuracy: 0.977906050955414 

Epoch 36 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07798581700890687; val_accuracy: 0.9780055732484076 

Epoch 37 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07662972817375402; val_accuracy: 0.977906050955414 

Epoch 38 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07649651364346219; val_accuracy: 0.9777070063694268 

Epoch 39 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691549279602469; val_accuracy: 0.9772093949044586 

Epoch 40 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07593533648237301; val_accuracy: 0.9773089171974523 

Epoch 41 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691942219426677; val_accuracy: 0.9774084394904459 

Epoch 42 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07587939059468592; val_accuracy: 0.977109872611465 

Epoch 43 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07690512630969855; val_accuracy: 0.977109872611465 

Epoch 44 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07707972898130204; val_accuracy: 0.9782046178343949 

Epoch 45 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07591299114713243; val_accuracy: 0.9780055732484076 

Epoch 46 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07606952101182027; val_accuracy: 0.9783041401273885 

Epoch 47 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07650643515928536; val_accuracy: 0.9785031847133758 

Epoch 48 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07535166184234011; val_accuracy: 0.9786027070063694 

Epoch 49 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07528582725460363; val_accuracy: 0.9782046178343949 

Epoch 50 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07638671291861565; val_accuracy: 0.9791003184713376 

plots/no_subspace_training/MLP/2020-01-19 01:34:12/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.008
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12061596313005041; val_accuracy: 0.9681528662420382 

Epoch 12 start
The current lr is: 0.008
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11441497559285467; val_accuracy: 0.9685509554140127 

Epoch 13 start
The current lr is: 0.008
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10976228243701018; val_accuracy: 0.9697452229299363 

Epoch 14 start
The current lr is: 0.008
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.08; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10569522515603691; val_accuracy: 0.9703423566878981 

Epoch 15 start
The current lr is: 0.008
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.102686685978607; val_accuracy: 0.9702428343949044 

Epoch 16 start
The current lr is: 0.008
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1012000854892336; val_accuracy: 0.9723328025477707 

Epoch 17 start
The current lr is: 0.008
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09711182421180094; val_accuracy: 0.9720342356687898 

Epoch 18 start
The current lr is: 0.008
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09424934851800561; val_accuracy: 0.9730294585987261 

Epoch 19 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09471942966056478; val_accuracy: 0.973328025477707 

Epoch 20 start
The current lr is: 0.008
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09275216215355381; val_accuracy: 0.9743232484076433 

Epoch 21 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08875561178110208; val_accuracy: 0.9739251592356688 

Epoch 22 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09163217623807063; val_accuracy: 0.9749203821656051 

Epoch 23 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08744401551166157; val_accuracy: 0.9748208598726115 

Epoch 24 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08646820625586873; val_accuracy: 0.9741242038216561 

Epoch 25 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08563837333089987; val_accuracy: 0.9738256369426752 

Epoch 26 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.21; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08436804689514409; val_accuracy: 0.9758160828025477 

Epoch 27 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08508844899049231; val_accuracy: 0.9759156050955414 

Epoch 28 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08262651667568334; val_accuracy: 0.9754179936305732 

Epoch 29 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08225953427090008; val_accuracy: 0.975218949044586 

Epoch 30 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08232257612476683; val_accuracy: 0.9765127388535032 

Epoch 31 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08249612933226452; val_accuracy: 0.9765127388535032 

Epoch 32 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08063929497151617; val_accuracy: 0.9765127388535032 

Epoch 33 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07938169830354155; val_accuracy: 0.9765127388535032 

Epoch 34 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07961710373971873; val_accuracy: 0.9766122611464968 

Epoch 35 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08022133797217326; val_accuracy: 0.9773089171974523 

Epoch 36 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07929271350430835; val_accuracy: 0.9773089171974523 

Epoch 37 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0782918963984699; val_accuracy: 0.9775079617834395 

Epoch 38 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07806568490756545; val_accuracy: 0.9774084394904459 

Epoch 39 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0782700268089012; val_accuracy: 0.9770103503184714 

Epoch 40 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07729341068370327; val_accuracy: 0.9767117834394905 

Epoch 41 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07757271645934719; val_accuracy: 0.9770103503184714 

Epoch 42 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0770772598019451; val_accuracy: 0.9769108280254777 

Epoch 43 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07797683618820397; val_accuracy: 0.9776074840764332 

Epoch 44 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07795305305700394; val_accuracy: 0.977906050955414 

Epoch 45 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0767981650628102; val_accuracy: 0.977109872611465 

Epoch 46 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07735088802162249; val_accuracy: 0.9776074840764332 

Epoch 47 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07736308687621621; val_accuracy: 0.9773089171974523 

Epoch 48 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07624510713633459; val_accuracy: 0.9776074840764332 

Epoch 49 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07608246879213175; val_accuracy: 0.9780055732484076 

Epoch 50 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07705420659036394; val_accuracy: 0.9782046178343949 

plots/no_subspace_training/MLP/2020-01-19 01:37:41/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.008
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1690151976172332; val_accuracy: 0.9526273885350318 

Epoch 7 start
The current lr is: 0.008
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.16144049051366036; val_accuracy: 0.9560111464968153 

Epoch 8 start
The current lr is: 0.008
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.18; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1456928579907888; val_accuracy: 0.9607882165605095 

Epoch 9 start
The current lr is: 0.008
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.91
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13974414739734048; val_accuracy: 0.9611863057324841 

Epoch 10 start
The current lr is: 0.008
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.08; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13025657740083468; val_accuracy: 0.9650676751592356 

Epoch 11 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.91
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12739320813565497; val_accuracy: 0.9651671974522293 

Epoch 12 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.22; acc: 0.89
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12202655749431082; val_accuracy: 0.9666600318471338 

Epoch 13 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11728009911384552; val_accuracy: 0.96765525477707 

Epoch 14 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11328712098632648; val_accuracy: 0.9688495222929936 

Epoch 15 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10998231820335054; val_accuracy: 0.9692476114649682 

Epoch 16 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10826118086363859; val_accuracy: 0.9699442675159236 

Epoch 17 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10490609067165928; val_accuracy: 0.9701433121019108 

Epoch 18 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10255677264871871; val_accuracy: 0.9711385350318471 

Epoch 19 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1024558707170046; val_accuracy: 0.9710390127388535 

Epoch 20 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10087292997320746; val_accuracy: 0.9716361464968153 

Epoch 21 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09748186567311834; val_accuracy: 0.9727308917197452 

Epoch 22 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0994881319392259; val_accuracy: 0.9727308917197452 

Epoch 23 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.08; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09581129416633563; val_accuracy: 0.9730294585987261 

Epoch 24 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09451304153082477; val_accuracy: 0.9723328025477707 

Epoch 25 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09365254066362502; val_accuracy: 0.9724323248407644 

Epoch 26 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09236035811578393; val_accuracy: 0.9732285031847133 

Epoch 27 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09323772503312226; val_accuracy: 0.9732285031847133 

Epoch 28 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09139269675798477; val_accuracy: 0.9730294585987261 

Epoch 29 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09053421044235777; val_accuracy: 0.973328025477707 

Epoch 30 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09030332762724275; val_accuracy: 0.9737261146496815 

Epoch 31 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0899495552442256; val_accuracy: 0.9742237261146497 

Epoch 32 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.21; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08852954170886118; val_accuracy: 0.9740246815286624 

Epoch 33 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.17; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08790861706065524; val_accuracy: 0.9737261146496815 

Epoch 34 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08772445145021578; val_accuracy: 0.9749203821656051 

Epoch 35 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.92
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08820205328950456; val_accuracy: 0.9746218152866242 

Epoch 36 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08678294464376322; val_accuracy: 0.9743232484076433 

Epoch 37 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08623003558653175; val_accuracy: 0.9743232484076433 

Epoch 38 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08609542022844788; val_accuracy: 0.9746218152866242 

Epoch 39 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08583535087905872; val_accuracy: 0.9748208598726115 

Epoch 40 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08515973773541724; val_accuracy: 0.974422770700637 

Epoch 41 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08514840663618344; val_accuracy: 0.974422770700637 

Epoch 42 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08472885423974627; val_accuracy: 0.974422770700637 

Epoch 43 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08525181122741123; val_accuracy: 0.9755175159235668 

Epoch 44 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08501484534543031; val_accuracy: 0.9753184713375797 

Epoch 45 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08405265575096865; val_accuracy: 0.9749203821656051 

Epoch 46 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08437418565154076; val_accuracy: 0.9755175159235668 

Epoch 47 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08399992103979087; val_accuracy: 0.9755175159235668 

Epoch 48 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0836184851967605; val_accuracy: 0.9754179936305732 

Epoch 49 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08341379423335099; val_accuracy: 0.9750199044585988 

Epoch 50 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08344570917498534; val_accuracy: 0.9756170382165605 

plots/no_subspace_training/MLP/2020-01-19 01:41:17/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.91
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12117072576834897; val_accuracy: 0.9671576433121019 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11298768466730026; val_accuracy: 0.96875 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10762371644852267; val_accuracy: 0.9697452229299363 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10281171815790188; val_accuracy: 0.9701433121019108 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09964878603246562; val_accuracy: 0.9707404458598726 

Epoch 16 start
The current lr is: 0.004
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09725908159165625; val_accuracy: 0.9731289808917197 

Epoch 17 start
The current lr is: 0.004
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0957802551424807; val_accuracy: 0.9726313694267515 

Epoch 18 start
The current lr is: 0.004
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09416370371439654; val_accuracy: 0.9730294585987261 

Epoch 19 start
The current lr is: 0.004
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0945679376696705; val_accuracy: 0.9728304140127388 

Epoch 20 start
The current lr is: 0.004
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09366216517652676; val_accuracy: 0.9740246815286624 

Epoch 21 start
The current lr is: 0.004
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09124546785168587; val_accuracy: 0.9736265923566879 

Epoch 22 start
The current lr is: 0.004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09330340451116015; val_accuracy: 0.9748208598726115 

Epoch 23 start
The current lr is: 0.004
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09033570296255647; val_accuracy: 0.9739251592356688 

Epoch 24 start
The current lr is: 0.004
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0891242848507538; val_accuracy: 0.9731289808917197 

Epoch 25 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08872372196738128; val_accuracy: 0.9728304140127388 

Epoch 26 start
The current lr is: 0.004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.21; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08760041107607495; val_accuracy: 0.9741242038216561 

Epoch 27 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08851023796637347; val_accuracy: 0.9748208598726115 

Epoch 28 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08660733384216666; val_accuracy: 0.9745222929936306 

Epoch 29 start
The current lr is: 0.004
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08595410755770222; val_accuracy: 0.9745222929936306 

Epoch 30 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0858746296281268; val_accuracy: 0.9748208598726115 

Epoch 31 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08502943474490932; val_accuracy: 0.9753184713375797 

Epoch 32 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.21; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08427603296033896; val_accuracy: 0.9746218152866242 

Epoch 33 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.14; acc: 0.92
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08423862110372561; val_accuracy: 0.9750199044585988 

Epoch 34 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08399039326579708; val_accuracy: 0.975218949044586 

Epoch 35 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.94
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08438083071522652; val_accuracy: 0.9758160828025477 

Epoch 36 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08355063757604095; val_accuracy: 0.975218949044586 

Epoch 37 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08319292469009472; val_accuracy: 0.9755175159235668 

Epoch 38 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0832435907973985; val_accuracy: 0.9755175159235668 

Epoch 39 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08301405157822712; val_accuracy: 0.9759156050955414 

Epoch 40 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08257704531880701; val_accuracy: 0.9757165605095541 

Epoch 41 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0825796705569811; val_accuracy: 0.9755175159235668 

Epoch 42 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08227436635999164; val_accuracy: 0.9758160828025477 

Epoch 43 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08270256078926622; val_accuracy: 0.9756170382165605 

Epoch 44 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08262738208197484; val_accuracy: 0.9761146496815286 

Epoch 45 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0817538348447745; val_accuracy: 0.9761146496815286 

Epoch 46 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08185208883065327; val_accuracy: 0.9761146496815286 

Epoch 47 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08174347317522498; val_accuracy: 0.9761146496815286 

Epoch 48 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0816096592053866; val_accuracy: 0.9762141719745223 

Epoch 49 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08156014224336405; val_accuracy: 0.9762141719745223 

Epoch 50 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08150037378072739; val_accuracy: 0.9761146496815286 

plots/no_subspace_training/MLP/2020-01-19 01:44:53/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.004
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1200201615074258; val_accuracy: 0.9669585987261147 

Epoch 12 start
The current lr is: 0.004
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.89
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11765742434817515; val_accuracy: 0.9681528662420382 

Epoch 13 start
The current lr is: 0.004
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11507135546606058; val_accuracy: 0.9686504777070064 

Epoch 14 start
The current lr is: 0.004
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11234037748947265; val_accuracy: 0.9689490445859873 

Epoch 15 start
The current lr is: 0.004
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11019454782555817; val_accuracy: 0.9694466560509554 

Epoch 16 start
The current lr is: 0.004
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10909820912749904; val_accuracy: 0.9700437898089171 

Epoch 17 start
The current lr is: 0.004
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10654933947571524; val_accuracy: 0.9699442675159236 

Epoch 18 start
The current lr is: 0.004
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.104574341967607; val_accuracy: 0.9708399681528662 

Epoch 19 start
The current lr is: 0.004
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10430746912291855; val_accuracy: 0.9711385350318471 

Epoch 20 start
The current lr is: 0.004
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10306124666788775; val_accuracy: 0.9707404458598726 

Epoch 21 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.08; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.15; acc: 0.92
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10076962674784053; val_accuracy: 0.9718351910828026 

Epoch 22 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10122959842537618; val_accuracy: 0.9717356687898089 

Epoch 23 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.09; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10008288948399247; val_accuracy: 0.9719347133757962 

Epoch 24 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09920246244236162; val_accuracy: 0.9718351910828026 

Epoch 25 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09868739172816277; val_accuracy: 0.9720342356687898 

Epoch 26 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09817422613216813; val_accuracy: 0.9722332802547771 

Epoch 27 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09854733822926594; val_accuracy: 0.9720342356687898 

Epoch 28 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.89
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0976154567187379; val_accuracy: 0.9721337579617835 

Epoch 29 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09688832160014256; val_accuracy: 0.9726313694267515 

Epoch 30 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09679277917476976; val_accuracy: 0.9729299363057324 

Epoch 31 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09618158697797234; val_accuracy: 0.9728304140127388 

Epoch 32 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0958729562628421; val_accuracy: 0.9730294585987261 

Epoch 33 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.095812813040747; val_accuracy: 0.9730294585987261 

Epoch 34 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0955518787596256; val_accuracy: 0.9732285031847133 

Epoch 35 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09558945770855921; val_accuracy: 0.9729299363057324 

Epoch 36 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09526801028638889; val_accuracy: 0.9729299363057324 

Epoch 37 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.91
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09496129190276383; val_accuracy: 0.9729299363057324 

Epoch 38 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09498914613560507; val_accuracy: 0.9731289808917197 

Epoch 39 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.95
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09474182356694702; val_accuracy: 0.9734275477707006 

Epoch 40 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0945160672732979; val_accuracy: 0.9732285031847133 

Epoch 41 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09445981334918624; val_accuracy: 0.9731289808917197 

Epoch 42 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09438178512700804; val_accuracy: 0.9732285031847133 

Epoch 43 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0943960286439604; val_accuracy: 0.9732285031847133 

Epoch 44 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09440098726635526; val_accuracy: 0.973328025477707 

Epoch 45 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09421663975734619; val_accuracy: 0.9731289808917197 

Epoch 46 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09419625435190596; val_accuracy: 0.9732285031847133 

Epoch 47 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.92
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09411455313586126; val_accuracy: 0.9732285031847133 

Epoch 48 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09401145112362637; val_accuracy: 0.9732285031847133 

Epoch 49 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0939705348603285; val_accuracy: 0.9732285031847133 

Epoch 50 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09389876524449155; val_accuracy: 0.9732285031847133 

plots/no_subspace_training/MLP/2020-01-19 01:48:25/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.004
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17394135270718555; val_accuracy: 0.9509355095541401 

Epoch 7 start
The current lr is: 0.004
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1687155393231067; val_accuracy: 0.9534235668789809 

Epoch 8 start
The current lr is: 0.004
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.09; acc: 1.0
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.07; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.08; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1605517343398492; val_accuracy: 0.9560111464968153 

Epoch 9 start
The current lr is: 0.004
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15598706844126342; val_accuracy: 0.9568073248407644 

Epoch 10 start
The current lr is: 0.004
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.14994015506688196; val_accuracy: 0.9592953821656051 

Epoch 11 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.89
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.147579415517438; val_accuracy: 0.9590963375796179 

Epoch 12 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1461993568831948; val_accuracy: 0.9596934713375797 

Epoch 13 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1444689705967903; val_accuracy: 0.9607882165605095 

Epoch 14 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1422767260936415; val_accuracy: 0.9608877388535032 

Epoch 15 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.14045423402148446; val_accuracy: 0.9615843949044586 

Epoch 16 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.08; acc: 1.0
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13984609893552816; val_accuracy: 0.9617834394904459 

Epoch 17 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.92
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13916288128798934; val_accuracy: 0.9613853503184714 

Epoch 18 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.08; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1386262251977708; val_accuracy: 0.9619824840764332 

Epoch 19 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13794516791014155; val_accuracy: 0.9621815286624203 

Epoch 20 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13738911171817475; val_accuracy: 0.9619824840764332 

Epoch 21 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.07; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13701320171451112; val_accuracy: 0.962281050955414 

Epoch 22 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13682557528565645; val_accuracy: 0.9620820063694268 

Epoch 23 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.19; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.08; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1365860920565523; val_accuracy: 0.9625796178343949 

Epoch 24 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13631782531263723; val_accuracy: 0.9626791401273885 

Epoch 25 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.1; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13607572533997; val_accuracy: 0.9624800955414012 

Epoch 26 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.07; acc: 1.0
Batch: 220; loss: 0.08; acc: 1.0
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13599375443189007; val_accuracy: 0.9624800955414012 

Epoch 27 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.1; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.08; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13593541854506086; val_accuracy: 0.9623805732484076 

Epoch 28 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13582273245237436; val_accuracy: 0.9628781847133758 

Epoch 29 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13572822767458145; val_accuracy: 0.9628781847133758 

Epoch 30 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.09; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.07; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13563503296511947; val_accuracy: 0.9628781847133758 

Epoch 31 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13559121937508795; val_accuracy: 0.9628781847133758 

Epoch 32 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13555220175226024; val_accuracy: 0.9629777070063694 

Epoch 33 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.21; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13551554987859574; val_accuracy: 0.9629777070063694 

Epoch 34 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13547920791586493; val_accuracy: 0.9629777070063694 

Epoch 35 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.23; acc: 0.97
Batch: 160; loss: 0.08; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1354428358423482; val_accuracy: 0.9629777070063694 

Epoch 36 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13542779720133277; val_accuracy: 0.9629777070063694 

Epoch 37 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13541233572800448; val_accuracy: 0.9629777070063694 

Epoch 38 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.08; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.06; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13539713891638314; val_accuracy: 0.9629777070063694 

Epoch 39 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13538156675210425; val_accuracy: 0.9628781847133758 

Epoch 40 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.09; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13536665517433433; val_accuracy: 0.9628781847133758 

Epoch 41 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.09; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1353603584134275; val_accuracy: 0.9628781847133758 

Epoch 42 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13535404124647188; val_accuracy: 0.9628781847133758 

Epoch 43 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13534777881992852; val_accuracy: 0.9627786624203821 

Epoch 44 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13534235109569162; val_accuracy: 0.9628781847133758 

Epoch 45 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1353366044676228; val_accuracy: 0.9629777070063694 

Epoch 46 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.08; acc: 1.0
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13533421513286364; val_accuracy: 0.9629777070063694 

Epoch 47 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13533183531301796; val_accuracy: 0.9629777070063694 

Epoch 48 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.07; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13532919315680578; val_accuracy: 0.9629777070063694 

Epoch 49 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13532682019434158; val_accuracy: 0.9629777070063694 

Epoch 50 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13532445176391844; val_accuracy: 0.9629777070063694 

plots/no_subspace_training/MLP/2020-01-19 01:52:00/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.91
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12117072576834897; val_accuracy: 0.9671576433121019 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11298768466730026; val_accuracy: 0.96875 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10762371644852267; val_accuracy: 0.9697452229299363 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10281171815790188; val_accuracy: 0.9701433121019108 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09964878603246562; val_accuracy: 0.9707404458598726 

Epoch 16 start
The current lr is: 0.002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09742210022393305; val_accuracy: 0.9730294585987261 

Epoch 17 start
The current lr is: 0.002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0967708667562266; val_accuracy: 0.972531847133758 

Epoch 18 start
The current lr is: 0.002
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0958463050852156; val_accuracy: 0.9729299363057324 

Epoch 19 start
The current lr is: 0.002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09587273549786798; val_accuracy: 0.9727308917197452 

Epoch 20 start
The current lr is: 0.002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09540889492839764; val_accuracy: 0.9730294585987261 

Epoch 21 start
The current lr is: 0.002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09416277374431586; val_accuracy: 0.9730294585987261 

Epoch 22 start
The current lr is: 0.002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09498442438947167; val_accuracy: 0.9740246815286624 

Epoch 23 start
The current lr is: 0.002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.08; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09367797772880572; val_accuracy: 0.9735270700636943 

Epoch 24 start
The current lr is: 0.002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09277323208701839; val_accuracy: 0.9724323248407644 

Epoch 25 start
The current lr is: 0.002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09241709153458572; val_accuracy: 0.9731289808917197 

Epoch 26 start
The current lr is: 0.002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09185026559954995; val_accuracy: 0.9735270700636943 

Epoch 27 start
The current lr is: 0.002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09238278614297794; val_accuracy: 0.9736265923566879 

Epoch 28 start
The current lr is: 0.002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09140039187897543; val_accuracy: 0.9727308917197452 

Epoch 29 start
The current lr is: 0.002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09071122093280409; val_accuracy: 0.9731289808917197 

Epoch 30 start
The current lr is: 0.002
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09071416381722802; val_accuracy: 0.9734275477707006 

Epoch 31 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09007734890766204; val_accuracy: 0.9731289808917197 

Epoch 32 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.21; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08993009689032652; val_accuracy: 0.9735270700636943 

Epoch 33 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08987857626786658; val_accuracy: 0.9736265923566879 

Epoch 34 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08976865547952378; val_accuracy: 0.9738256369426752 

Epoch 35 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.94
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08976329454950466; val_accuracy: 0.9735270700636943 

Epoch 36 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08962471422496116; val_accuracy: 0.9736265923566879 

Epoch 37 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08945950267800859; val_accuracy: 0.9735270700636943 

Epoch 38 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08951404747689605; val_accuracy: 0.9742237261146497 

Epoch 39 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08937650563041116; val_accuracy: 0.9740246815286624 

Epoch 40 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08928046880918704; val_accuracy: 0.9741242038216561 

Epoch 41 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08920118713359924; val_accuracy: 0.9737261146496815 

Epoch 42 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08911234508179555; val_accuracy: 0.9742237261146497 

Epoch 43 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08916363407187401; val_accuracy: 0.9741242038216561 

Epoch 44 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08918343700326173; val_accuracy: 0.9739251592356688 

Epoch 45 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0889048121964476; val_accuracy: 0.9742237261146497 

Epoch 46 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08890999995978774; val_accuracy: 0.9743232484076433 

Epoch 47 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08890535086867915; val_accuracy: 0.9740246815286624 

Epoch 48 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08888460444227146; val_accuracy: 0.9741242038216561 

Epoch 49 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08887726367469047; val_accuracy: 0.9740246815286624 

Epoch 50 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0888602828761195; val_accuracy: 0.9740246815286624 

plots/no_subspace_training/MLP/2020-01-19 01:55:30/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12036229212098061; val_accuracy: 0.9671576433121019 

Epoch 12 start
The current lr is: 0.002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.89
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11938016906760301; val_accuracy: 0.9673566878980892 

Epoch 13 start
The current lr is: 0.002
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11820448581485232; val_accuracy: 0.9677547770700637 

Epoch 14 start
The current lr is: 0.002
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11650532994205785; val_accuracy: 0.9674562101910829 

Epoch 15 start
The current lr is: 0.002
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11528087945975315; val_accuracy: 0.9680533439490446 

Epoch 16 start
The current lr is: 0.002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11444460930433242; val_accuracy: 0.9686504777070064 

Epoch 17 start
The current lr is: 0.002
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11318692351412621; val_accuracy: 0.96875 

Epoch 18 start
The current lr is: 0.002
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1120066722724468; val_accuracy: 0.9689490445859873 

Epoch 19 start
The current lr is: 0.002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11142175329054237; val_accuracy: 0.9692476114649682 

Epoch 20 start
The current lr is: 0.002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11057983573739695; val_accuracy: 0.9686504777070064 

Epoch 21 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10970846665607896; val_accuracy: 0.9692476114649682 

Epoch 22 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10963026726037074; val_accuracy: 0.9691480891719745 

Epoch 23 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10943812373906943; val_accuracy: 0.9696457006369427 

Epoch 24 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1091526864915137; val_accuracy: 0.9695461783439491 

Epoch 25 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.94
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.07; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10894897530318066; val_accuracy: 0.9695461783439491 

Epoch 26 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10877996146868749; val_accuracy: 0.9697452229299363 

Epoch 27 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1087714029820102; val_accuracy: 0.9696457006369427 

Epoch 28 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.88
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10850697611073021; val_accuracy: 0.9699442675159236 

Epoch 29 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10825962556680296; val_accuracy: 0.9696457006369427 

Epoch 30 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.09; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.07; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10811995095603026; val_accuracy: 0.9694466560509554 

Epoch 31 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10805017202143456; val_accuracy: 0.9694466560509554 

Epoch 32 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10800068993951864; val_accuracy: 0.9696457006369427 

Epoch 33 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.06; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1079630545654874; val_accuracy: 0.9696457006369427 

Epoch 34 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1079272261470746; val_accuracy: 0.96984474522293 

Epoch 35 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.06; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10789343702850068; val_accuracy: 0.9697452229299363 

Epoch 36 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10785891763438844; val_accuracy: 0.9697452229299363 

Epoch 37 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.89
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10781027623422586; val_accuracy: 0.9696457006369427 

Epoch 38 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10778346529621986; val_accuracy: 0.9697452229299363 

Epoch 39 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10774104783584358; val_accuracy: 0.9696457006369427 

Epoch 40 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10770745226626943; val_accuracy: 0.9696457006369427 

Epoch 41 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10769907563070583; val_accuracy: 0.9696457006369427 

Epoch 42 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1076914884956779; val_accuracy: 0.9697452229299363 

Epoch 43 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.107684782759589; val_accuracy: 0.9697452229299363 

Epoch 44 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10768033381385408; val_accuracy: 0.9697452229299363 

Epoch 45 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.06; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1076738342737696; val_accuracy: 0.9697452229299363 

Epoch 46 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10766776364035667; val_accuracy: 0.96984474522293 

Epoch 47 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10766095611130357; val_accuracy: 0.96984474522293 

Epoch 48 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10765221508540165; val_accuracy: 0.96984474522293 

Epoch 49 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.07; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10764601584642557; val_accuracy: 0.96984474522293 

Epoch 50 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10763908417266645; val_accuracy: 0.96984474522293 

plots/no_subspace_training/MLP/2020-01-19 01:59:01/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17669283565442273; val_accuracy: 0.9499402866242038 

Epoch 7 start
The current lr is: 0.002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17366222636240303; val_accuracy: 0.9523288216560509 

Epoch 8 start
The current lr is: 0.002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.1; acc: 1.0
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.08; acc: 1.0
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16943226856695617; val_accuracy: 0.9534235668789809 

Epoch 9 start
The current lr is: 0.002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16652370464934665; val_accuracy: 0.9535230891719745 

Epoch 10 start
The current lr is: 0.002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.08; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16325617023998765; val_accuracy: 0.9547173566878981 

Epoch 11 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.07; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16239409907987923; val_accuracy: 0.9553144904458599 

Epoch 12 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16187535878864062; val_accuracy: 0.9554140127388535 

Epoch 13 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16133045590227577; val_accuracy: 0.9556130573248408 

Epoch 14 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.09; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16064776900182864; val_accuracy: 0.9557125796178344 

Epoch 15 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16007603149695002; val_accuracy: 0.9560111464968153 

Epoch 16 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.08; acc: 1.0
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15993775417850276; val_accuracy: 0.955812101910828 

Epoch 17 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1598101621553017; val_accuracy: 0.9559116242038217 

Epoch 18 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1596993141018661; val_accuracy: 0.9559116242038217 

Epoch 19 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.92
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15957568438759276; val_accuracy: 0.9559116242038217 

Epoch 20 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15946518992827197; val_accuracy: 0.9559116242038217 

Epoch 21 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.08; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15944095178964032; val_accuracy: 0.9559116242038217 

Epoch 22 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.98
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15941822317660234; val_accuracy: 0.9559116242038217 

Epoch 23 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15939467594881726; val_accuracy: 0.9559116242038217 

Epoch 24 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.98
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15937050984828335; val_accuracy: 0.9559116242038217 

Epoch 25 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.14; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.08; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15934656249584667; val_accuracy: 0.9559116242038217 

Epoch 26 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1593420153163421; val_accuracy: 0.9559116242038217 

Epoch 27 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.159337852007834; val_accuracy: 0.9559116242038217 

Epoch 28 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15933329201522906; val_accuracy: 0.9559116242038217 

Epoch 29 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15932870744045374; val_accuracy: 0.9559116242038217 

Epoch 30 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 1.0
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.159324259040462; val_accuracy: 0.9559116242038217 

Epoch 31 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.1; acc: 1.0
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15932341280636514; val_accuracy: 0.9559116242038217 

Epoch 32 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15932257497196745; val_accuracy: 0.9559116242038217 

Epoch 33 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.23; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.08; acc: 1.0
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15932171820265473; val_accuracy: 0.9559116242038217 

Epoch 34 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15932092158372996; val_accuracy: 0.9559116242038217 

Epoch 35 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.12; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15932011072802696; val_accuracy: 0.9559116242038217 

Epoch 36 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.08; acc: 1.0
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.08; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1593199934881584; val_accuracy: 0.9559116242038217 

Epoch 37 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931988051932328; val_accuracy: 0.9559116242038217 

Epoch 38 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.88
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.09; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931977955683782; val_accuracy: 0.9559116242038217 

Epoch 39 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1593196892007521; val_accuracy: 0.9559116242038217 

Epoch 40 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.08; acc: 1.0
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931959466854478; val_accuracy: 0.9559116242038217 

Epoch 41 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.07; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.89
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931959091952652; val_accuracy: 0.9559116242038217 

Epoch 42 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931958183171643; val_accuracy: 0.9559116242038217 

Epoch 43 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931957775050667; val_accuracy: 0.9559116242038217 

Epoch 44 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1593195753065264; val_accuracy: 0.9559116242038217 

Epoch 45 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.89
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931956963554308; val_accuracy: 0.9559116242038217 

Epoch 46 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931956790340174; val_accuracy: 0.9559116242038217 

Epoch 47 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1593195676661221; val_accuracy: 0.9559116242038217 

Epoch 48 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1593195685915127; val_accuracy: 0.9559116242038217 

Epoch 49 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931956920843976; val_accuracy: 0.9559116242038217 

Epoch 50 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.08; acc: 1.0
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15931956768985006; val_accuracy: 0.9559116242038217 

plots/no_subspace_training/MLP/2020-01-19 02:02:40/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.91
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12117072576834897; val_accuracy: 0.9671576433121019 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11298768466730026; val_accuracy: 0.96875 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10762371644852267; val_accuracy: 0.9697452229299363 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10281171815790188; val_accuracy: 0.9701433121019108 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09964878603246562; val_accuracy: 0.9707404458598726 

Epoch 16 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09759687599103162; val_accuracy: 0.9728304140127388 

Epoch 17 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09716391765103219; val_accuracy: 0.9726313694267515 

Epoch 18 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09652873959131301; val_accuracy: 0.9730294585987261 

Epoch 19 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09642283291004267; val_accuracy: 0.9727308917197452 

Epoch 20 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.096133043431932; val_accuracy: 0.9732285031847133 

Epoch 21 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.07; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0953205633362767; val_accuracy: 0.9728304140127388 

Epoch 22 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09569242321381903; val_accuracy: 0.973328025477707 

Epoch 23 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09502041387330194; val_accuracy: 0.9732285031847133 

Epoch 24 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09432431630742778; val_accuracy: 0.972531847133758 

Epoch 25 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09402582754090333; val_accuracy: 0.973328025477707 

Epoch 26 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0936609485489168; val_accuracy: 0.9736265923566879 

Epoch 27 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09396389155251206; val_accuracy: 0.9736265923566879 

Epoch 28 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.91
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09339070925192469; val_accuracy: 0.972531847133758 

Epoch 29 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09281390751148486; val_accuracy: 0.9729299363057324 

Epoch 30 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09282945606643987; val_accuracy: 0.973328025477707 

Epoch 31 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09251128160839628; val_accuracy: 0.9734275477707006 

Epoch 32 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.21; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09241658983052156; val_accuracy: 0.9734275477707006 

Epoch 33 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09236328159073356; val_accuracy: 0.973328025477707 

Epoch 34 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09231789436215049; val_accuracy: 0.973328025477707 

Epoch 35 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09228842873956747; val_accuracy: 0.9734275477707006 

Epoch 36 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0922433112267476; val_accuracy: 0.9734275477707006 

Epoch 37 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.92
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09216768802351254; val_accuracy: 0.9734275477707006 

Epoch 38 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09217205036218; val_accuracy: 0.9735270700636943 

Epoch 39 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09211764527354271; val_accuracy: 0.9734275477707006 

Epoch 40 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09207933127975008; val_accuracy: 0.9735270700636943 

Epoch 41 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09202249032581687; val_accuracy: 0.9734275477707006 

Epoch 42 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0919837383375426; val_accuracy: 0.973328025477707 

Epoch 43 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09198616241004057; val_accuracy: 0.973328025477707 

Epoch 44 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09200258235073393; val_accuracy: 0.973328025477707 

Epoch 45 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09190356795480296; val_accuracy: 0.9734275477707006 

Epoch 46 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09190231303025963; val_accuracy: 0.973328025477707 

Epoch 47 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.92
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09189930233131548; val_accuracy: 0.9734275477707006 

Epoch 48 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09189347334348472; val_accuracy: 0.9734275477707006 

Epoch 49 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.08; acc: 1.0
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09189080280862796; val_accuracy: 0.9735270700636943 

Epoch 50 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09188667846143625; val_accuracy: 0.9735270700636943 

plots/no_subspace_training/MLP/2020-01-19 02:06:09/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12074199129062094; val_accuracy: 0.9669585987261147 

Epoch 12 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.89
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12004063204879974; val_accuracy: 0.966859076433121 

Epoch 13 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11934093390680423; val_accuracy: 0.9675557324840764 

Epoch 14 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11817528670475741; val_accuracy: 0.9673566878980892 

Epoch 15 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11739097637640443; val_accuracy: 0.9678542993630573 

Epoch 16 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11667236113908944; val_accuracy: 0.9678542993630573 

Epoch 17 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11585902847397099; val_accuracy: 0.9683519108280255 

Epoch 18 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.08; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1150720197541319; val_accuracy: 0.9678542993630573 

Epoch 19 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11449270522233787; val_accuracy: 0.9685509554140127 

Epoch 20 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11390720203423955; val_accuracy: 0.9681528662420382 

Epoch 21 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.06; acc: 1.0
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11351855853750448; val_accuracy: 0.9680533439490446 

Epoch 22 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11343211277275328; val_accuracy: 0.9680533439490446 

Epoch 23 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11333410607971203; val_accuracy: 0.9682523885350318 

Epoch 24 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.92
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11322806808789065; val_accuracy: 0.9682523885350318 

Epoch 25 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.94
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11312927486030919; val_accuracy: 0.9680533439490446 

Epoch 26 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11304358497356913; val_accuracy: 0.9681528662420382 

Epoch 27 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11300937677170061; val_accuracy: 0.9682523885350318 

Epoch 28 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.09; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.88
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11289102966143827; val_accuracy: 0.9683519108280255 

Epoch 29 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11279465571330612; val_accuracy: 0.9683519108280255 

Epoch 30 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.1; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11270118879664476; val_accuracy: 0.9682523885350318 

Epoch 31 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11268769299528401; val_accuracy: 0.9682523885350318 

Epoch 32 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11267450845735089; val_accuracy: 0.9682523885350318 

Epoch 33 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11266156974112153; val_accuracy: 0.9682523885350318 

Epoch 34 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1126494966684633; val_accuracy: 0.9682523885350318 

Epoch 35 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11263754104922531; val_accuracy: 0.9682523885350318 

Epoch 36 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11262574431243216; val_accuracy: 0.9682523885350318 

Epoch 37 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.89
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.89
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11261296053980566; val_accuracy: 0.9683519108280255 

Epoch 38 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11260129651351339; val_accuracy: 0.9683519108280255 

Epoch 39 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11258855950870332; val_accuracy: 0.9683519108280255 

Epoch 40 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11257684287751556; val_accuracy: 0.9683519108280255 

Epoch 41 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11257506121590638; val_accuracy: 0.9683519108280255 

Epoch 42 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11257339518066424; val_accuracy: 0.9683519108280255 

Epoch 43 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.92
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11257177826230692; val_accuracy: 0.9683519108280255 

Epoch 44 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11257050736884402; val_accuracy: 0.9683519108280255 

Epoch 45 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11256907156603352; val_accuracy: 0.9683519108280255 

Epoch 46 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11256762658050105; val_accuracy: 0.9683519108280255 

Epoch 47 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1125661038384316; val_accuracy: 0.9683519108280255 

Epoch 48 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11256429394054565; val_accuracy: 0.9683519108280255 

Epoch 49 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.14; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11256287007290088; val_accuracy: 0.9683519108280255 

Epoch 50 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11256137241128904; val_accuracy: 0.9683519108280255 

plots/no_subspace_training/MLP/2020-01-19 02:09:39/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17774263940229537; val_accuracy: 0.9496417197452229 

Epoch 7 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1756834537265407; val_accuracy: 0.9514331210191083 

Epoch 8 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.1; acc: 1.0
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.08; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17281767851702726; val_accuracy: 0.9521297770700637 

Epoch 9 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1708035472139811; val_accuracy: 0.95203025477707 

Epoch 10 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16857952311350283; val_accuracy: 0.9533240445859873 

Epoch 11 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.26; acc: 0.86
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.07; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1681754707720629; val_accuracy: 0.9535230891719745 

Epoch 12 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16792713625320962; val_accuracy: 0.9533240445859873 

Epoch 13 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

slurmstepd: error: _is_a_lwp: open() /proc/26339/status failed: No such file or directory
Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16766254736739358; val_accuracy: 0.9536226114649682 

Epoch 14 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.09; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1673709293649455; val_accuracy: 0.9534235668789809 

Epoch 15 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1671039634354555; val_accuracy: 0.9538216560509554 

Epoch 16 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16706802292614226; val_accuracy: 0.9538216560509554 

Epoch 17 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16703093455285783; val_accuracy: 0.9538216560509554 

Epoch 18 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16699532182163493; val_accuracy: 0.9538216560509554 

Epoch 19 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.07; acc: 1.0
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.98
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1669591551848278; val_accuracy: 0.9537221337579618 

Epoch 20 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16692403369363706; val_accuracy: 0.9538216560509554 

Epoch 21 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.09; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16691936481340675; val_accuracy: 0.9538216560509554 

Epoch 22 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16691488691955614; val_accuracy: 0.9538216560509554 

Epoch 23 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16691025258724096; val_accuracy: 0.9538216560509554 

Epoch 24 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1669054402002863; val_accuracy: 0.9538216560509554 

Epoch 25 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.15; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16690066117484859; val_accuracy: 0.9538216560509554 

Epoch 26 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16690015095244548; val_accuracy: 0.9538216560509554 

Epoch 27 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.166899691294333; val_accuracy: 0.9538216560509554 

Epoch 28 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.13; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689919082412294; val_accuracy: 0.9538216560509554 

Epoch 29 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.08; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689869588252845; val_accuracy: 0.9538216560509554 

Epoch 30 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.09; acc: 1.0
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689820115448564; val_accuracy: 0.9538216560509554 

Epoch 31 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1668981802026937; val_accuracy: 0.9538216560509554 

Epoch 32 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.07; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689815481377254; val_accuracy: 0.9538216560509554 

Epoch 33 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689813621104901; val_accuracy: 0.9538216560509554 

Epoch 34 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689811912691518; val_accuracy: 0.9538216560509554 

Epoch 35 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810137839833; val_accuracy: 0.9538216560509554 

Epoch 36 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.08; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.08; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810268343633; val_accuracy: 0.9538216560509554 

Epoch 37 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1668981001682722; val_accuracy: 0.9538216560509554 

Epoch 38 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.88
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810237497282; val_accuracy: 0.9538216560509554 

Epoch 39 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810128348648; val_accuracy: 0.9538216560509554 

Epoch 40 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810137839833; val_accuracy: 0.9538216560509554 

Epoch 41 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810185295761; val_accuracy: 0.9538216560509554 

Epoch 42 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810204278133; val_accuracy: 0.9538216560509554 

Epoch 43 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810142585426; val_accuracy: 0.9538216560509554 

Epoch 44 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810092756702; val_accuracy: 0.9538216560509554 

Epoch 45 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.33; acc: 0.86
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810064283145; val_accuracy: 0.9538216560509554 

Epoch 46 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.08; acc: 1.0
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810064283145; val_accuracy: 0.9538216560509554 

Epoch 47 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810064283145; val_accuracy: 0.9538216560509554 

Epoch 48 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810064283145; val_accuracy: 0.9538216560509554 

Epoch 49 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.88
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810064283145; val_accuracy: 0.9538216560509554 

Epoch 50 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.09; acc: 1.0
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16689810064283145; val_accuracy: 0.9538216560509554 

plots/no_subspace_training/MLP/2020-01-19 02:13:12/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.91
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12117072576834897; val_accuracy: 0.9671576433121019 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11298768466730026; val_accuracy: 0.96875 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10762371644852267; val_accuracy: 0.9697452229299363 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10281171815790188; val_accuracy: 0.9701433121019108 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09964878603246562; val_accuracy: 0.9707404458598726 

Epoch 16 start
The current lr is: 0.001
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0977120492725995; val_accuracy: 0.9726313694267515 

Epoch 17 start
The current lr is: 0.001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09734375713167677; val_accuracy: 0.9726313694267515 

Epoch 18 start
The current lr is: 0.001
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09683179117407009; val_accuracy: 0.9729299363057324 

Epoch 19 start
The current lr is: 0.001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09668340593291695; val_accuracy: 0.9727308917197452 

Epoch 20 start
The current lr is: 0.001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09647571326346155; val_accuracy: 0.9731289808917197 

Epoch 21 start
The current lr is: 0.001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.07; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0958539727292243; val_accuracy: 0.9728304140127388 

Epoch 22 start
The current lr is: 0.001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09606267649466825; val_accuracy: 0.9732285031847133 

Epoch 23 start
The current lr is: 0.001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09562945918767315; val_accuracy: 0.9730294585987261 

Epoch 24 start
The current lr is: 0.001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09505739437926347; val_accuracy: 0.9724323248407644 

Epoch 25 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09479467100968027; val_accuracy: 0.9730294585987261 

Epoch 26 start
The current lr is: 0.001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.22; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09451245533148195; val_accuracy: 0.973328025477707 

Epoch 27 start
The current lr is: 0.001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09471565658214745; val_accuracy: 0.9734275477707006 

Epoch 28 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.91
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09430965100219295; val_accuracy: 0.9726313694267515 

Epoch 29 start
The current lr is: 0.001
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09380875100755388; val_accuracy: 0.9729299363057324 

Epoch 30 start
The current lr is: 0.001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09379939160719039; val_accuracy: 0.9731289808917197 

Epoch 31 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09362870426314651; val_accuracy: 0.9734275477707006 

Epoch 32 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09355537040503162; val_accuracy: 0.9734275477707006 

Epoch 33 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09351115201593964; val_accuracy: 0.9735270700636943 

Epoch 34 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09347738989978839; val_accuracy: 0.9735270700636943 

Epoch 35 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09345249024925718; val_accuracy: 0.9735270700636943 

Epoch 36 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09342556164427927; val_accuracy: 0.9735270700636943 

Epoch 37 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.91
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.92
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09338351782814712; val_accuracy: 0.9735270700636943 

Epoch 38 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09337416393267121; val_accuracy: 0.9735270700636943 

Epoch 39 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09334341887455837; val_accuracy: 0.9735270700636943 

Epoch 40 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09332019880793657; val_accuracy: 0.9735270700636943 

Epoch 41 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09328238620974455; val_accuracy: 0.9735270700636943 

Epoch 42 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09325616268120754; val_accuracy: 0.9735270700636943 

Epoch 43 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09325010391177645; val_accuracy: 0.9735270700636943 

Epoch 44 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.06; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09325712399592825; val_accuracy: 0.9735270700636943 

Epoch 45 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09321298806151007; val_accuracy: 0.9735270700636943 

Epoch 46 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.093211530405245; val_accuracy: 0.9735270700636943 

Epoch 47 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.92
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09320947093663702; val_accuracy: 0.9735270700636943 

Epoch 48 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09320635723460252; val_accuracy: 0.9735270700636943 

Epoch 49 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09320469440263547; val_accuracy: 0.9735270700636943 

Epoch 50 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09320254085265148; val_accuracy: 0.9735270700636943 

plots/no_subspace_training/MLP/2020-01-19 02:16:46/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16657718325591392; val_accuracy: 0.9533240445859873 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1583467859798556; val_accuracy: 0.956906847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 1.0
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13967546056600133; val_accuracy: 0.9621815286624203 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13335899009731164; val_accuracy: 0.9619824840764332 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12302515197806298; val_accuracy: 0.9667595541401274 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12097094972042521; val_accuracy: 0.9669585987261147 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12036200358894221; val_accuracy: 0.9672571656050956 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11982466650616591; val_accuracy: 0.9673566878980892 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11891286703906241; val_accuracy: 0.9673566878980892 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11832277396112491; val_accuracy: 0.9677547770700637 

Epoch 16 start
The current lr is: 0.001
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1176930339710348; val_accuracy: 0.96765525477707 

Epoch 17 start
The current lr is: 0.001
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1170772573655578; val_accuracy: 0.9681528662420382 

Epoch 18 start
The current lr is: 0.001
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.08; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11646722907283504; val_accuracy: 0.96765525477707 

Epoch 19 start
The current lr is: 0.001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11593906211245592; val_accuracy: 0.9679538216560509 

Epoch 20 start
The current lr is: 0.001
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11547600767415041; val_accuracy: 0.9679538216560509 

Epoch 21 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.06; acc: 1.0
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11525703442229587; val_accuracy: 0.9680533439490446 

Epoch 22 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11517850755696084; val_accuracy: 0.9681528662420382 

Epoch 23 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11510741584904634; val_accuracy: 0.9680533439490446 

Epoch 24 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.92
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11504169373185771; val_accuracy: 0.9680533439490446 

Epoch 25 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11497909435705775; val_accuracy: 0.9680533439490446 

Epoch 26 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11492449711937054; val_accuracy: 0.9680533439490446 

Epoch 27 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11489557261299935; val_accuracy: 0.9678542993630573 

Epoch 28 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.09; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.88
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11482916651352955; val_accuracy: 0.9680533439490446 

Epoch 29 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11477210496071798; val_accuracy: 0.9680533439490446 

Epoch 30 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11471325042805854; val_accuracy: 0.9680533439490446 

Epoch 31 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11470734501245675; val_accuracy: 0.9680533439490446 

Epoch 32 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11470135823366748; val_accuracy: 0.9680533439490446 

Epoch 33 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.18; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11469518497681162; val_accuracy: 0.9680533439490446 

Epoch 34 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1146893551110462; val_accuracy: 0.9680533439490446 

Epoch 35 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11468350144613321; val_accuracy: 0.9680533439490446 

Epoch 36 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11467774160158863; val_accuracy: 0.9680533439490446 

Epoch 37 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.89
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.89
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11467183212850504; val_accuracy: 0.9680533439490446 

Epoch 38 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.07; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11466596012187612; val_accuracy: 0.9680533439490446 

Epoch 39 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11465988011591753; val_accuracy: 0.9680533439490446 

Epoch 40 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1146541012178181; val_accuracy: 0.9680533439490446 

Epoch 41 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11465342155402633; val_accuracy: 0.9680533439490446 

Epoch 42 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11465279219351757; val_accuracy: 0.9680533439490446 

Epoch 43 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.92
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1146521648498857; val_accuracy: 0.9680533439490446 

Epoch 44 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11465166036964981; val_accuracy: 0.9680533439490446 

Epoch 45 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11465111201640907; val_accuracy: 0.9680533439490446 

Epoch 46 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11465052304089449; val_accuracy: 0.9680533439490446 

Epoch 47 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11464995226472806; val_accuracy: 0.9680533439490446 

Epoch 48 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11464924673745587; val_accuracy: 0.9680533439490446 

Epoch 49 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.14; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1146487062381711; val_accuracy: 0.9680533439490446 

Epoch 50 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11464813558064449; val_accuracy: 0.9680533439490446 

plots/no_subspace_training/MLP/2020-01-19 02:20:22/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.28
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.09; acc: 0.61
Batch: 80; loss: 2.04; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.58
Batch: 120; loss: 1.78; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.77
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 1.21; acc: 0.77
Batch: 200; loss: 1.05; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.65; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.79 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.349950318978091; val_accuracy: 0.9021695859872612 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2731309201639549; val_accuracy: 0.9224721337579618 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2299924182236954; val_accuracy: 0.9316281847133758 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21068593463415553; val_accuracy: 0.939390923566879 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.19070508302586853; val_accuracy: 0.9460589171974523 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1782118045268165; val_accuracy: 0.9494426751592356 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1766029661343356; val_accuracy: 0.951234076433121 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.1; acc: 1.0
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.08; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.08; acc: 1.0
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17432639064492694; val_accuracy: 0.9511345541401274 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17274296967087277; val_accuracy: 0.9517316878980892 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17097275818039656; val_accuracy: 0.9527269108280255 

Epoch 11 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.86
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.07; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17073453879755013; val_accuracy: 0.9530254777070064 

Epoch 12 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17057680723014151; val_accuracy: 0.9529259554140127 

Epoch 13 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17041401894893615; val_accuracy: 0.953125 

Epoch 14 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17024181889975146; val_accuracy: 0.953125 

Epoch 15 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17007588362617856; val_accuracy: 0.9532245222929936 

Epoch 16 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1700597988192443; val_accuracy: 0.9532245222929936 

Epoch 17 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1700429485719295; val_accuracy: 0.9532245222929936 

Epoch 18 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17002640244592526; val_accuracy: 0.9532245222929936 

Epoch 19 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.07; acc: 1.0
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.98
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17000972809400527; val_accuracy: 0.9532245222929936 

Epoch 20 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16999325839577206; val_accuracy: 0.9532245222929936 

Epoch 21 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.09; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1699916505414969; val_accuracy: 0.9532245222929936 

Epoch 22 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16999009873267193; val_accuracy: 0.9532245222929936 

Epoch 23 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998850276610653; val_accuracy: 0.9532245222929936 

Epoch 24 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998683312421392; val_accuracy: 0.9532245222929936 

Epoch 25 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.15; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1699851613230766; val_accuracy: 0.9532245222929936 

Epoch 26 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.98
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998508812230864; val_accuracy: 0.9532245222929936 

Epoch 27 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.08; acc: 1.0
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998501942985378; val_accuracy: 0.9532245222929936 

Epoch 28 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998494599180616; val_accuracy: 0.9532245222929936 

Epoch 29 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998487604176923; val_accuracy: 0.9532245222929936 

Epoch 30 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.1; acc: 1.0
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1699847881059358; val_accuracy: 0.9532245222929936 

Epoch 31 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998479263797686; val_accuracy: 0.9532245222929936 

Epoch 32 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.07; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478718054522; val_accuracy: 0.9532245222929936 

Epoch 33 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478663480207; val_accuracy: 0.9532245222929936 

Epoch 34 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478620769872; val_accuracy: 0.9532245222929936 

Epoch 35 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478506875644; val_accuracy: 0.9532245222929936 

Epoch 36 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.08; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.09; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478597041908; val_accuracy: 0.9532245222929936 

Epoch 37 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478578059537; val_accuracy: 0.9532245222929936 

Epoch 38 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.88
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478502130052; val_accuracy: 0.9532245222929936 

Epoch 39 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478416709384; val_accuracy: 0.9532245222929936 

Epoch 40 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.22; acc: 0.89
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.37; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478383490234; val_accuracy: 0.9532245222929936 

Epoch 41 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478383490234; val_accuracy: 0.9532245222929936 

Epoch 42 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478402472605; val_accuracy: 0.9532245222929936 

Epoch 43 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478402472605; val_accuracy: 0.9532245222929936 

Epoch 44 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.89
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478397727013; val_accuracy: 0.9532245222929936 

Epoch 45 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478397727013; val_accuracy: 0.9532245222929936 

Epoch 46 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.08; acc: 1.0
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478397727013; val_accuracy: 0.9532245222929936 

Epoch 47 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478397727013; val_accuracy: 0.9532245222929936 

Epoch 48 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478397727013; val_accuracy: 0.9532245222929936 

Epoch 49 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.86
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478397727013; val_accuracy: 0.9532245222929936 

Epoch 50 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.31; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.16998478397727013; val_accuracy: 0.9532245222929936 

plots/no_subspace_training/MLP/2020-01-19 02:23:51/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.325642324082411; val_accuracy: 0.908140923566879 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.4; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3168912383306558; val_accuracy: 0.9098328025477707 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3087596929851611; val_accuracy: 0.9117237261146497 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3008695425596207; val_accuracy: 0.9144108280254777 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2940892424837799; val_accuracy: 0.9157046178343949 

Epoch 16 start
The current lr is: 0.0008
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.73; acc: 0.78
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.28936336597629414; val_accuracy: 0.9168988853503185 

Epoch 17 start
The current lr is: 0.0008
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2843194865402143; val_accuracy: 0.9184912420382165 

Epoch 18 start
The current lr is: 0.0008
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.84
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2803588851716868; val_accuracy: 0.9190883757961783 

Epoch 19 start
The current lr is: 0.0008
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2760523675353664; val_accuracy: 0.9209792993630573 

Epoch 20 start
The current lr is: 0.0008
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.26; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27213032939916204; val_accuracy: 0.921875 

Epoch 21 start
The current lr is: 0.0008
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.2; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.26875113463325867; val_accuracy: 0.9226711783439491 

Epoch 22 start
The current lr is: 0.0008
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.08; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.26535062446810637; val_accuracy: 0.9239649681528662 

Epoch 23 start
The current lr is: 0.0008
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.86
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.84
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2617006113005292; val_accuracy: 0.9250597133757962 

Epoch 24 start
The current lr is: 0.0008
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25813730454938427; val_accuracy: 0.924562101910828 

Epoch 25 start
The current lr is: 0.0008
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.81
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2549644516436917; val_accuracy: 0.925656847133758 

Epoch 26 start
The current lr is: 0.0008
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2521273572543624; val_accuracy: 0.9267515923566879 

Epoch 27 start
The current lr is: 0.0008
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.63; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2495094619121901; val_accuracy: 0.9268511146496815 

Epoch 28 start
The current lr is: 0.0008
Batch: 0; loss: 0.23; acc: 0.86
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.8
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.24630525338042314; val_accuracy: 0.9284434713375797 

Epoch 29 start
The current lr is: 0.0008
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.81
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24356667678447286; val_accuracy: 0.9299363057324841 

Epoch 30 start
The current lr is: 0.0008
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.24068928355718874; val_accuracy: 0.9296377388535032 

Epoch 31 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23849964739790389; val_accuracy: 0.9303343949044586 

Epoch 32 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2361315468883818; val_accuracy: 0.931031050955414 

Epoch 33 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.13; acc: 1.0
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23439340879488144; val_accuracy: 0.9313296178343949 

Epoch 34 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23219621691639256; val_accuracy: 0.9323248407643312 

Epoch 35 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23053551481882478; val_accuracy: 0.9327229299363057 

Epoch 36 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22832989042541782; val_accuracy: 0.9331210191082803 

Epoch 37 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.7; acc: 0.84
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22624593714524985; val_accuracy: 0.9341162420382165 

Epoch 38 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.44; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22466931522936578; val_accuracy: 0.9340167197452229 

Epoch 39 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22261974704303558; val_accuracy: 0.9344148089171974 

Epoch 40 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22087956167710054; val_accuracy: 0.9346138535031847 

Epoch 41 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21920216942478896; val_accuracy: 0.9357085987261147 

Epoch 42 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21732367506357514; val_accuracy: 0.93640525477707 

Epoch 43 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.84
Batch: 320; loss: 0.22; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.08; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.08; acc: 1.0
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21596761660021582; val_accuracy: 0.9366042993630573 

Epoch 44 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21401711826206773; val_accuracy: 0.9372014331210191 

Epoch 45 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21244141020497698; val_accuracy: 0.9378980891719745 

Epoch 46 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.1; acc: 1.0
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21111670793716314; val_accuracy: 0.9380971337579618 

Epoch 47 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.88
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.09; acc: 1.0
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20974531069777574; val_accuracy: 0.93859474522293 

Epoch 48 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20815268080610377; val_accuracy: 0.9390923566878981 

Epoch 49 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20695141828648603; val_accuracy: 0.9394904458598726 

Epoch 50 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20579622208977202; val_accuracy: 0.9404856687898089 

plots/no_subspace_training/MLP/2020-01-19 02:27:21/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.0008
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3277234455012971; val_accuracy: 0.9079418789808917 

Epoch 12 start
The current lr is: 0.0008
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32028048751270694; val_accuracy: 0.9090366242038217 

Epoch 13 start
The current lr is: 0.0008
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.51; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3134314382247105; val_accuracy: 0.9098328025477707 

Epoch 14 start
The current lr is: 0.0008
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.30651754772017714; val_accuracy: 0.9134156050955414 

Epoch 15 start
The current lr is: 0.0008
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.30067996671245356; val_accuracy: 0.9138136942675159 

Epoch 16 start
The current lr is: 0.0008
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.78
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29551189976513004; val_accuracy: 0.9149084394904459 

Epoch 17 start
The current lr is: 0.0008
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29008118537770716; val_accuracy: 0.9169984076433121 

Epoch 18 start
The current lr is: 0.0008
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2857791327272251; val_accuracy: 0.9174960191082803 

Epoch 19 start
The current lr is: 0.0008
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2811783108931438; val_accuracy: 0.9193869426751592 

Epoch 20 start
The current lr is: 0.0008
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.27; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2770130290726947; val_accuracy: 0.919984076433121 

Epoch 21 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.2; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.83
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2740818881162792; val_accuracy: 0.9210788216560509 

Epoch 22 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2711957009971901; val_accuracy: 0.9223726114649682 

Epoch 23 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.84
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2681196875110933; val_accuracy: 0.9226711783439491 

Epoch 24 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26500550265999356; val_accuracy: 0.9224721337579618 

Epoch 25 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.81
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26228779293359467; val_accuracy: 0.9244625796178344 

Epoch 26 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2598023735413885; val_accuracy: 0.924562101910828 

Epoch 27 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.86
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2574957980044708; val_accuracy: 0.9248606687898089 

Epoch 28 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.24; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.8
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2547491910825869; val_accuracy: 0.9266520700636943 

Epoch 29 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.86
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.81
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25238784817848237; val_accuracy: 0.9270501592356688 

Epoch 30 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24986704733151538; val_accuracy: 0.9273487261146497 

Epoch 31 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.49; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2478973554672709; val_accuracy: 0.9282444267515924 

Epoch 32 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2458603101646065; val_accuracy: 0.9282444267515924 

Epoch 33 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.98
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24434783179194305; val_accuracy: 0.9287420382165605 

Epoch 34 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.24238023899827793; val_accuracy: 0.9292396496815286 

Epoch 35 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.86
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.24091409452876467; val_accuracy: 0.9301353503184714 

Epoch 36 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23894969452243703; val_accuracy: 0.9304339171974523 

Epoch 37 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.72; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.08; acc: 1.0
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.47; acc: 0.81
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23709928239606748; val_accuracy: 0.9306329617834395 

Epoch 38 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.45; acc: 0.81
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.45; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23572734114565666; val_accuracy: 0.9308320063694268 

Epoch 39 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2338673559960666; val_accuracy: 0.9317277070063694 

Epoch 40 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.98
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2323667950406196; val_accuracy: 0.9324243630573248 

Epoch 41 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.22; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23110649464236702; val_accuracy: 0.9321257961783439 

Epoch 42 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22977292328883128; val_accuracy: 0.932921974522293 

Epoch 43 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.84
Batch: 320; loss: 0.24; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.09; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.09; acc: 1.0
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22872175691518815; val_accuracy: 0.9331210191082803 

Epoch 44 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.16; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22737484166671515; val_accuracy: 0.9331210191082803 

Epoch 45 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2262310842704621; val_accuracy: 0.9331210191082803 

Epoch 46 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.11; acc: 1.0
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2250678167458932; val_accuracy: 0.9342157643312102 

Epoch 47 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.88
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.11; acc: 1.0
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22387954258140486; val_accuracy: 0.9345143312101911 

Epoch 48 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.81
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.08; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2224375728968602; val_accuracy: 0.9345143312101911 

Epoch 49 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.86
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.86
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2213546289902204; val_accuracy: 0.934812898089172 

Epoch 50 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.07; acc: 1.0
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22030414441588578; val_accuracy: 0.9349124203821656 

plots/no_subspace_training/MLP/2020-01-19 02:30:56/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.0008
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44227673018434244; val_accuracy: 0.8872412420382165 

Epoch 7 start
The current lr is: 0.0008
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.81
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.42; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.40818683527837135; val_accuracy: 0.8929140127388535 

Epoch 8 start
The current lr is: 0.0008
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.75
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.8
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.8
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3839907314462267; val_accuracy: 0.8967953821656051 

Epoch 9 start
The current lr is: 0.0008
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3658404681997694; val_accuracy: 0.9002786624203821 

Epoch 10 start
The current lr is: 0.0008
slurmstepd: error: _is_a_lwp: open() /proc/47179/status failed: No such file or directory
slurmstepd: error: _is_a_lwp: open() /proc/47180/status failed: No such file or directory
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.67; acc: 0.81
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3512943528924778; val_accuracy: 0.9039609872611465 

Epoch 11 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.34179871254096367; val_accuracy: 0.9056528662420382 

Epoch 12 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.43; acc: 0.81
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.75
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33406589531404957; val_accuracy: 0.9068471337579618 

Epoch 13 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3271945319642687; val_accuracy: 0.90734474522293 

Epoch 14 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3203114294891904; val_accuracy: 0.9095342356687898 

Epoch 15 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3145607158921327; val_accuracy: 0.9103304140127388 

Epoch 16 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.310196103516278; val_accuracy: 0.911922770700637 

Epoch 17 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.81
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.30597047071168376; val_accuracy: 0.913515127388535 

Epoch 18 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3024316740453623; val_accuracy: 0.9136146496815286 

Epoch 19 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29872791862981335; val_accuracy: 0.9144108280254777 

Epoch 20 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.98
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2953330685568463; val_accuracy: 0.9151074840764332 

Epoch 21 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.83
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29287296998652684; val_accuracy: 0.9163017515923567 

Epoch 22 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.290496414539161; val_accuracy: 0.916202229299363 

Epoch 23 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.84
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2880456167136788; val_accuracy: 0.9174960191082803 

Epoch 24 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2854784918818504; val_accuracy: 0.9174960191082803 

Epoch 25 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.81
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2832184300111358; val_accuracy: 0.9183917197452229 

Epoch 26 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2815004814012795; val_accuracy: 0.9188893312101911 

Epoch 27 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2799159339658774; val_accuracy: 0.9190883757961783 

Epoch 28 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.28; acc: 0.84
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27817891599835864; val_accuracy: 0.9202826433121019 

Epoch 29 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27663504982450204; val_accuracy: 0.9205812101910829 

Epoch 30 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2749435457928925; val_accuracy: 0.9206807324840764 

Epoch 31 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2735964321786431; val_accuracy: 0.9216759554140127 

Epoch 32 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.98
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2722714512970797; val_accuracy: 0.9217754777070064 

Epoch 33 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27120294165649234; val_accuracy: 0.9216759554140127 

Epoch 34 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2698708568598814; val_accuracy: 0.9221735668789809 

Epoch 35 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26882581151784607; val_accuracy: 0.9221735668789809 

Epoch 36 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.86
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2677398571732697; val_accuracy: 0.9228702229299363 

Epoch 37 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.76; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.1; acc: 1.0
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.52; acc: 0.8
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2667332791076724; val_accuracy: 0.9226711783439491 

Epoch 38 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.51; acc: 0.8
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26593999941922297; val_accuracy: 0.9230692675159236 

Epoch 39 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26490879794404765; val_accuracy: 0.9234673566878981 

Epoch 40 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2641105251802001; val_accuracy: 0.923765923566879 

Epoch 41 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26336384559892545; val_accuracy: 0.9238654458598726 

Epoch 42 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.81
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.09; acc: 1.0
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.84
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2626162638092876; val_accuracy: 0.9241640127388535 

Epoch 43 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.83
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.11; acc: 1.0
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2619450798459873; val_accuracy: 0.9244625796178344 

Epoch 44 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26120685539238014; val_accuracy: 0.9243630573248408 

Epoch 45 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2605532564127901; val_accuracy: 0.9246616242038217 

Epoch 46 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25997773031140586; val_accuracy: 0.9246616242038217 

Epoch 47 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.88
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2594502709426318; val_accuracy: 0.9247611464968153 

Epoch 48 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.29; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.81
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.1; acc: 1.0
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2588093862364626; val_accuracy: 0.9250597133757962 

Epoch 49 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.83
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25828069697614686; val_accuracy: 0.9250597133757962 

Epoch 50 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.15; acc: 0.98
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25776665743179383; val_accuracy: 0.9251592356687898 

plots/no_subspace_training/MLP/2020-01-19 02:34:24/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.325642324082411; val_accuracy: 0.908140923566879 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.4; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3168912383306558; val_accuracy: 0.9098328025477707 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3087596929851611; val_accuracy: 0.9117237261146497 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3008695425596207; val_accuracy: 0.9144108280254777 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2940892424837799; val_accuracy: 0.9157046178343949 

Epoch 16 start
The current lr is: 0.0004
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2914183899570423; val_accuracy: 0.916202229299363 

Epoch 17 start
The current lr is: 0.0004
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2888548065237938; val_accuracy: 0.9169984076433121 

Epoch 18 start
The current lr is: 0.0004
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2867319664567899; val_accuracy: 0.9163017515923567 

Epoch 19 start
The current lr is: 0.0004
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28435178846120834; val_accuracy: 0.9182921974522293 

Epoch 20 start
The current lr is: 0.0004
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.84
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2821664803062275; val_accuracy: 0.9185907643312102 

Epoch 21 start
The current lr is: 0.0004
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28016189637647315; val_accuracy: 0.9196855095541401 

Epoch 22 start
The current lr is: 0.0004
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27822104888926646; val_accuracy: 0.919984076433121 

Epoch 23 start
The current lr is: 0.0004
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2761841104573505; val_accuracy: 0.9213773885350318 

Epoch 24 start
The current lr is: 0.0004
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2740009270466057; val_accuracy: 0.9205812101910829 

Epoch 25 start
The current lr is: 0.0004
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.81
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2720905372955997; val_accuracy: 0.9216759554140127 

Epoch 26 start
The current lr is: 0.0004
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27030442517464326; val_accuracy: 0.9216759554140127 

Epoch 27 start
The current lr is: 0.0004
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.83
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2686435977459713; val_accuracy: 0.9220740445859873 

Epoch 28 start
The current lr is: 0.0004
Batch: 0; loss: 0.26; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.8
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.266752305897368; val_accuracy: 0.92296974522293 

Epoch 29 start
The current lr is: 0.0004
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.81
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2651048116385937; val_accuracy: 0.9238654458598726 

Epoch 30 start
The current lr is: 0.0004
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26330202317256834; val_accuracy: 0.9233678343949044 

Epoch 31 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26249570530026584; val_accuracy: 0.9241640127388535 

Epoch 32 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.98
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.66; acc: 0.83
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2617842978352954; val_accuracy: 0.9243630573248408 

Epoch 33 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.58; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.261189960939869; val_accuracy: 0.924562101910828 

Epoch 34 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2604733000325549; val_accuracy: 0.9241640127388535 

Epoch 35 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25987890215626186; val_accuracy: 0.9246616242038217 

Epoch 36 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.86
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2591600570424347; val_accuracy: 0.9247611464968153 

Epoch 37 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.75; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.8
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25845911353826523; val_accuracy: 0.9249601910828026 

Epoch 38 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25790148828724385; val_accuracy: 0.9248606687898089 

Epoch 39 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2571790974088915; val_accuracy: 0.9255573248407644 

Epoch 40 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.98
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2566202128436535; val_accuracy: 0.9253582802547771 

Epoch 41 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2559739709090275; val_accuracy: 0.9254578025477707 

Epoch 42 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.08; acc: 1.0
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25531301263031686; val_accuracy: 0.9259554140127388 

Epoch 43 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.84
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.11; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.12; acc: 1.0
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2547223126147963; val_accuracy: 0.9257563694267515 

Epoch 44 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.254076730815848; val_accuracy: 0.9261544585987261 

Epoch 45 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2534895256209146; val_accuracy: 0.9263535031847133 

Epoch 46 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25322446575874735; val_accuracy: 0.9260549363057324 

Epoch 47 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2529838692610431; val_accuracy: 0.9263535031847133 

Epoch 48 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.81
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.1; acc: 1.0
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.252705620139078; val_accuracy: 0.9262539808917197 

Epoch 49 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.83
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.25246304202421455; val_accuracy: 0.9263535031847133 

Epoch 50 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.1; acc: 1.0
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2522246033475278; val_accuracy: 0.9263535031847133 

plots/no_subspace_training/MLP/2020-01-19 02:38:00/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.0004
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33219282357556046; val_accuracy: 0.9068471337579618 

Epoch 12 start
The current lr is: 0.0004
Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.61; acc: 0.75
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.27; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3279257650682881; val_accuracy: 0.9072452229299363 

Epoch 13 start
The current lr is: 0.0004
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32401811749122705; val_accuracy: 0.908140923566879 

Epoch 14 start
The current lr is: 0.0004
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31978660755476374; val_accuracy: 0.9099323248407644 

Epoch 15 start
The current lr is: 0.0004
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31624417676098027; val_accuracy: 0.9094347133757962 

Epoch 16 start
The current lr is: 0.0004
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31268386656691316; val_accuracy: 0.9111265923566879 

Epoch 17 start
The current lr is: 0.0004
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.30924799895969923; val_accuracy: 0.9126194267515924 

Epoch 18 start
The current lr is: 0.0004
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.97
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.83
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3063185626439228; val_accuracy: 0.9126194267515924 

Epoch 19 start
The current lr is: 0.0004
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.30324198068327207; val_accuracy: 0.9143113057324841 

Epoch 20 start
The current lr is: 0.0004
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.84
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.98
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.30039773269253933; val_accuracy: 0.9142117834394905 

Epoch 21 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.83
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29932107619798864; val_accuracy: 0.9139132165605095 

Epoch 22 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.83
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2982738941053676; val_accuracy: 0.914609872611465 

Epoch 23 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29722944210479213; val_accuracy: 0.9147093949044586 

Epoch 24 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2961096002892324; val_accuracy: 0.914609872611465 

Epoch 25 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.81
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29505162619671244; val_accuracy: 0.9148089171974523 

Epoch 26 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2940393300477866; val_accuracy: 0.915406050955414 

Epoch 27 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.97
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.81
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2930921143407275; val_accuracy: 0.915406050955414 

Epoch 28 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.31; acc: 0.84
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29209872966359374; val_accuracy: 0.916202229299363 

Epoch 29 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2911722247198129; val_accuracy: 0.9160031847133758 

Epoch 30 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29018563160281274; val_accuracy: 0.9166003184713376 

Epoch 31 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.53; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2897809956009221; val_accuracy: 0.9166998407643312 

Epoch 32 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.73; acc: 0.78
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2893851724494794; val_accuracy: 0.9166003184713376 

Epoch 33 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28901911398787405; val_accuracy: 0.9166003184713376 

Epoch 34 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.83
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2886293698932714; val_accuracy: 0.9165007961783439 

Epoch 35 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.83
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28827090333601474; val_accuracy: 0.9167993630573248 

Epoch 36 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2878919810436334; val_accuracy: 0.9168988853503185 

Epoch 37 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.83
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2875079601338715; val_accuracy: 0.9170979299363057 

Epoch 38 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2871573605355184; val_accuracy: 0.9168988853503185 

Epoch 39 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28677680348135104; val_accuracy: 0.9173964968152867 

Epoch 40 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.98
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2864331249503573; val_accuracy: 0.917296974522293 

Epoch 41 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.15; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28628622009685845; val_accuracy: 0.9173964968152867 

Epoch 42 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.1; acc: 1.0
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28613821497768355; val_accuracy: 0.917296974522293 

Epoch 43 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.83
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.86
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.13; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28599311525274995; val_accuracy: 0.9174960191082803 

Epoch 44 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.285847000398074; val_accuracy: 0.9175955414012739 

Epoch 45 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.98
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28570614888030255; val_accuracy: 0.9177945859872612 

Epoch 46 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2855614837567518; val_accuracy: 0.9176950636942676 

Epoch 47 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.98
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28542040848428274; val_accuracy: 0.9177945859872612 

Epoch 48 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28526987111682345; val_accuracy: 0.9176950636942676 

Epoch 49 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.83
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2851262084522824; val_accuracy: 0.9176950636942676 

Epoch 50 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28498449428066325; val_accuracy: 0.9177945859872612 

plots/no_subspace_training/MLP/2020-01-19 02:41:35/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.0004
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.78
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.46452513583906135; val_accuracy: 0.884156050955414 

Epoch 7 start
The current lr is: 0.0004
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.59; acc: 0.8
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.57; acc: 0.8
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.45; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44195341209697114; val_accuracy: 0.8884355095541401 

Epoch 8 start
The current lr is: 0.0004
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.78
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4234584394344099; val_accuracy: 0.8904259554140127 

Epoch 9 start
The current lr is: 0.0004
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.408080894618657; val_accuracy: 0.8934116242038217 

Epoch 10 start
The current lr is: 0.0004
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3949310974615395; val_accuracy: 0.8951035031847133 

Epoch 11 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3902399324972159; val_accuracy: 0.8960987261146497 

Epoch 12 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.77
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.69; acc: 0.75
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.81
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3858981742315991; val_accuracy: 0.8967953821656051 

Epoch 13 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.81
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3818467328218138; val_accuracy: 0.8972929936305732 

Epoch 14 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.95
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3778188419854565; val_accuracy: 0.8982882165605095 

Epoch 15 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.62; acc: 0.78
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.41; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3741803031628299; val_accuracy: 0.8985867834394905 

Epoch 16 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.41; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3726969206124354; val_accuracy: 0.8985867834394905 

Epoch 17 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3712578775586596; val_accuracy: 0.8987858280254777 

Epoch 18 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.8
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36988232199363646; val_accuracy: 0.8991839171974523 

Epoch 19 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.97
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3685283463472014; val_accuracy: 0.8996815286624203 

Epoch 20 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.97
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.98
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36720314849713803; val_accuracy: 0.8998805732484076 

Epoch 21 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.78
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.81
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.78
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36668351202443905; val_accuracy: 0.9000796178343949 

Epoch 22 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.78
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.81
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3661713067703186; val_accuracy: 0.9003781847133758 

Epoch 23 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3656612507951487; val_accuracy: 0.9004777070063694 

Epoch 24 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.8
Batch: 260; loss: 0.46; acc: 0.8
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36515066687278686; val_accuracy: 0.900577229299363 

Epoch 25 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3646438704079883; val_accuracy: 0.9007762738853503 

Epoch 26 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.84
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3644426616418893; val_accuracy: 0.9007762738853503 

Epoch 27 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.81
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.7; acc: 0.83
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3642431844951241; val_accuracy: 0.9007762738853503 

Epoch 28 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.78
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.364043194349784; val_accuracy: 0.9009753184713376 

Epoch 29 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.8
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.8
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36384375578468775; val_accuracy: 0.9009753184713376 

Epoch 30 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3636460412459768; val_accuracy: 0.9012738853503185 

Epoch 31 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.8
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36356692101545396; val_accuracy: 0.9012738853503185 

Epoch 32 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.53; acc: 0.77
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.84
Batch: 420; loss: 0.28; acc: 0.97
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.83; acc: 0.77
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3634872962335113; val_accuracy: 0.9012738853503185 

Epoch 33 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.62; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3634081438278696; val_accuracy: 0.9012738853503185 

Epoch 34 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.78
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36332889950009667; val_accuracy: 0.9012738853503185 

Epoch 35 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.83
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.38; acc: 0.94
Batch: 320; loss: 0.59; acc: 0.78
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.8
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36325010932554863; val_accuracy: 0.9012738853503185 

Epoch 36 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.67; acc: 0.81
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.83
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36321863797819537; val_accuracy: 0.9012738853503185 

Epoch 37 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.81
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.81; acc: 0.77
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.83
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.62; acc: 0.78
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36318722092042305; val_accuracy: 0.9014729299363057 

Epoch 38 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3631557897207843; val_accuracy: 0.9015724522292994 

Epoch 39 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.65; acc: 0.8
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3631244498738058; val_accuracy: 0.9015724522292994 

Epoch 40 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3630930392225836; val_accuracy: 0.9015724522292994 

Epoch 41 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.83
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.8
Batch: 680; loss: 0.21; acc: 0.98
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36308084414073616; val_accuracy: 0.9015724522292994 

Epoch 42 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.98
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.62; acc: 0.81
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36306858665434416; val_accuracy: 0.9015724522292994 

Epoch 43 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.81
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.81
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36305639252161526; val_accuracy: 0.9015724522292994 

Epoch 44 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.49; acc: 0.81
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.97
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36304422273377707; val_accuracy: 0.9015724522292994 

Epoch 45 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3630320424581789; val_accuracy: 0.9015724522292994 

Epoch 46 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36302802298858666; val_accuracy: 0.9015724522292994 

Epoch 47 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.97
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36302401149159025; val_accuracy: 0.9015724522292994 

Epoch 48 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.97
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.83
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.8
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3630199583282896; val_accuracy: 0.9015724522292994 

Epoch 49 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.8
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.81
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36301593207249977; val_accuracy: 0.9015724522292994 

Epoch 50 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.81
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3630118614454178; val_accuracy: 0.9015724522292994 

plots/no_subspace_training/MLP/2020-01-19 02:45:05/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.325642324082411; val_accuracy: 0.908140923566879 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.4; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3168912383306558; val_accuracy: 0.9098328025477707 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3087596929851611; val_accuracy: 0.9117237261146497 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3008695425596207; val_accuracy: 0.9144108280254777 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2940892424837799; val_accuracy: 0.9157046178343949 

Epoch 16 start
The current lr is: 0.0002
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2925832162427295; val_accuracy: 0.916202229299363 

Epoch 17 start
The current lr is: 0.0002
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.83
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29126704422531613; val_accuracy: 0.916202229299363 

Epoch 18 start
The current lr is: 0.0002
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2901549807209877; val_accuracy: 0.9163017515923567 

Epoch 19 start
The current lr is: 0.0002
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.11; acc: 1.0
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28890185912323607; val_accuracy: 0.9165007961783439 

Epoch 20 start
The current lr is: 0.0002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28773639887381514; val_accuracy: 0.9171974522292994 

Epoch 21 start
The current lr is: 0.0002
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2866227362945581; val_accuracy: 0.9174960191082803 

Epoch 22 start
The current lr is: 0.0002
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2855418806622742; val_accuracy: 0.9176950636942676 

Epoch 23 start
The current lr is: 0.0002
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2844495087577279; val_accuracy: 0.9181926751592356 

Epoch 24 start
The current lr is: 0.0002
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28325378211440555; val_accuracy: 0.9183917197452229 

Epoch 25 start
The current lr is: 0.0002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.81
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28215002211605666; val_accuracy: 0.9185907643312102 

Epoch 26 start
The current lr is: 0.0002
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28110102934252684; val_accuracy: 0.9187898089171974 

Epoch 27 start
The current lr is: 0.0002
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28012357813537503; val_accuracy: 0.9188893312101911 

Epoch 28 start
The current lr is: 0.0002
Batch: 0; loss: 0.28; acc: 0.84
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27906732171014614; val_accuracy: 0.9200835987261147 

Epoch 29 start
The current lr is: 0.0002
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2781125295693707; val_accuracy: 0.9198845541401274 

Epoch 30 start
The current lr is: 0.0002
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2770675477708221; val_accuracy: 0.9202826433121019 

Epoch 31 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.276840399784647; val_accuracy: 0.9204816878980892 

Epoch 32 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.98
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27662570224066446; val_accuracy: 0.9205812101910829 

Epoch 33 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27642568181844274; val_accuracy: 0.9203821656050956 

Epoch 34 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27621796489900846; val_accuracy: 0.9204816878980892 

Epoch 35 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.83
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.276024040333025; val_accuracy: 0.9204816878980892 

Epoch 36 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.86
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2758222686447156; val_accuracy: 0.9206807324840764 

Epoch 37 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.86
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.77; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.1; acc: 1.0
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.53; acc: 0.8
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2756175292999881; val_accuracy: 0.92078025477707 

Epoch 38 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2754263157962234; val_accuracy: 0.92078025477707 

Epoch 39 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27522350282995567; val_accuracy: 0.92078025477707 

Epoch 40 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.88
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.16; acc: 1.0
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.275035362144944; val_accuracy: 0.92078025477707 

Epoch 41 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2748403962060904; val_accuracy: 0.9208797770700637 

Epoch 42 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.09; acc: 1.0
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27464209743745766; val_accuracy: 0.9209792993630573 

Epoch 43 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.83
Batch: 320; loss: 0.3; acc: 0.86
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.12; acc: 1.0
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2744488205974269; val_accuracy: 0.9209792993630573 

Epoch 44 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.18; acc: 0.98
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27425213324226394; val_accuracy: 0.9209792993630573 

Epoch 45 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.98
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.88
Batch: 660; loss: 0.17; acc: 0.98
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2740651249980471; val_accuracy: 0.9209792993630573 

Epoch 46 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27402612320177117; val_accuracy: 0.9209792993630573 

Epoch 47 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.273987591978471; val_accuracy: 0.9209792993630573 

Epoch 48 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.81
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2739475426400543; val_accuracy: 0.9208797770700637 

Epoch 49 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.83
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27390848944900903; val_accuracy: 0.9208797770700637 

Epoch 50 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2738696128414695; val_accuracy: 0.9208797770700637 

plots/no_subspace_training/MLP/2020-01-19 02:48:43/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.0002
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33458875950164857; val_accuracy: 0.9065485668789809 

Epoch 12 start
The current lr is: 0.0002
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.75
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33229490583110005; val_accuracy: 0.9065485668789809 

Epoch 13 start
The current lr is: 0.0002
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3301365094579709; val_accuracy: 0.9068471337579618 

Epoch 14 start
The current lr is: 0.0002
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3277894091454281; val_accuracy: 0.9071457006369427 

Epoch 15 start
The current lr is: 0.0002
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3257618809866298; val_accuracy: 0.9075437898089171 

Epoch 16 start
The current lr is: 0.0002
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32363587845662595; val_accuracy: 0.908140923566879 

Epoch 17 start
The current lr is: 0.0002
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3215785527210327; val_accuracy: 0.908937101910828 

Epoch 18 start
The current lr is: 0.0002
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.81
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.83
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3197389359402049; val_accuracy: 0.9093351910828026 

Epoch 19 start
The current lr is: 0.0002
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31782751487698524; val_accuracy: 0.9103304140127388 

Epoch 20 start
The current lr is: 0.0002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.98
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3160148576186721; val_accuracy: 0.9104299363057324 

Epoch 21 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.81
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3156688435916688; val_accuracy: 0.9103304140127388 

Epoch 22 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.81
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31532550214012717; val_accuracy: 0.9104299363057324 

Epoch 23 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31498102530552324; val_accuracy: 0.9109275477707006 

Epoch 24 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.314627891370825; val_accuracy: 0.9112261146496815 

Epoch 25 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3142765743812178; val_accuracy: 0.9112261146496815 

Epoch 26 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3139273511471262; val_accuracy: 0.9112261146496815 

Epoch 27 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.81
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.313591158076836; val_accuracy: 0.9112261146496815 

Epoch 28 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.34; acc: 0.83
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3132517055910864; val_accuracy: 0.9111265923566879 

Epoch 29 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31291651180025876; val_accuracy: 0.9111265923566879 

Epoch 30 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31258231073997583; val_accuracy: 0.9113256369426752 

Epoch 31 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.54; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3125147209710376; val_accuracy: 0.9113256369426752 

Epoch 32 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.98
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31244632003793293; val_accuracy: 0.9113256369426752 

Epoch 33 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31237880700522924; val_accuracy: 0.9113256369426752 

Epoch 34 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31231086938434344; val_accuracy: 0.9113256369426752 

Epoch 35 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31224382996179495; val_accuracy: 0.9113256369426752 

Epoch 36 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.84
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3121765795501934; val_accuracy: 0.9113256369426752 

Epoch 37 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.83
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31210941968450123; val_accuracy: 0.9113256369426752 

Epoch 38 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.88
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.54; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31204269869122536; val_accuracy: 0.9114251592356688 

Epoch 39 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31197602553352427; val_accuracy: 0.9114251592356688 

Epoch 40 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3119095652631134; val_accuracy: 0.9114251592356688 

Epoch 41 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3118963093040096; val_accuracy: 0.9114251592356688 

Epoch 42 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3118830158547231; val_accuracy: 0.9114251592356688 

Epoch 43 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.81
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.81
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.16; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3118697507366253; val_accuracy: 0.9114251592356688 

Epoch 44 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3118566365283766; val_accuracy: 0.9114251592356688 

Epoch 45 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31184349156868685; val_accuracy: 0.9114251592356688 

Epoch 46 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31183026636102396; val_accuracy: 0.9114251592356688 

Epoch 47 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31181710763911535; val_accuracy: 0.9114251592356688 

Epoch 48 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.83
Batch: 440; loss: 0.5; acc: 0.8
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3118037327080016; val_accuracy: 0.9114251592356688 

Epoch 49 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.83
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31179044946173956; val_accuracy: 0.9114251592356688 

Epoch 50 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.31177715159905184; val_accuracy: 0.9114251592356688 

plots/no_subspace_training/MLP/2020-01-19 02:52:13/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.0002
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.78
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.94
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.95
Val Epoch over. val_loss: 0.47753585362510315; val_accuracy: 0.8821656050955414 

Epoch 7 start
The current lr is: 0.0002
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.94
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.61; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.46438688191638633; val_accuracy: 0.8845541401273885 

Epoch 8 start
The current lr is: 0.0002
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.79; acc: 0.73
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.78
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4525365726013852; val_accuracy: 0.8862460191082803 

Epoch 9 start
The current lr is: 0.0002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.97
Batch: 580; loss: 0.44; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.8
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4418943739810567; val_accuracy: 0.8883359872611465 

Epoch 10 start
The current lr is: 0.0002
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.29; acc: 0.97
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.72; acc: 0.8
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.8
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43219532281350176; val_accuracy: 0.8903264331210191 

Epoch 11 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.65; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.95
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43036044526631667; val_accuracy: 0.8902269108280255 

Epoch 12 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.94
Batch: 280; loss: 0.75; acc: 0.75
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.94
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.95
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.81
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4285760985058584; val_accuracy: 0.8900278662420382 

Epoch 13 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.64; acc: 0.78
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.67; acc: 0.78
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4268313703263641; val_accuracy: 0.8902269108280255 

Epoch 14 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.95
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4250976676773873; val_accuracy: 0.8910230891719745 

Epoch 15 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.78
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.58; acc: 0.8
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42341279100840257; val_accuracy: 0.8910230891719745 

Epoch 16 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.63; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.95
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4230772608025059; val_accuracy: 0.8911226114649682 

Epoch 17 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4227418538871085; val_accuracy: 0.8911226114649682 

Epoch 18 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.8
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.58; acc: 0.8
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.81
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42240752963123807; val_accuracy: 0.8911226114649682 

Epoch 19 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.98
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42207482865281926; val_accuracy: 0.8912221337579618 

Epoch 20 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.97
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.59; acc: 0.78
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.97
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4217436947165781; val_accuracy: 0.8911226114649682 

Epoch 21 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.77
Batch: 140; loss: 0.56; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.77
Batch: 420; loss: 0.59; acc: 0.8
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.59; acc: 0.78
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4216776792031185; val_accuracy: 0.8911226114649682 

Epoch 22 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.98
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.8
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.8
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42161195715711375; val_accuracy: 0.8911226114649682 

Epoch 23 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.75
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.95
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4215461004311871; val_accuracy: 0.8911226114649682 

Epoch 24 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.51; acc: 0.8
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42148025395574085; val_accuracy: 0.8911226114649682 

Epoch 25 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4214145524581526; val_accuracy: 0.8911226114649682 

Epoch 26 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.58; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.81
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.421402914793628; val_accuracy: 0.8911226114649682 

Epoch 27 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.58; acc: 0.78
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.88
Batch: 740; loss: 0.74; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42139132382573596; val_accuracy: 0.8911226114649682 

Epoch 28 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.7; acc: 0.75
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.78
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42137967372775836; val_accuracy: 0.8911226114649682 

Epoch 29 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.78
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.78
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213680184096288; val_accuracy: 0.8911226114649682 

Epoch 30 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.97
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.421356412872767; val_accuracy: 0.8911226114649682 

Epoch 31 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.97
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.63; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.81
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.42; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213555768891505; val_accuracy: 0.8911226114649682 

Epoch 32 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.62; acc: 0.78
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.97
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.95
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213547232993849; val_accuracy: 0.8911226114649682 

Epoch 33 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.66; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.94
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.71; acc: 0.81
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.8
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213539081963764; val_accuracy: 0.8911226114649682 

Epoch 34 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.61; acc: 0.78
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.54; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135306836882974; val_accuracy: 0.8911226114649682 

Epoch 35 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.37; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.62; acc: 0.78
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135225639791246; val_accuracy: 0.8911226114649682 

Epoch 36 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.65; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.83
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213522316259184; val_accuracy: 0.8911226114649682 

Epoch 37 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.97
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.78
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.82; acc: 0.75
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.66; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135219608142876; val_accuracy: 0.8911226114649682 

Epoch 38 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.52; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.61; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213521689366383; val_accuracy: 0.8911226114649682 

Epoch 39 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.73; acc: 0.75
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.6; acc: 0.8
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.62; acc: 0.81
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135214174439195; val_accuracy: 0.8911226114649682 

Epoch 40 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.64; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.95
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.81
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.77
Batch: 640; loss: 0.46; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213521180638842; val_accuracy: 0.8911226114649682 

Epoch 41 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213521179689723; val_accuracy: 0.8911226114649682 

Epoch 42 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.84
Batch: 160; loss: 0.63; acc: 0.8
Batch: 180; loss: 0.34; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.8
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.81
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.98
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.95
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.68; acc: 0.81
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135211953501794; val_accuracy: 0.8911226114649682 

Epoch 43 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.84
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.8
Batch: 280; loss: 0.5; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.48; acc: 0.83
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.97
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135211421995405; val_accuracy: 0.8911226114649682 

Epoch 44 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.95
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213521052033279; val_accuracy: 0.8911226114649682 

Epoch 45 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135210757612424; val_accuracy: 0.8911226114649682 

Epoch 46 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213521090947139; val_accuracy: 0.8911226114649682 

Epoch 47 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.56; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.78
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.97
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.49; acc: 0.8
Batch: 740; loss: 0.55; acc: 0.77
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213521094743613; val_accuracy: 0.8911226114649682 

Epoch 48 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.81
Batch: 340; loss: 0.27; acc: 0.98
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.6; acc: 0.78
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.8
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135210795577166; val_accuracy: 0.8911226114649682 

Epoch 49 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.77
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.83
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.97
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.42135210776594795; val_accuracy: 0.8911226114649682 

Epoch 50 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.36; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.81
Batch: 500; loss: 0.53; acc: 0.8
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.98
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.4213521089048902; val_accuracy: 0.8911226114649682 

plots/no_subspace_training/MLP/2020-01-19 02:55:43/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.325642324082411; val_accuracy: 0.908140923566879 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.4; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3168912383306558; val_accuracy: 0.9098328025477707 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3087596929851611; val_accuracy: 0.9117237261146497 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3008695425596207; val_accuracy: 0.9144108280254777 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2940892424837799; val_accuracy: 0.9157046178343949 

Epoch 16 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29302148245702125; val_accuracy: 0.9160031847133758 

Epoch 17 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.83
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2921472030935014; val_accuracy: 0.9160031847133758 

Epoch 18 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29139747332995103; val_accuracy: 0.9159036624203821 

Epoch 19 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.11; acc: 1.0
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29057233154204243; val_accuracy: 0.9161027070063694 

Epoch 20 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.28979621196438554; val_accuracy: 0.9165007961783439 

Epoch 21 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.289041061832267; val_accuracy: 0.9166998407643312 

Epoch 22 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28830319633529444; val_accuracy: 0.9170979299363057 

Epoch 23 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28756980313237307; val_accuracy: 0.917296974522293 

Epoch 24 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28677558476568027; val_accuracy: 0.917296974522293 

Epoch 25 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.81
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28601615738337205; val_accuracy: 0.9173964968152867 

Epoch 26 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28528519126640006; val_accuracy: 0.9175955414012739 

Epoch 27 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2846043597266173; val_accuracy: 0.9177945859872612 

Epoch 28 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.29; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.283881498987128; val_accuracy: 0.9179936305732485 

Epoch 29 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2832079925070143; val_accuracy: 0.9177945859872612 

Epoch 30 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2824907022866474; val_accuracy: 0.9184912420382165 

Epoch 31 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2823924907738236; val_accuracy: 0.9184912420382165 

Epoch 32 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.71; acc: 0.8
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2822952211662463; val_accuracy: 0.9184912420382165 

Epoch 33 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28220127214481877; val_accuracy: 0.9185907643312102 

Epoch 34 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.49; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2821064449514553; val_accuracy: 0.9185907643312102 

Epoch 35 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.83
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.86
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2820149307512933; val_accuracy: 0.9186902866242038 

Epoch 36 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.86
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2819224454130337; val_accuracy: 0.9186902866242038 

Epoch 37 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.8
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.281829682410143; val_accuracy: 0.9186902866242038 

Epoch 38 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28173950736879544; val_accuracy: 0.9186902866242038 

Epoch 39 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2816479684915512; val_accuracy: 0.9186902866242038 

Epoch 40 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.17; acc: 1.0
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28155886761511967; val_accuracy: 0.9186902866242038 

Epoch 41 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.86
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28146895358137264; val_accuracy: 0.9186902866242038 

Epoch 42 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.1; acc: 1.0
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2813782575213985; val_accuracy: 0.9186902866242038 

Epoch 43 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.81
Batch: 320; loss: 0.31; acc: 0.86
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.13; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28128854906672884; val_accuracy: 0.9187898089171974 

Epoch 44 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.98
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28119922823207394; val_accuracy: 0.9186902866242038 

Epoch 45 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.98
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2811116362168531; val_accuracy: 0.9186902866242038 

Epoch 46 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28110001321621003; val_accuracy: 0.9186902866242038 

Epoch 47 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.98
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2810884681381997; val_accuracy: 0.9186902866242038 

Epoch 48 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2810766376127863; val_accuracy: 0.9186902866242038 

Epoch 49 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.81
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2810649785930943; val_accuracy: 0.9186902866242038 

Epoch 50 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28105330177742965; val_accuracy: 0.9186902866242038 

plots/no_subspace_training/MLP/2020-01-19 02:59:14/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33545652791193337; val_accuracy: 0.9067476114649682 

Epoch 12 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.75
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33392470244579253; val_accuracy: 0.9063495222929936 

Epoch 13 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.332459414128665; val_accuracy: 0.9064490445859873 

Epoch 14 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.33088434905193415; val_accuracy: 0.9069466560509554 

Epoch 15 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.81
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32948266415838984; val_accuracy: 0.9068471337579618 

Epoch 16 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3280162977849602; val_accuracy: 0.9070461783439491 

Epoch 17 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3265745896063033; val_accuracy: 0.90734474522293 

Epoch 18 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.81
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3252534696440788; val_accuracy: 0.9080414012738853 

Epoch 19 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32389636157424584; val_accuracy: 0.9085390127388535 

Epoch 20 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.98
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32259081883035645; val_accuracy: 0.9086385350318471 

Epoch 21 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.95
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.81
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3224269319207046; val_accuracy: 0.9086385350318471 

Epoch 22 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.8
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32226481772721954; val_accuracy: 0.9086385350318471 

Epoch 23 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.84
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3221013684561298; val_accuracy: 0.9086385350318471 

Epoch 24 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32193563460924063; val_accuracy: 0.9087380573248408 

Epoch 25 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.83
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3217705697960155; val_accuracy: 0.9088375796178344 

Epoch 26 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32160536024221187; val_accuracy: 0.908937101910828 

Epoch 27 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3214430372901024; val_accuracy: 0.908937101910828 

Epoch 28 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.35; acc: 0.83
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3212793134390169; val_accuracy: 0.908937101910828 

Epoch 29 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.8
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.57; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32111629378643763; val_accuracy: 0.9088375796178344 

Epoch 30 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.27; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3209547156551082; val_accuracy: 0.908937101910828 

Epoch 31 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.83
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3209336211632012; val_accuracy: 0.908937101910828 

Epoch 32 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.320912270646566; val_accuracy: 0.908937101910828 

Epoch 33 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3208910437526217; val_accuracy: 0.908937101910828 

Epoch 34 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32086979417474404; val_accuracy: 0.908937101910828 

Epoch 35 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32084864183406164; val_accuracy: 0.908937101910828 

Epoch 36 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.84
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3208274652434003; val_accuracy: 0.908937101910828 

Epoch 37 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.83
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.86
Batch: 580; loss: 0.59; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3208063380543593; val_accuracy: 0.908937101910828 

Epoch 38 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32078519435065567; val_accuracy: 0.908937101910828 

Epoch 39 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32076418005926594; val_accuracy: 0.9090366242038217 

Epoch 40 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32074308267254736; val_accuracy: 0.9090366242038217 

Epoch 41 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.83
Batch: 680; loss: 0.17; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32074086890099157; val_accuracy: 0.9090366242038217 

Epoch 42 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.33; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32073862618132004; val_accuracy: 0.9090366242038217 

Epoch 43 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.81
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.81
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.81
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.17; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3207364073794359; val_accuracy: 0.9090366242038217 

Epoch 44 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32073422896254594; val_accuracy: 0.9090366242038217 

Epoch 45 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3207320412442942; val_accuracy: 0.9091361464968153 

Epoch 46 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3207298416620607; val_accuracy: 0.9091361464968153 

Epoch 47 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3207276619638607; val_accuracy: 0.9091361464968153 

Epoch 48 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.8
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3207254227559278; val_accuracy: 0.9091361464968153 

Epoch 49 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.83
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32072322526175506; val_accuracy: 0.9091361464968153 

Epoch 50 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.83
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3207209953551839; val_accuracy: 0.9091361464968153 

plots/no_subspace_training/MLP/2020-01-19 03:02:50/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.78
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.94
Batch: 680; loss: 0.58; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.48244593553482346; val_accuracy: 0.8811703821656051 

Epoch 7 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.94
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.59; acc: 0.81
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.91
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4734137230997632; val_accuracy: 0.883359872611465 

Epoch 8 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.4; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.78
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.73
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.6; acc: 0.78
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.94
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4649661402603623; val_accuracy: 0.8843550955414012 

Epoch 9 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.97
Batch: 580; loss: 0.46; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.8
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4571145837474021; val_accuracy: 0.8851512738853503 

Epoch 10 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.77
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.3; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.97
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.81
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.84
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.44976120598756586; val_accuracy: 0.8862460191082803 

Epoch 11 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.77
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.95
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4488400368933465; val_accuracy: 0.8864450636942676 

Epoch 12 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.81
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.44793119077469895; val_accuracy: 0.8866441082802548 

Epoch 13 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.34; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.78
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.69; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.44703114355445667; val_accuracy: 0.8865445859872612 

Epoch 14 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.95
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.81
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4461351474569102; val_accuracy: 0.8865445859872612 

Epoch 15 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.94
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.35; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.57; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.78
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.8
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.445249598117391; val_accuracy: 0.8867436305732485 

Epoch 16 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.75; acc: 0.78
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.95
Batch: 360; loss: 0.47; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4451349925273543; val_accuracy: 0.8867436305732485 

Epoch 17 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.77
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.6; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.64; acc: 0.8
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44502033596965157; val_accuracy: 0.886843152866242 

Epoch 18 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.8
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.6; acc: 0.8
Batch: 440; loss: 0.45; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4449056368914379; val_accuracy: 0.886843152866242 

Epoch 19 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.98
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.61; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44479113931109193; val_accuracy: 0.886843152866242 

Epoch 20 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.97
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.78
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.65; acc: 0.8
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.97
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446768643939571; val_accuracy: 0.886843152866242 

Epoch 21 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.58; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.75
Batch: 420; loss: 0.61; acc: 0.78
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.94
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.61; acc: 0.77
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44466409096672277; val_accuracy: 0.886843152866242 

Epoch 22 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.95
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.98
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.8
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.78
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.78
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446513653750632; val_accuracy: 0.886843152866242 

Epoch 23 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.94
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.68; acc: 0.75
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.94
Batch: 620; loss: 0.62; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44463862364838835; val_accuracy: 0.886843152866242 

Epoch 24 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.77
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.53; acc: 0.8
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.95
Batch: 360; loss: 0.69; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.444625812351324; val_accuracy: 0.886843152866242 

Epoch 25 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.47; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.83
Batch: 720; loss: 0.43; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.444613009311591; val_accuracy: 0.886843152866242 

Epoch 26 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.6; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.94
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461266278840933; val_accuracy: 0.886843152866242 

Epoch 27 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.6; acc: 0.78
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.76; acc: 0.81
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446123221497627; val_accuracy: 0.886843152866242 

Epoch 28 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.8
Batch: 340; loss: 0.71; acc: 0.75
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.6; acc: 0.8
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.64; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.81
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446119798976145; val_accuracy: 0.886843152866242 

Epoch 29 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.58; acc: 0.77
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.6; acc: 0.78
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446116412521168; val_accuracy: 0.886843152866242 

Epoch 30 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.97
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.77; acc: 0.78
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446112956780537; val_accuracy: 0.886843152866242 

Epoch 31 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.95
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.65; acc: 0.88
Batch: 420; loss: 0.55; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.86
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446112859970445; val_accuracy: 0.886843152866242 

Epoch 32 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.6; acc: 0.8
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.81
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.95
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.89; acc: 0.77
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.97
Batch: 720; loss: 0.45; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.95
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461128314968884; val_accuracy: 0.886843152866242 

Epoch 33 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.78
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.81
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.94
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461127603129974; val_accuracy: 0.886843152866242 

Epoch 34 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.64; acc: 0.78
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.83
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.8
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446112665401143; val_accuracy: 0.886843152866242 

Epoch 35 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.94
Batch: 320; loss: 0.63; acc: 0.77
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.66; acc: 0.8
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446112635978468; val_accuracy: 0.886843152866242 

Epoch 36 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.63; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446112610352267; val_accuracy: 0.886843152866242 

Epoch 37 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.97
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.78
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.83; acc: 0.75
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.77
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446112629334638; val_accuracy: 0.886843152866242 

Epoch 38 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.95
Batch: 580; loss: 0.51; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446112623639927; val_accuracy: 0.886843152866242 

Epoch 39 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.45; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.94
Batch: 480; loss: 0.76; acc: 0.75
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.61; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.61; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4446112616046978; val_accuracy: 0.886843152866242 

Epoch 40 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.8
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.95
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.94
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.77
Batch: 640; loss: 0.49; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126435714166; val_accuracy: 0.886843152866242 

Epoch 41 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.59; acc: 0.8
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.53; acc: 0.81
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.444611264736789; val_accuracy: 0.886843152866242 

Epoch 42 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.98
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.95
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.444611264736789; val_accuracy: 0.886843152866242 

Epoch 43 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.62; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126511643645; val_accuracy: 0.886843152866242 

Epoch 44 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.95
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.81
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126492661274; val_accuracy: 0.886843152866242 

Epoch 45 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.58; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126492661274; val_accuracy: 0.886843152866242 

Epoch 46 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.51; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126492661274; val_accuracy: 0.886843152866242 

Epoch 47 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.58; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.56; acc: 0.77
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.51; acc: 0.81
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.97
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.5; acc: 0.8
Batch: 740; loss: 0.57; acc: 0.77
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126492661274; val_accuracy: 0.886843152866242 

Epoch 48 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.81
Batch: 340; loss: 0.3; acc: 0.98
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.95
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126492661274; val_accuracy: 0.886843152866242 

Epoch 49 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.94
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 0.58; acc: 0.8
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.77
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.97
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126492661274; val_accuracy: 0.886843152866242 

Epoch 50 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.39; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.55; acc: 0.8
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.98
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.44461126492661274; val_accuracy: 0.886843152866242 

plots/no_subspace_training/MLP/2020-01-19 03:06:22/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.325642324082411; val_accuracy: 0.908140923566879 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.4; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3168912383306558; val_accuracy: 0.9098328025477707 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3087596929851611; val_accuracy: 0.9117237261146497 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3008695425596207; val_accuracy: 0.9144108280254777 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2940892424837799; val_accuracy: 0.9157046178343949 

Epoch 16 start
The current lr is: 0.0001
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2932188460116933; val_accuracy: 0.9161027070063694 

Epoch 17 start
The current lr is: 0.0001
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.83
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29253347488535436; val_accuracy: 0.9160031847133758 

Epoch 18 start
The current lr is: 0.0001
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29194125898514584; val_accuracy: 0.9156050955414012 

Epoch 19 start
The current lr is: 0.0001
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.11; acc: 1.0
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2913030380276358; val_accuracy: 0.9159036624203821 

Epoch 20 start
The current lr is: 0.0001
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2906980600421596; val_accuracy: 0.916202229299363 

Epoch 21 start
The current lr is: 0.0001
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2901061804621083; val_accuracy: 0.9164012738853503 

Epoch 22 start
The current lr is: 0.0001
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.28952463464752126; val_accuracy: 0.9166998407643312 

Epoch 23 start
The current lr is: 0.0001
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.28895066673778425; val_accuracy: 0.9167993630573248 

Epoch 24 start
The current lr is: 0.0001
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2883352917755485; val_accuracy: 0.9166998407643312 

Epoch 25 start
The current lr is: 0.0001
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.81
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28773721070236463; val_accuracy: 0.9168988853503185 

Epoch 26 start
The current lr is: 0.0001
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2871573387057918; val_accuracy: 0.9170979299363057 

Epoch 27 start
The current lr is: 0.0001
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28661613066674796; val_accuracy: 0.917296974522293 

Epoch 28 start
The current lr is: 0.0001
Batch: 0; loss: 0.29; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2860464840463013; val_accuracy: 0.9176950636942676 

Epoch 29 start
The current lr is: 0.0001
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2855078705651745; val_accuracy: 0.917296974522293 

Epoch 30 start
The current lr is: 0.0001
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.58; acc: 0.81
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2849445352983323; val_accuracy: 0.9178941082802548 

Epoch 31 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.52; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2848858225877118; val_accuracy: 0.9178941082802548 

Epoch 32 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28482694454064034; val_accuracy: 0.9178941082802548 

Epoch 33 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.6; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2847693002053127; val_accuracy: 0.9178941082802548 

Epoch 34 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28471136866671265; val_accuracy: 0.9178941082802548 

Epoch 35 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.83
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28465475535886303; val_accuracy: 0.9178941082802548 

Epoch 36 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.86
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2845978503868838; val_accuracy: 0.9179936305732485 

Epoch 37 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.84
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2845410012705311; val_accuracy: 0.9179936305732485 

Epoch 38 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.284485063877455; val_accuracy: 0.9179936305732485 

Epoch 39 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28442885266367796; val_accuracy: 0.9179936305732485 

Epoch 40 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.17; acc: 1.0
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2843734405602619; val_accuracy: 0.9179936305732485 

Epoch 41 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2843178079291514; val_accuracy: 0.9179936305732485 

Epoch 42 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.1; acc: 1.0
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28426191343623364; val_accuracy: 0.9179936305732485 

Epoch 43 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.86
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.13; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2842063667003516; val_accuracy: 0.9179936305732485 

Epoch 44 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28415144287097227; val_accuracy: 0.9179936305732485 

Epoch 45 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.98
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2840970907431499; val_accuracy: 0.9179936305732485 

Epoch 46 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2840916654391653; val_accuracy: 0.9179936305732485 

Epoch 47 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.98
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28408626761216266; val_accuracy: 0.9179936305732485 

Epoch 48 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2840807565078614; val_accuracy: 0.9179936305732485 

Epoch 49 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.83
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2840753025404967; val_accuracy: 0.9179936305732485 

Epoch 50 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2840698202893992; val_accuracy: 0.9179936305732485 

plots/no_subspace_training/MLP/2020-01-19 03:09:50/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.92
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.43270206584292614; val_accuracy: 0.8878383757961783 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.55; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.39523022398827184; val_accuracy: 0.8948049363057324 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3699739895713557; val_accuracy: 0.8994824840764332 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.35165584799210736; val_accuracy: 0.903562898089172 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3371214442856752; val_accuracy: 0.90625 

Epoch 11 start
The current lr is: 0.0001
Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3358328624800512; val_accuracy: 0.9065485668789809 

Epoch 12 start
The current lr is: 0.0001
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.62; acc: 0.75
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.97
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3346406237524786; val_accuracy: 0.9061504777070064 

Epoch 13 start
The current lr is: 0.0001
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3334897631767449; val_accuracy: 0.90625 

Epoch 14 start
The current lr is: 0.0001
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3322642579864544; val_accuracy: 0.9067476114649682 

Epoch 15 start
The current lr is: 0.0001
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.81
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.33115259885408316; val_accuracy: 0.9068471337579618 

Epoch 16 start
The current lr is: 0.0001
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.73; acc: 0.81
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3299982398748398; val_accuracy: 0.9069466560509554 

Epoch 17 start
The current lr is: 0.0001
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32885211668196757; val_accuracy: 0.90734474522293 

Epoch 18 start
The current lr is: 0.0001
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.81
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3277861236766645; val_accuracy: 0.90734474522293 

Epoch 19 start
The current lr is: 0.0001
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3266981444351233; val_accuracy: 0.9075437898089171 

Epoch 20 start
The current lr is: 0.0001
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.98
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3256427157836355; val_accuracy: 0.9078423566878981 

Epoch 21 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.81
Batch: 660; loss: 0.33; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32554008152074876; val_accuracy: 0.9078423566878981 

Epoch 22 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.81
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3254386863321256; val_accuracy: 0.9078423566878981 

Epoch 23 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.95
Batch: 200; loss: 0.39; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32533634482485474; val_accuracy: 0.9079418789808917 

Epoch 24 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32523304746029486; val_accuracy: 0.9080414012738853 

Epoch 25 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.83
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32513036104334386; val_accuracy: 0.9080414012738853 

Epoch 26 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3250275950428027; val_accuracy: 0.9080414012738853 

Epoch 27 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3249260489443305; val_accuracy: 0.9080414012738853 

Epoch 28 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.36; acc: 0.83
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.81
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.81
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3248234667405961; val_accuracy: 0.908140923566879 

Epoch 29 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.8
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32472110283412753; val_accuracy: 0.908140923566879 

Epoch 30 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.27; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32461969720520034; val_accuracy: 0.9082404458598726 

Epoch 31 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.55; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32460964248058904; val_accuracy: 0.9082404458598726 

Epoch 32 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.45; acc: 0.83
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3245994644180225; val_accuracy: 0.9082404458598726 

Epoch 33 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3245893514649883; val_accuracy: 0.9082404458598726 

Epoch 34 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32457921834318504; val_accuracy: 0.9082404458598726 

Epoch 35 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.83
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32456911981675274; val_accuracy: 0.9082404458598726 

Epoch 36 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3245590234732932; val_accuracy: 0.9082404458598726 

Epoch 37 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.81
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.84
Batch: 560; loss: 0.33; acc: 0.86
Batch: 580; loss: 0.59; acc: 0.8
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.45; acc: 0.83
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3245489463969401; val_accuracy: 0.9082404458598726 

Epoch 38 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.55; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3245388638156994; val_accuracy: 0.9082404458598726 

Epoch 39 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.59; acc: 0.81
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3245288279310913; val_accuracy: 0.9082404458598726 

Epoch 40 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3245187319672791; val_accuracy: 0.9082404458598726 

Epoch 41 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.81
Batch: 680; loss: 0.17; acc: 0.98
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32451829152881717; val_accuracy: 0.9082404458598726 

Epoch 42 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32451783025720315; val_accuracy: 0.9082404458598726 

Epoch 43 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.81
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.81
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.81
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.18; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.83
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32451739257118506; val_accuracy: 0.9082404458598726 

Epoch 44 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.48; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.324516968552474; val_accuracy: 0.9082404458598726 

Epoch 45 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32451654505577815; val_accuracy: 0.9082404458598726 

Epoch 46 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3245161210370671; val_accuracy: 0.9082404458598726 

Epoch 47 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32451569184566; val_accuracy: 0.9082404458598726 

Epoch 48 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.52; acc: 0.8
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32451524462100045; val_accuracy: 0.9082404458598726 

Epoch 49 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.83
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.81
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32451480869085164; val_accuracy: 0.9082404458598726 

Epoch 50 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.83
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32451434400241086; val_accuracy: 0.9082404458598726 

plots/no_subspace_training/MLP/2020-01-19 03:13:23/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.32; acc: 0.05
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.2
Batch: 140; loss: 2.28; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.2
Batch: 180; loss: 2.26; acc: 0.25
Batch: 200; loss: 2.26; acc: 0.23
Batch: 220; loss: 2.24; acc: 0.34
Batch: 240; loss: 2.23; acc: 0.33
Batch: 260; loss: 2.24; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.39
Batch: 300; loss: 2.22; acc: 0.41
Batch: 320; loss: 2.23; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.52
Batch: 360; loss: 2.2; acc: 0.53
Batch: 380; loss: 2.18; acc: 0.52
Batch: 400; loss: 2.18; acc: 0.42
Batch: 420; loss: 2.16; acc: 0.53
Batch: 440; loss: 2.16; acc: 0.55
Batch: 460; loss: 2.18; acc: 0.42
Batch: 480; loss: 2.17; acc: 0.47
Batch: 500; loss: 2.13; acc: 0.59
Batch: 520; loss: 2.17; acc: 0.5
Batch: 540; loss: 2.17; acc: 0.42
Batch: 560; loss: 2.12; acc: 0.64
Batch: 580; loss: 2.14; acc: 0.55
Batch: 600; loss: 2.13; acc: 0.52
Batch: 620; loss: 2.1; acc: 0.58
Batch: 640; loss: 2.12; acc: 0.45
Batch: 660; loss: 2.11; acc: 0.55
Batch: 680; loss: 2.04; acc: 0.67
Batch: 700; loss: 2.08; acc: 0.53
Batch: 720; loss: 2.08; acc: 0.53
Batch: 740; loss: 2.06; acc: 0.69
Batch: 760; loss: 2.02; acc: 0.53
Batch: 780; loss: 2.04; acc: 0.61
Train Epoch over. train_loss: 2.18; train_accuracy: 0.43 

Batch: 0; loss: 2.03; acc: 0.64
Batch: 20; loss: 1.99; acc: 0.53
Batch: 40; loss: 1.92; acc: 0.77
Batch: 60; loss: 1.99; acc: 0.64
Batch: 80; loss: 1.99; acc: 0.69
Batch: 100; loss: 2.02; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.58
Batch: 140; loss: 1.96; acc: 0.72
Val Epoch over. val_loss: 2.0103653935110493; val_accuracy: 0.6457006369426752 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.01; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.66
Batch: 40; loss: 2.0; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.78
Batch: 120; loss: 1.95; acc: 0.7
Batch: 140; loss: 1.93; acc: 0.64
Batch: 160; loss: 1.93; acc: 0.61
Batch: 180; loss: 1.87; acc: 0.7
Batch: 200; loss: 1.87; acc: 0.69
Batch: 220; loss: 1.87; acc: 0.7
Batch: 240; loss: 1.84; acc: 0.64
Batch: 260; loss: 1.9; acc: 0.53
Batch: 280; loss: 1.83; acc: 0.62
Batch: 300; loss: 1.86; acc: 0.56
Batch: 320; loss: 1.77; acc: 0.7
Batch: 340; loss: 1.8; acc: 0.73
Batch: 360; loss: 1.79; acc: 0.59
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.76; acc: 0.69
Batch: 420; loss: 1.71; acc: 0.7
Batch: 440; loss: 1.73; acc: 0.77
Batch: 460; loss: 1.68; acc: 0.67
Batch: 480; loss: 1.6; acc: 0.73
Batch: 500; loss: 1.69; acc: 0.61
Batch: 520; loss: 1.62; acc: 0.72
Batch: 540; loss: 1.57; acc: 0.69
Batch: 560; loss: 1.57; acc: 0.75
Batch: 580; loss: 1.48; acc: 0.84
Batch: 600; loss: 1.63; acc: 0.64
Batch: 620; loss: 1.65; acc: 0.64
Batch: 640; loss: 1.54; acc: 0.67
Batch: 660; loss: 1.51; acc: 0.73
Batch: 680; loss: 1.47; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.66
Batch: 720; loss: 1.53; acc: 0.69
Batch: 740; loss: 1.53; acc: 0.66
Batch: 760; loss: 1.39; acc: 0.73
Batch: 780; loss: 1.36; acc: 0.77
Train Epoch over. train_loss: 1.72; train_accuracy: 0.69 

Batch: 0; loss: 1.43; acc: 0.75
Batch: 20; loss: 1.4; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.83
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.26; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.91
Batch: 120; loss: 1.49; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.81
Val Epoch over. val_loss: 1.3688275730533965; val_accuracy: 0.7597531847133758 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.4; acc: 0.7
Batch: 20; loss: 1.34; acc: 0.72
Batch: 40; loss: 1.4; acc: 0.73
Batch: 60; loss: 1.3; acc: 0.73
Batch: 80; loss: 1.35; acc: 0.7
Batch: 100; loss: 1.38; acc: 0.73
Batch: 120; loss: 1.29; acc: 0.78
Batch: 140; loss: 1.22; acc: 0.77
Batch: 160; loss: 1.39; acc: 0.66
Batch: 180; loss: 1.23; acc: 0.77
Batch: 200; loss: 1.26; acc: 0.69
Batch: 220; loss: 1.17; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.77
Batch: 260; loss: 1.17; acc: 0.83
Batch: 280; loss: 1.33; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.86
Batch: 320; loss: 1.12; acc: 0.83
Batch: 340; loss: 1.18; acc: 0.77
Batch: 360; loss: 1.03; acc: 0.86
Batch: 380; loss: 1.09; acc: 0.81
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.86
Batch: 480; loss: 1.02; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.78
Batch: 540; loss: 0.99; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.83
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.85; acc: 0.86
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 1.01; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.88
Train Epoch over. train_loss: 1.1; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.59; acc: 0.94
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.89
Batch: 100; loss: 0.81; acc: 0.94
Batch: 120; loss: 1.05; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.92
Val Epoch over. val_loss: 0.8315459994753455; val_accuracy: 0.8388734076433121 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 0.95; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.75
Batch: 180; loss: 0.68; acc: 0.91
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.88; acc: 0.84
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.72; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.84
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.94
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.92
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.41; acc: 0.95
Val Epoch over. val_loss: 0.6002794304850755; val_accuracy: 0.863953025477707 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.89
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.95
Val Epoch over. val_loss: 0.49239057720087137; val_accuracy: 0.8799761146496815 

Epoch 6 start
The current lr is: 0.0001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.78
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.94
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.87 

Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.484614933467215; val_accuracy: 0.8809713375796179 

Epoch 7 start
The current lr is: 0.0001
Batch: 0; loss: 0.61; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.94
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.81
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.91
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.97
Val Epoch over. val_loss: 0.4774952042064849; val_accuracy: 0.8821656050955414 

Epoch 8 start
The current lr is: 0.0001
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.92
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.8; acc: 0.73
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.62; acc: 0.78
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.78
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.94
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.91
Batch: 580; loss: 0.7; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.97
Val Epoch over. val_loss: 0.4707328115299249; val_accuracy: 0.8839570063694268 

Epoch 9 start
The current lr is: 0.0001
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.94
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.97
Batch: 580; loss: 0.47; acc: 0.92
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.65; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.8
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.46434401307895684; val_accuracy: 0.8844546178343949 

Epoch 10 start
The current lr is: 0.0001
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.62; acc: 0.77
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.31; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.97
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.67; acc: 0.78
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.51; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4582796783014468; val_accuracy: 0.8850517515923567 

Epoch 11 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.66; acc: 0.77
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.29; acc: 0.97
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.95
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4576905346979761; val_accuracy: 0.8850517515923567 

Epoch 12 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.92
Batch: 280; loss: 0.78; acc: 0.75
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4571066529128202; val_accuracy: 0.8855493630573248 

Epoch 13 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.67; acc: 0.78
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.7; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.4565259366278436; val_accuracy: 0.8854498407643312 

Epoch 14 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.63; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 0.43; acc: 0.95
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.91
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.45594693995585106; val_accuracy: 0.8855493630573248 

Epoch 15 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.94
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.36; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.92
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.88
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.78
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.63; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.49; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4553718358088451; val_accuracy: 0.8855493630573248 

Epoch 16 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.75; acc: 0.78
Batch: 200; loss: 0.59; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.95
Batch: 360; loss: 0.48; acc: 0.92
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.92
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45531491261378976; val_accuracy: 0.8854498407643312 

Epoch 17 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.65; acc: 0.78
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4552579188043145; val_accuracy: 0.8854498407643312 

Epoch 18 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.8
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.47; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4552008879792159; val_accuracy: 0.8854498407643312 

Epoch 19 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.52; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.98
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.7; acc: 0.78
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4551439379241056; val_accuracy: 0.8854498407643312 

Epoch 20 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.97
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.78
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.97
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45508703940613254; val_accuracy: 0.8854498407643312 

Epoch 21 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.89
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.62; acc: 0.75
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.94
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.78
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45508430746330575; val_accuracy: 0.8854498407643312 

Epoch 22 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.45; acc: 0.95
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.98
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.8
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.78
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550815975400293; val_accuracy: 0.8854498407643312 

Epoch 23 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.77
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.94
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.7; acc: 0.75
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.8
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.94
Batch: 620; loss: 0.63; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550788790746859; val_accuracy: 0.8854498407643312 

Epoch 24 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.78
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.8
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.95
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.83
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550761540604245; val_accuracy: 0.8854498407643312 

Epoch 25 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.68; acc: 0.78
Batch: 220; loss: 0.42; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.49; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.92
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550734429982058; val_accuracy: 0.8854498407643312 

Epoch 26 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.61; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.81
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.57; acc: 0.91
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45507341870077095; val_accuracy: 0.8854498407643312 

Epoch 27 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.44; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.8
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.78
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45507339060686197; val_accuracy: 0.8854498407643312 

Epoch 28 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.58; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.8
Batch: 340; loss: 0.72; acc: 0.75
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.8
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.65; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733701059013; val_accuracy: 0.8854498407643312 

Epoch 29 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.57; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.77
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.8
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.68; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.68; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.455073337646047; val_accuracy: 0.8854498407643312 

Epoch 30 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.97
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45507332227032654; val_accuracy: 0.8854498407643312 

Epoch 31 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.95
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.66; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.62; acc: 0.81
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.455073321321208; val_accuracy: 0.8854498407643312 

Epoch 32 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.95
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.89; acc: 0.77
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.97
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.95
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733194229709; val_accuracy: 0.8854498407643312 

Epoch 33 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.94
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.64; acc: 0.8
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45507331847385235; val_accuracy: 0.8854498407643312 

Epoch 34 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.78
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.61; acc: 0.84
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.56; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.455073321321208; val_accuracy: 0.8854498407643312 

Epoch 35 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.94
Batch: 320; loss: 0.64; acc: 0.77
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.8
Batch: 440; loss: 0.62; acc: 0.8
Batch: 460; loss: 0.64; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.45507332018226576; val_accuracy: 0.8854498407643312 

Epoch 36 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.75; acc: 0.8
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733205619132; val_accuracy: 0.8854498407643312 

Epoch 37 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.97
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.83; acc: 0.77
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.55; acc: 0.8
Batch: 380; loss: 0.54; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.53; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.77
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.38; acc: 0.94
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733203720895; val_accuracy: 0.8854498407643312 

Epoch 38 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.52; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.52; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.63; acc: 0.86
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 39 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.94
Batch: 480; loss: 0.77; acc: 0.75
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 0.72; acc: 0.8
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 40 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 0.45; acc: 0.94
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.55; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.77
Batch: 640; loss: 0.5; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 41 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.6; acc: 0.8
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.33; acc: 0.94
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 42 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.53; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.8
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.63; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.98
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.95
Batch: 740; loss: 0.5; acc: 0.91
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 43 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.56; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.8
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.8
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 44 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.8
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.95
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 45 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 46 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 47 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.59; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.41; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.78
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.97
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.8
Batch: 740; loss: 0.58; acc: 0.77
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 48 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.31; acc: 0.98
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.8
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.67; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.62; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.95
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 49 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.69; acc: 0.81
Batch: 160; loss: 0.59; acc: 0.8
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.77
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 0.39; acc: 0.97
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.94
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

Epoch 50 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.41; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.98
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.4550733207517369; val_accuracy: 0.8854498407643312 

plots/no_subspace_training/MLP/2020-01-19 03:16:54/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
