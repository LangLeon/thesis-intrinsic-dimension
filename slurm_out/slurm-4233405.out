nonzero elements in E: 10494
elements in E: 2221300
fraction nonzero: 0.004724260568135776
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 7.27; acc: 0.2
Batch: 40; loss: 5.8; acc: 0.14
Batch: 60; loss: 5.65; acc: 0.2
Batch: 80; loss: 4.84; acc: 0.3
Batch: 100; loss: 3.82; acc: 0.31
Batch: 120; loss: 3.89; acc: 0.41
Batch: 140; loss: 6.14; acc: 0.33
Batch: 160; loss: 5.08; acc: 0.28
Batch: 180; loss: 4.48; acc: 0.33
Batch: 200; loss: 4.72; acc: 0.42
Batch: 220; loss: 6.61; acc: 0.2
Batch: 240; loss: 5.6; acc: 0.36
Batch: 260; loss: 7.25; acc: 0.22
Batch: 280; loss: 7.84; acc: 0.23
Batch: 300; loss: 5.45; acc: 0.36
Batch: 320; loss: 5.77; acc: 0.31
Batch: 340; loss: 5.22; acc: 0.33
Batch: 360; loss: 4.78; acc: 0.42
Batch: 380; loss: 5.65; acc: 0.33
Batch: 400; loss: 5.64; acc: 0.27
Batch: 420; loss: 6.58; acc: 0.25
Batch: 440; loss: 3.77; acc: 0.5
Batch: 460; loss: 4.32; acc: 0.39
Batch: 480; loss: 4.73; acc: 0.28
Batch: 500; loss: 5.04; acc: 0.28
Batch: 520; loss: 6.0; acc: 0.3
Batch: 540; loss: 5.69; acc: 0.3
Batch: 560; loss: 5.23; acc: 0.38
Batch: 580; loss: 4.76; acc: 0.31
Batch: 600; loss: 4.96; acc: 0.31
Batch: 620; loss: 6.29; acc: 0.3
Train Epoch over. train_loss: 5.54; train_accuracy: 0.3 

Batch: 0; loss: 4.49; acc: 0.44
Batch: 20; loss: 7.16; acc: 0.25
Batch: 40; loss: 4.61; acc: 0.33
Batch: 60; loss: 5.77; acc: 0.28
Batch: 80; loss: 6.5; acc: 0.23
Batch: 100; loss: 5.93; acc: 0.31
Batch: 120; loss: 5.45; acc: 0.27
Batch: 140; loss: 6.77; acc: 0.27
Val Epoch over. val_loss: 5.129563563948224; val_accuracy: 0.33379777070063693 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 4.44; acc: 0.38
Batch: 20; loss: 5.75; acc: 0.33
Batch: 40; loss: 5.66; acc: 0.25
Batch: 60; loss: 4.13; acc: 0.38
Batch: 80; loss: 6.06; acc: 0.3
Batch: 100; loss: 5.26; acc: 0.34
Batch: 120; loss: 5.44; acc: 0.31
Batch: 140; loss: 5.12; acc: 0.44
Batch: 160; loss: 5.04; acc: 0.33
Batch: 180; loss: 4.33; acc: 0.33
Batch: 200; loss: 6.6; acc: 0.22
Batch: 220; loss: 3.45; acc: 0.44
Batch: 240; loss: 5.2; acc: 0.33
Batch: 260; loss: 5.16; acc: 0.3
Batch: 280; loss: 5.44; acc: 0.28
Batch: 300; loss: 4.51; acc: 0.41
Batch: 320; loss: 5.6; acc: 0.25
Batch: 340; loss: 6.61; acc: 0.3
Batch: 360; loss: 6.16; acc: 0.22
Batch: 380; loss: 5.42; acc: 0.41
Batch: 400; loss: 5.27; acc: 0.41
Batch: 420; loss: 4.58; acc: 0.38
Batch: 440; loss: 4.28; acc: 0.38
Batch: 460; loss: 6.32; acc: 0.33
Batch: 480; loss: 3.98; acc: 0.44
Batch: 500; loss: 5.05; acc: 0.36
Batch: 520; loss: 3.77; acc: 0.31
Batch: 540; loss: 4.77; acc: 0.23
Batch: 560; loss: 5.34; acc: 0.36
Batch: 580; loss: 4.43; acc: 0.41
Batch: 600; loss: 4.91; acc: 0.28
Batch: 620; loss: 6.03; acc: 0.31
Train Epoch over. train_loss: 5.11; train_accuracy: 0.33 

Batch: 0; loss: 4.44; acc: 0.33
Batch: 20; loss: 7.39; acc: 0.28
Batch: 40; loss: 4.47; acc: 0.3
Batch: 60; loss: 5.38; acc: 0.25
Batch: 80; loss: 4.65; acc: 0.33
Batch: 100; loss: 6.86; acc: 0.25
Batch: 120; loss: 4.45; acc: 0.38
Batch: 140; loss: 6.44; acc: 0.33
Val Epoch over. val_loss: 4.943485624471288; val_accuracy: 0.32334792993630573 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 4.76; acc: 0.38
Batch: 20; loss: 5.35; acc: 0.3
Batch: 40; loss: 4.05; acc: 0.45
Batch: 60; loss: 5.64; acc: 0.33
Batch: 80; loss: 5.23; acc: 0.31
Batch: 100; loss: 5.15; acc: 0.36
Batch: 120; loss: 5.48; acc: 0.3
Batch: 140; loss: 5.55; acc: 0.3
Batch: 160; loss: 5.09; acc: 0.22
Batch: 180; loss: 4.7; acc: 0.25
Batch: 200; loss: 3.88; acc: 0.48
Batch: 220; loss: 5.35; acc: 0.31
Batch: 240; loss: 5.17; acc: 0.3
Batch: 260; loss: 5.19; acc: 0.3
Batch: 280; loss: 4.69; acc: 0.3
Batch: 300; loss: 4.8; acc: 0.27
Batch: 320; loss: 4.43; acc: 0.33
Batch: 340; loss: 3.59; acc: 0.44
Batch: 360; loss: 5.39; acc: 0.3
Batch: 380; loss: 5.06; acc: 0.27
Batch: 400; loss: 5.34; acc: 0.38
Batch: 420; loss: 4.59; acc: 0.33
Batch: 440; loss: 5.26; acc: 0.38
Batch: 460; loss: 4.17; acc: 0.44
Batch: 480; loss: 6.71; acc: 0.28
Batch: 500; loss: 7.36; acc: 0.27
Batch: 520; loss: 4.41; acc: 0.34
Batch: 540; loss: 4.16; acc: 0.44
Batch: 560; loss: 4.19; acc: 0.36
Batch: 580; loss: 4.44; acc: 0.39
Batch: 600; loss: 3.35; acc: 0.31
Batch: 620; loss: 4.93; acc: 0.33
Train Epoch over. train_loss: 5.02; train_accuracy: 0.33 

Batch: 0; loss: 5.88; acc: 0.33
Batch: 20; loss: 8.19; acc: 0.22
Batch: 40; loss: 5.38; acc: 0.31
Batch: 60; loss: 7.58; acc: 0.27
Batch: 80; loss: 6.49; acc: 0.23
Batch: 100; loss: 6.56; acc: 0.25
Batch: 120; loss: 5.9; acc: 0.42
Batch: 140; loss: 7.94; acc: 0.19
Val Epoch over. val_loss: 6.057570644245026; val_accuracy: 0.31558519108280253 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 5.89; acc: 0.28
Batch: 20; loss: 6.1; acc: 0.23
Batch: 40; loss: 3.81; acc: 0.41
Batch: 60; loss: 5.06; acc: 0.38
Batch: 80; loss: 5.34; acc: 0.27
Batch: 100; loss: 4.52; acc: 0.36
Batch: 120; loss: 4.4; acc: 0.34
Batch: 140; loss: 4.92; acc: 0.38
Batch: 160; loss: 3.79; acc: 0.41
Batch: 180; loss: 6.15; acc: 0.27
Batch: 200; loss: 3.92; acc: 0.44
Batch: 220; loss: 5.35; acc: 0.28
Batch: 240; loss: 3.77; acc: 0.5
Batch: 260; loss: 5.55; acc: 0.28
Batch: 280; loss: 5.54; acc: 0.33
Batch: 300; loss: 5.13; acc: 0.19
Batch: 320; loss: 4.88; acc: 0.38
Batch: 340; loss: 3.92; acc: 0.39
Batch: 360; loss: 5.26; acc: 0.31
Batch: 380; loss: 5.57; acc: 0.27
Batch: 400; loss: 4.0; acc: 0.34
Batch: 420; loss: 4.51; acc: 0.31
Batch: 440; loss: 5.64; acc: 0.34
Batch: 460; loss: 5.27; acc: 0.36
Batch: 480; loss: 4.04; acc: 0.33
Batch: 500; loss: 5.21; acc: 0.41
Batch: 520; loss: 6.14; acc: 0.22
Batch: 540; loss: 5.51; acc: 0.23
Batch: 560; loss: 6.33; acc: 0.33
Batch: 580; loss: 4.19; acc: 0.33
Batch: 600; loss: 3.6; acc: 0.44
Batch: 620; loss: 4.08; acc: 0.41
Train Epoch over. train_loss: 5.01; train_accuracy: 0.33 

Batch: 0; loss: 5.01; acc: 0.28
Batch: 20; loss: 7.0; acc: 0.22
Batch: 40; loss: 4.99; acc: 0.34
Batch: 60; loss: 6.47; acc: 0.28
Batch: 80; loss: 4.46; acc: 0.28
Batch: 100; loss: 6.37; acc: 0.25
Batch: 120; loss: 4.82; acc: 0.44
Batch: 140; loss: 6.28; acc: 0.23
Val Epoch over. val_loss: 5.073384500612878; val_accuracy: 0.31658041401273884 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 3.41; acc: 0.42
Batch: 20; loss: 7.65; acc: 0.31
Batch: 40; loss: 5.6; acc: 0.33
Batch: 60; loss: 5.68; acc: 0.33
Batch: 80; loss: 4.93; acc: 0.39
Batch: 100; loss: 4.53; acc: 0.44
Batch: 120; loss: 3.38; acc: 0.42
Batch: 140; loss: 4.18; acc: 0.41
Batch: 160; loss: 4.18; acc: 0.45
Batch: 180; loss: 4.24; acc: 0.33
Batch: 200; loss: 5.04; acc: 0.33
Batch: 220; loss: 4.83; acc: 0.33
Batch: 240; loss: 5.23; acc: 0.22
Batch: 260; loss: 5.42; acc: 0.33
Batch: 280; loss: 4.37; acc: 0.33
Batch: 300; loss: 5.13; acc: 0.36
Batch: 320; loss: 5.5; acc: 0.3
Batch: 340; loss: 4.42; acc: 0.34
Batch: 360; loss: 6.45; acc: 0.28
Batch: 380; loss: 3.7; acc: 0.48
Batch: 400; loss: 4.38; acc: 0.31
Batch: 420; loss: 5.42; acc: 0.27
Batch: 440; loss: 6.36; acc: 0.27
Batch: 460; loss: 4.72; acc: 0.36
Batch: 480; loss: 3.68; acc: 0.39
Batch: 500; loss: 7.03; acc: 0.25
Batch: 520; loss: 3.93; acc: 0.42
Batch: 540; loss: 4.39; acc: 0.39
Batch: 560; loss: 5.48; acc: 0.34
Batch: 580; loss: 3.62; acc: 0.34
Batch: 600; loss: 3.37; acc: 0.44
Batch: 620; loss: 4.69; acc: 0.38
Train Epoch over. train_loss: 5.04; train_accuracy: 0.33 

Batch: 0; loss: 4.22; acc: 0.36
Batch: 20; loss: 6.5; acc: 0.28
Batch: 40; loss: 4.74; acc: 0.23
Batch: 60; loss: 5.31; acc: 0.25
Batch: 80; loss: 4.99; acc: 0.33
Batch: 100; loss: 6.03; acc: 0.3
Batch: 120; loss: 4.54; acc: 0.38
Batch: 140; loss: 6.07; acc: 0.28
Val Epoch over. val_loss: 4.933632903797611; val_accuracy: 0.32464171974522293 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 4.33; acc: 0.38
Batch: 20; loss: 4.62; acc: 0.38
Batch: 40; loss: 4.71; acc: 0.33
Batch: 60; loss: 4.48; acc: 0.36
Batch: 80; loss: 5.94; acc: 0.34
Batch: 100; loss: 5.44; acc: 0.3
Batch: 120; loss: 5.26; acc: 0.36
Batch: 140; loss: 3.99; acc: 0.42
Batch: 160; loss: 4.15; acc: 0.31
Batch: 180; loss: 4.4; acc: 0.39
Batch: 200; loss: 4.56; acc: 0.39
Batch: 220; loss: 4.26; acc: 0.41
Batch: 240; loss: 5.2; acc: 0.36
Batch: 260; loss: 6.19; acc: 0.3
Batch: 280; loss: 5.77; acc: 0.31
Batch: 300; loss: 6.31; acc: 0.2
Batch: 320; loss: 5.13; acc: 0.25
Batch: 340; loss: 4.86; acc: 0.34
Batch: 360; loss: 5.34; acc: 0.3
Batch: 380; loss: 5.05; acc: 0.44
Batch: 400; loss: 4.56; acc: 0.38
Batch: 420; loss: 4.71; acc: 0.33
Batch: 440; loss: 4.38; acc: 0.38
Batch: 460; loss: 5.23; acc: 0.31
Batch: 480; loss: 3.49; acc: 0.5
Batch: 500; loss: 4.54; acc: 0.38
Batch: 520; loss: 6.52; acc: 0.22
Batch: 540; loss: 4.91; acc: 0.34
Batch: 560; loss: 5.41; acc: 0.36
Batch: 580; loss: 5.0; acc: 0.31
Batch: 600; loss: 4.66; acc: 0.34
Batch: 620; loss: 3.47; acc: 0.44
Train Epoch over. train_loss: 5.02; train_accuracy: 0.33 

Batch: 0; loss: 4.4; acc: 0.33
Batch: 20; loss: 7.01; acc: 0.28
Batch: 40; loss: 4.45; acc: 0.34
Batch: 60; loss: 5.64; acc: 0.16
Batch: 80; loss: 4.34; acc: 0.3
Batch: 100; loss: 6.17; acc: 0.25
Batch: 120; loss: 4.33; acc: 0.36
Batch: 140; loss: 6.46; acc: 0.27
Val Epoch over. val_loss: 4.782718981906867; val_accuracy: 0.31817277070063693 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 5.05; acc: 0.31
Batch: 20; loss: 4.97; acc: 0.38
Batch: 40; loss: 5.12; acc: 0.31
Batch: 60; loss: 6.23; acc: 0.23
Batch: 80; loss: 4.29; acc: 0.36
Batch: 100; loss: 4.1; acc: 0.39
Batch: 120; loss: 6.13; acc: 0.27
Batch: 140; loss: 4.9; acc: 0.38
Batch: 160; loss: 4.43; acc: 0.23
Batch: 180; loss: 5.98; acc: 0.36
Batch: 200; loss: 5.86; acc: 0.31
Batch: 220; loss: 5.01; acc: 0.27
Batch: 240; loss: 6.13; acc: 0.3
Batch: 260; loss: 4.85; acc: 0.42
Batch: 280; loss: 4.4; acc: 0.39
Batch: 300; loss: 6.15; acc: 0.36
Batch: 320; loss: 4.73; acc: 0.36
Batch: 340; loss: 3.95; acc: 0.34
Batch: 360; loss: 5.4; acc: 0.45
Batch: 380; loss: 4.59; acc: 0.39
Batch: 400; loss: 5.21; acc: 0.23
Batch: 420; loss: 4.16; acc: 0.39
Batch: 440; loss: 5.4; acc: 0.36
Batch: 460; loss: 5.9; acc: 0.3
Batch: 480; loss: 4.39; acc: 0.27
Batch: 500; loss: 3.93; acc: 0.39
Batch: 520; loss: 4.16; acc: 0.41
Batch: 540; loss: 5.15; acc: 0.23
Batch: 560; loss: 4.27; acc: 0.38
Batch: 580; loss: 6.17; acc: 0.25
Batch: 600; loss: 4.22; acc: 0.41
Batch: 620; loss: 5.24; acc: 0.3
Train Epoch over. train_loss: 5.05; train_accuracy: 0.33 

Batch: 0; loss: 3.92; acc: 0.34
Batch: 20; loss: 6.99; acc: 0.25
Batch: 40; loss: 5.02; acc: 0.31
Batch: 60; loss: 5.75; acc: 0.19
Batch: 80; loss: 4.78; acc: 0.31
Batch: 100; loss: 6.08; acc: 0.25
Batch: 120; loss: 4.25; acc: 0.36
Batch: 140; loss: 6.44; acc: 0.22
Val Epoch over. val_loss: 5.026549902691204; val_accuracy: 0.31528662420382164 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 4.77; acc: 0.36
Batch: 20; loss: 4.79; acc: 0.36
Batch: 40; loss: 5.18; acc: 0.34
Batch: 60; loss: 4.01; acc: 0.41
Batch: 80; loss: 5.35; acc: 0.3
Batch: 100; loss: 6.44; acc: 0.31
Batch: 120; loss: 5.89; acc: 0.33
Batch: 140; loss: 4.59; acc: 0.27
Batch: 160; loss: 4.25; acc: 0.33
Batch: 180; loss: 5.57; acc: 0.3
Batch: 200; loss: 5.09; acc: 0.27
Batch: 220; loss: 4.88; acc: 0.34
Batch: 240; loss: 5.74; acc: 0.25
Batch: 260; loss: 4.47; acc: 0.44
Batch: 280; loss: 4.58; acc: 0.33
Batch: 300; loss: 5.24; acc: 0.33
Batch: 320; loss: 4.17; acc: 0.33
Batch: 340; loss: 4.11; acc: 0.31
Batch: 360; loss: 5.52; acc: 0.38
Batch: 380; loss: 5.71; acc: 0.25
Batch: 400; loss: 4.23; acc: 0.36
Batch: 420; loss: 4.25; acc: 0.41
Batch: 440; loss: 5.18; acc: 0.25
Batch: 460; loss: 3.92; acc: 0.44
Batch: 480; loss: 4.98; acc: 0.38
Batch: 500; loss: 4.45; acc: 0.34
Batch: 520; loss: 5.51; acc: 0.23
Batch: 540; loss: 5.39; acc: 0.34
Batch: 560; loss: 5.21; acc: 0.34
Batch: 580; loss: 6.19; acc: 0.31
Batch: 600; loss: 4.92; acc: 0.27
Batch: 620; loss: 4.65; acc: 0.39
Train Epoch over. train_loss: 5.03; train_accuracy: 0.33 

Batch: 0; loss: 4.57; acc: 0.3
Batch: 20; loss: 7.54; acc: 0.31
Batch: 40; loss: 4.27; acc: 0.41
Batch: 60; loss: 5.51; acc: 0.28
Batch: 80; loss: 4.56; acc: 0.33
Batch: 100; loss: 6.77; acc: 0.3
Batch: 120; loss: 4.56; acc: 0.41
Batch: 140; loss: 6.21; acc: 0.25
Val Epoch over. val_loss: 4.818235154364519; val_accuracy: 0.3203622611464968 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 3.62; acc: 0.45
Batch: 20; loss: 4.76; acc: 0.33
Batch: 40; loss: 5.27; acc: 0.41
Batch: 60; loss: 5.03; acc: 0.39
Batch: 80; loss: 4.31; acc: 0.41
Batch: 100; loss: 5.91; acc: 0.22
Batch: 120; loss: 5.11; acc: 0.28
Batch: 140; loss: 3.56; acc: 0.45
Batch: 160; loss: 4.07; acc: 0.34
Batch: 180; loss: 5.54; acc: 0.33
Batch: 200; loss: 4.25; acc: 0.38
Batch: 220; loss: 4.93; acc: 0.39
Batch: 240; loss: 4.99; acc: 0.27
Batch: 260; loss: 6.42; acc: 0.25
Batch: 280; loss: 5.64; acc: 0.36
Batch: 300; loss: 3.67; acc: 0.33
Batch: 320; loss: 5.78; acc: 0.27
Batch: 340; loss: 4.31; acc: 0.5
Batch: 360; loss: 6.52; acc: 0.25
Batch: 380; loss: 4.35; acc: 0.3
Batch: 400; loss: 5.98; acc: 0.34
Batch: 420; loss: 4.47; acc: 0.41
Batch: 440; loss: 5.06; acc: 0.27
Batch: 460; loss: 5.39; acc: 0.23
Batch: 480; loss: 3.85; acc: 0.42
Batch: 500; loss: 5.64; acc: 0.27
Batch: 520; loss: 5.01; acc: 0.34
Batch: 540; loss: 4.88; acc: 0.28
Batch: 560; loss: 4.43; acc: 0.33
Batch: 580; loss: 6.25; acc: 0.23
Batch: 600; loss: 6.16; acc: 0.33
Batch: 620; loss: 4.83; acc: 0.2
Train Epoch over. train_loss: 5.04; train_accuracy: 0.33 

Batch: 0; loss: 5.21; acc: 0.33
Batch: 20; loss: 8.07; acc: 0.27
Batch: 40; loss: 4.72; acc: 0.3
Batch: 60; loss: 5.57; acc: 0.27
Batch: 80; loss: 5.82; acc: 0.33
Batch: 100; loss: 7.42; acc: 0.27
Batch: 120; loss: 5.25; acc: 0.38
Batch: 140; loss: 6.89; acc: 0.22
Val Epoch over. val_loss: 5.453137809303915; val_accuracy: 0.30971337579617836 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 5.81; acc: 0.33
Batch: 20; loss: 5.71; acc: 0.28
Batch: 40; loss: 4.09; acc: 0.33
Batch: 60; loss: 6.84; acc: 0.3
Batch: 80; loss: 5.37; acc: 0.34
Batch: 100; loss: 5.75; acc: 0.33
Batch: 120; loss: 4.38; acc: 0.34
Batch: 140; loss: 4.36; acc: 0.45
Batch: 160; loss: 5.05; acc: 0.33
Batch: 180; loss: 4.29; acc: 0.34
Batch: 200; loss: 5.69; acc: 0.34
Batch: 220; loss: 3.7; acc: 0.38
Batch: 240; loss: 5.07; acc: 0.23
Batch: 260; loss: 5.45; acc: 0.31
Batch: 280; loss: 4.64; acc: 0.22
Batch: 300; loss: 4.51; acc: 0.28
Batch: 320; loss: 4.35; acc: 0.41
Batch: 340; loss: 3.15; acc: 0.44
Batch: 360; loss: 4.19; acc: 0.39
Batch: 380; loss: 4.99; acc: 0.39
Batch: 400; loss: 4.25; acc: 0.44
Batch: 420; loss: 3.9; acc: 0.39
Batch: 440; loss: 5.37; acc: 0.27
Batch: 460; loss: 5.99; acc: 0.27
Batch: 480; loss: 4.73; acc: 0.36
Batch: 500; loss: 3.76; acc: 0.39
Batch: 520; loss: 5.66; acc: 0.36
Batch: 540; loss: 5.39; acc: 0.36
Batch: 560; loss: 5.54; acc: 0.23
Batch: 580; loss: 5.26; acc: 0.41
Batch: 600; loss: 5.59; acc: 0.28
Batch: 620; loss: 4.83; acc: 0.28
Train Epoch over. train_loss: 5.04; train_accuracy: 0.33 

Batch: 0; loss: 5.46; acc: 0.33
Batch: 20; loss: 8.52; acc: 0.23
Batch: 40; loss: 5.32; acc: 0.34
Batch: 60; loss: 7.41; acc: 0.3
Batch: 80; loss: 6.15; acc: 0.28
Batch: 100; loss: 6.62; acc: 0.25
Batch: 120; loss: 5.65; acc: 0.39
Batch: 140; loss: 7.53; acc: 0.22
Val Epoch over. val_loss: 5.905052338436151; val_accuracy: 0.3063296178343949 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 6.0; acc: 0.27
Batch: 20; loss: 5.12; acc: 0.27
Batch: 40; loss: 4.94; acc: 0.3
Batch: 60; loss: 4.56; acc: 0.34
Batch: 80; loss: 4.4; acc: 0.3
Batch: 100; loss: 4.81; acc: 0.33
Batch: 120; loss: 5.5; acc: 0.33
Batch: 140; loss: 3.39; acc: 0.48
Batch: 160; loss: 5.22; acc: 0.34
Batch: 180; loss: 4.96; acc: 0.27
Batch: 200; loss: 4.15; acc: 0.27
Batch: 220; loss: 4.68; acc: 0.25
Batch: 240; loss: 3.77; acc: 0.38
Batch: 260; loss: 4.79; acc: 0.34
Batch: 280; loss: 3.86; acc: 0.45
Batch: 300; loss: 4.92; acc: 0.34
Batch: 320; loss: 4.79; acc: 0.38
Batch: 340; loss: 5.23; acc: 0.3
Batch: 360; loss: 4.67; acc: 0.33
Batch: 380; loss: 4.08; acc: 0.38
Batch: 400; loss: 4.86; acc: 0.33
Batch: 420; loss: 3.77; acc: 0.41
Batch: 440; loss: 4.0; acc: 0.38
Batch: 460; loss: 4.49; acc: 0.38
Batch: 480; loss: 5.74; acc: 0.27
Batch: 500; loss: 4.37; acc: 0.31
Batch: 520; loss: 3.9; acc: 0.38
Batch: 540; loss: 4.3; acc: 0.36
Batch: 560; loss: 5.32; acc: 0.33
Batch: 580; loss: 4.85; acc: 0.23
Batch: 600; loss: 4.45; acc: 0.36
Batch: 620; loss: 4.18; acc: 0.42
Train Epoch over. train_loss: 4.51; train_accuracy: 0.35 

Batch: 0; loss: 4.09; acc: 0.38
Batch: 20; loss: 7.02; acc: 0.33
Batch: 40; loss: 4.04; acc: 0.36
Batch: 60; loss: 5.07; acc: 0.22
Batch: 80; loss: 4.18; acc: 0.39
Batch: 100; loss: 6.39; acc: 0.28
Batch: 120; loss: 4.22; acc: 0.41
Batch: 140; loss: 5.89; acc: 0.27
Val Epoch over. val_loss: 4.5605240824875555; val_accuracy: 0.33867436305732485 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.26; acc: 0.38
Batch: 20; loss: 5.6; acc: 0.38
Batch: 40; loss: 4.94; acc: 0.36
Batch: 60; loss: 4.94; acc: 0.34
Batch: 80; loss: 5.1; acc: 0.33
Batch: 100; loss: 3.68; acc: 0.41
Batch: 120; loss: 5.54; acc: 0.25
Batch: 140; loss: 3.9; acc: 0.3
Batch: 160; loss: 5.18; acc: 0.3
Batch: 180; loss: 5.4; acc: 0.34
Batch: 200; loss: 3.87; acc: 0.33
Batch: 220; loss: 4.06; acc: 0.34
Batch: 240; loss: 4.13; acc: 0.3
Batch: 260; loss: 3.63; acc: 0.36
Batch: 280; loss: 4.81; acc: 0.31
Batch: 300; loss: 5.01; acc: 0.33
Batch: 320; loss: 4.3; acc: 0.5
Batch: 340; loss: 4.46; acc: 0.41
Batch: 360; loss: 4.16; acc: 0.38
Batch: 380; loss: 5.0; acc: 0.27
Batch: 400; loss: 4.82; acc: 0.36
Batch: 420; loss: 4.13; acc: 0.39
Batch: 440; loss: 3.66; acc: 0.45
Batch: 460; loss: 3.96; acc: 0.31
Batch: 480; loss: 4.61; acc: 0.33
Batch: 500; loss: 3.48; acc: 0.38
Batch: 520; loss: 3.63; acc: 0.39
Batch: 540; loss: 4.53; acc: 0.36
Batch: 560; loss: 4.53; acc: 0.28
Batch: 580; loss: 4.53; acc: 0.38
Batch: 600; loss: 4.84; acc: 0.25
Batch: 620; loss: 4.99; acc: 0.38
Train Epoch over. train_loss: 4.49; train_accuracy: 0.35 

Batch: 0; loss: 4.25; acc: 0.34
Batch: 20; loss: 7.22; acc: 0.34
Batch: 40; loss: 3.94; acc: 0.39
Batch: 60; loss: 5.09; acc: 0.23
Batch: 80; loss: 4.27; acc: 0.36
Batch: 100; loss: 6.47; acc: 0.27
Batch: 120; loss: 4.27; acc: 0.39
Batch: 140; loss: 6.0; acc: 0.22
Val Epoch over. val_loss: 4.567763386258654; val_accuracy: 0.33937101910828027 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.31; acc: 0.36
Batch: 20; loss: 4.97; acc: 0.34
Batch: 40; loss: 5.1; acc: 0.31
Batch: 60; loss: 3.39; acc: 0.41
Batch: 80; loss: 5.38; acc: 0.34
Batch: 100; loss: 3.96; acc: 0.39
Batch: 120; loss: 4.52; acc: 0.34
Batch: 140; loss: 4.43; acc: 0.3
Batch: 160; loss: 4.48; acc: 0.33
Batch: 180; loss: 3.87; acc: 0.34
Batch: 200; loss: 5.44; acc: 0.38
Batch: 220; loss: 3.81; acc: 0.38
Batch: 240; loss: 4.18; acc: 0.34
Batch: 260; loss: 3.92; acc: 0.39
Batch: 280; loss: 4.79; acc: 0.44
Batch: 300; loss: 5.32; acc: 0.28
Batch: 320; loss: 5.13; acc: 0.33
Batch: 340; loss: 4.48; acc: 0.34
Batch: 360; loss: 3.38; acc: 0.38
Batch: 380; loss: 4.45; acc: 0.38
Batch: 400; loss: 4.62; acc: 0.3
Batch: 420; loss: 4.04; acc: 0.41
Batch: 440; loss: 4.77; acc: 0.28
Batch: 460; loss: 3.31; acc: 0.47
Batch: 480; loss: 4.09; acc: 0.33
Batch: 500; loss: 4.03; acc: 0.36
Batch: 520; loss: 3.48; acc: 0.42
Batch: 540; loss: 4.33; acc: 0.34
Batch: 560; loss: 4.27; acc: 0.31
Batch: 580; loss: 4.98; acc: 0.31
Batch: 600; loss: 4.99; acc: 0.36
Batch: 620; loss: 3.87; acc: 0.38
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.05; acc: 0.36
Batch: 20; loss: 7.08; acc: 0.31
Batch: 40; loss: 4.0; acc: 0.41
Batch: 60; loss: 5.13; acc: 0.2
Batch: 80; loss: 4.11; acc: 0.34
Batch: 100; loss: 6.45; acc: 0.27
Batch: 120; loss: 4.35; acc: 0.38
Batch: 140; loss: 5.96; acc: 0.23
Val Epoch over. val_loss: 4.562026692044204; val_accuracy: 0.33320063694267515 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.47; acc: 0.27
Batch: 20; loss: 4.65; acc: 0.33
Batch: 40; loss: 4.18; acc: 0.31
Batch: 60; loss: 4.83; acc: 0.3
Batch: 80; loss: 5.24; acc: 0.39
Batch: 100; loss: 3.8; acc: 0.45
Batch: 120; loss: 4.66; acc: 0.33
Batch: 140; loss: 3.94; acc: 0.31
Batch: 160; loss: 4.6; acc: 0.33
Batch: 180; loss: 3.47; acc: 0.42
Batch: 200; loss: 3.73; acc: 0.28
Batch: 220; loss: 4.74; acc: 0.36
Batch: 240; loss: 4.52; acc: 0.3
Batch: 260; loss: 4.68; acc: 0.34
Batch: 280; loss: 4.27; acc: 0.36
Batch: 300; loss: 5.02; acc: 0.33
Batch: 320; loss: 5.09; acc: 0.34
Batch: 340; loss: 4.32; acc: 0.39
Batch: 360; loss: 2.84; acc: 0.38
Batch: 380; loss: 6.2; acc: 0.25
Batch: 400; loss: 5.69; acc: 0.33
Batch: 420; loss: 4.16; acc: 0.44
Batch: 440; loss: 4.36; acc: 0.34
Batch: 460; loss: 3.78; acc: 0.42
Batch: 480; loss: 5.52; acc: 0.31
Batch: 500; loss: 6.0; acc: 0.25
Batch: 520; loss: 3.72; acc: 0.34
Batch: 540; loss: 3.85; acc: 0.36
Batch: 560; loss: 3.82; acc: 0.38
Batch: 580; loss: 4.14; acc: 0.39
Batch: 600; loss: 4.37; acc: 0.41
Batch: 620; loss: 5.39; acc: 0.3
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.09; acc: 0.38
Batch: 20; loss: 6.94; acc: 0.36
Batch: 40; loss: 3.98; acc: 0.44
Batch: 60; loss: 5.15; acc: 0.22
Batch: 80; loss: 4.07; acc: 0.36
Batch: 100; loss: 6.38; acc: 0.28
Batch: 120; loss: 4.22; acc: 0.41
Batch: 140; loss: 5.91; acc: 0.3
Val Epoch over. val_loss: 4.525432282951987; val_accuracy: 0.33678343949044587 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 5.24; acc: 0.25
Batch: 20; loss: 4.77; acc: 0.33
Batch: 40; loss: 3.94; acc: 0.41
Batch: 60; loss: 5.85; acc: 0.25
Batch: 80; loss: 4.05; acc: 0.44
Batch: 100; loss: 4.63; acc: 0.41
Batch: 120; loss: 3.86; acc: 0.39
Batch: 140; loss: 4.39; acc: 0.33
Batch: 160; loss: 5.22; acc: 0.31
Batch: 180; loss: 4.69; acc: 0.42
Batch: 200; loss: 4.97; acc: 0.36
Batch: 220; loss: 4.44; acc: 0.38
Batch: 240; loss: 4.74; acc: 0.41
Batch: 260; loss: 4.35; acc: 0.28
Batch: 280; loss: 4.58; acc: 0.3
Batch: 300; loss: 4.6; acc: 0.44
Batch: 320; loss: 3.81; acc: 0.42
Batch: 340; loss: 4.33; acc: 0.3
Batch: 360; loss: 4.05; acc: 0.31
Batch: 380; loss: 4.75; acc: 0.36
Batch: 400; loss: 4.18; acc: 0.31
Batch: 420; loss: 3.64; acc: 0.5
Batch: 440; loss: 3.91; acc: 0.36
Batch: 460; loss: 4.03; acc: 0.44
Batch: 480; loss: 4.59; acc: 0.3
Batch: 500; loss: 5.1; acc: 0.31
Batch: 520; loss: 4.97; acc: 0.41
Batch: 540; loss: 5.58; acc: 0.28
Batch: 560; loss: 4.85; acc: 0.41
Batch: 580; loss: 5.18; acc: 0.3
Batch: 600; loss: 5.22; acc: 0.36
Batch: 620; loss: 4.47; acc: 0.33
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.18; acc: 0.33
Batch: 20; loss: 6.98; acc: 0.3
Batch: 40; loss: 4.07; acc: 0.39
Batch: 60; loss: 5.15; acc: 0.2
Batch: 80; loss: 3.98; acc: 0.33
Batch: 100; loss: 6.49; acc: 0.25
Batch: 120; loss: 4.26; acc: 0.39
Batch: 140; loss: 5.86; acc: 0.27
Val Epoch over. val_loss: 4.556067459142891; val_accuracy: 0.33031449044585987 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 5.82; acc: 0.3
Batch: 20; loss: 4.5; acc: 0.34
Batch: 40; loss: 4.87; acc: 0.27
Batch: 60; loss: 4.22; acc: 0.39
Batch: 80; loss: 4.25; acc: 0.41
Batch: 100; loss: 3.31; acc: 0.41
Batch: 120; loss: 4.17; acc: 0.31
Batch: 140; loss: 4.64; acc: 0.38
Batch: 160; loss: 4.52; acc: 0.31
Batch: 180; loss: 5.26; acc: 0.34
Batch: 200; loss: 4.96; acc: 0.3
Batch: 220; loss: 3.45; acc: 0.41
Batch: 240; loss: 4.94; acc: 0.36
Batch: 260; loss: 4.5; acc: 0.34
Batch: 280; loss: 4.21; acc: 0.41
Batch: 300; loss: 4.3; acc: 0.33
Batch: 320; loss: 4.5; acc: 0.38
Batch: 340; loss: 4.01; acc: 0.39
Batch: 360; loss: 5.35; acc: 0.33
Batch: 380; loss: 3.44; acc: 0.41
Batch: 400; loss: 5.6; acc: 0.39
Batch: 420; loss: 3.37; acc: 0.48
Batch: 440; loss: 4.03; acc: 0.33
Batch: 460; loss: 3.13; acc: 0.44
Batch: 480; loss: 4.35; acc: 0.25
Batch: 500; loss: 4.73; acc: 0.36
Batch: 520; loss: 3.43; acc: 0.48
Batch: 540; loss: 3.56; acc: 0.42
Batch: 560; loss: 3.68; acc: 0.39
Batch: 580; loss: 3.36; acc: 0.41
Batch: 600; loss: 5.25; acc: 0.34
Batch: 620; loss: 5.13; acc: 0.28
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.15; acc: 0.36
Batch: 20; loss: 6.88; acc: 0.33
Batch: 40; loss: 4.1; acc: 0.44
Batch: 60; loss: 5.16; acc: 0.2
Batch: 80; loss: 4.0; acc: 0.34
Batch: 100; loss: 6.33; acc: 0.3
Batch: 120; loss: 4.22; acc: 0.39
Batch: 140; loss: 5.86; acc: 0.25
Val Epoch over. val_loss: 4.524193365862415; val_accuracy: 0.3349920382165605 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.3; acc: 0.33
Batch: 20; loss: 4.8; acc: 0.28
Batch: 40; loss: 4.87; acc: 0.44
Batch: 60; loss: 4.69; acc: 0.31
Batch: 80; loss: 4.81; acc: 0.42
Batch: 100; loss: 4.14; acc: 0.33
Batch: 120; loss: 5.07; acc: 0.33
Batch: 140; loss: 4.66; acc: 0.39
Batch: 160; loss: 5.37; acc: 0.33
Batch: 180; loss: 4.46; acc: 0.34
Batch: 200; loss: 4.05; acc: 0.33
Batch: 220; loss: 5.3; acc: 0.38
Batch: 240; loss: 4.45; acc: 0.36
Batch: 260; loss: 4.03; acc: 0.36
Batch: 280; loss: 5.15; acc: 0.3
Batch: 300; loss: 4.72; acc: 0.42
Batch: 320; loss: 4.35; acc: 0.33
Batch: 340; loss: 4.32; acc: 0.34
Batch: 360; loss: 4.53; acc: 0.39
Batch: 380; loss: 3.04; acc: 0.39
Batch: 400; loss: 3.62; acc: 0.39
Batch: 420; loss: 5.02; acc: 0.38
Batch: 440; loss: 4.18; acc: 0.41
Batch: 460; loss: 4.87; acc: 0.22
Batch: 480; loss: 5.09; acc: 0.3
Batch: 500; loss: 3.73; acc: 0.38
Batch: 520; loss: 4.62; acc: 0.34
Batch: 540; loss: 4.81; acc: 0.34
Batch: 560; loss: 3.88; acc: 0.42
Batch: 580; loss: 4.6; acc: 0.39
Batch: 600; loss: 4.06; acc: 0.39
Batch: 620; loss: 5.13; acc: 0.31
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.15; acc: 0.36
Batch: 20; loss: 6.98; acc: 0.36
Batch: 40; loss: 4.01; acc: 0.39
Batch: 60; loss: 5.17; acc: 0.19
Batch: 80; loss: 4.01; acc: 0.36
Batch: 100; loss: 6.5; acc: 0.27
Batch: 120; loss: 4.22; acc: 0.38
Batch: 140; loss: 5.98; acc: 0.22
Val Epoch over. val_loss: 4.551855032611045; val_accuracy: 0.33160828025477707 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 5.0; acc: 0.28
Batch: 20; loss: 5.32; acc: 0.27
Batch: 40; loss: 5.42; acc: 0.34
Batch: 60; loss: 3.36; acc: 0.38
Batch: 80; loss: 3.65; acc: 0.39
Batch: 100; loss: 4.45; acc: 0.34
Batch: 120; loss: 4.64; acc: 0.41
Batch: 140; loss: 4.72; acc: 0.28
Batch: 160; loss: 4.61; acc: 0.3
Batch: 180; loss: 5.26; acc: 0.31
Batch: 200; loss: 3.48; acc: 0.41
Batch: 220; loss: 4.55; acc: 0.33
Batch: 240; loss: 5.2; acc: 0.28
Batch: 260; loss: 4.94; acc: 0.31
Batch: 280; loss: 4.49; acc: 0.44
Batch: 300; loss: 5.03; acc: 0.3
Batch: 320; loss: 4.48; acc: 0.36
Batch: 340; loss: 4.72; acc: 0.31
Batch: 360; loss: 6.6; acc: 0.38
Batch: 380; loss: 4.81; acc: 0.41
Batch: 400; loss: 4.8; acc: 0.3
Batch: 420; loss: 4.43; acc: 0.41
Batch: 440; loss: 4.76; acc: 0.27
Batch: 460; loss: 4.44; acc: 0.39
Batch: 480; loss: 3.99; acc: 0.41
Batch: 500; loss: 4.7; acc: 0.33
Batch: 520; loss: 4.31; acc: 0.31
Batch: 540; loss: 4.24; acc: 0.31
Batch: 560; loss: 4.34; acc: 0.34
Batch: 580; loss: 5.34; acc: 0.31
Batch: 600; loss: 4.37; acc: 0.44
Batch: 620; loss: 5.5; acc: 0.33
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.05; acc: 0.36
Batch: 20; loss: 6.93; acc: 0.33
Batch: 40; loss: 4.04; acc: 0.41
Batch: 60; loss: 5.22; acc: 0.2
Batch: 80; loss: 3.96; acc: 0.34
Batch: 100; loss: 6.44; acc: 0.25
Batch: 120; loss: 4.25; acc: 0.41
Batch: 140; loss: 5.93; acc: 0.28
Val Epoch over. val_loss: 4.544360880639143; val_accuracy: 0.3351910828025478 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 5.07; acc: 0.28
Batch: 20; loss: 3.93; acc: 0.36
Batch: 40; loss: 4.8; acc: 0.34
Batch: 60; loss: 5.18; acc: 0.31
Batch: 80; loss: 3.27; acc: 0.38
Batch: 100; loss: 5.42; acc: 0.3
Batch: 120; loss: 4.78; acc: 0.33
Batch: 140; loss: 4.71; acc: 0.25
Batch: 160; loss: 4.38; acc: 0.31
Batch: 180; loss: 4.61; acc: 0.39
Batch: 200; loss: 4.42; acc: 0.31
Batch: 220; loss: 3.68; acc: 0.42
Batch: 240; loss: 4.76; acc: 0.38
Batch: 260; loss: 5.16; acc: 0.34
Batch: 280; loss: 4.29; acc: 0.41
Batch: 300; loss: 4.27; acc: 0.36
Batch: 320; loss: 4.84; acc: 0.36
Batch: 340; loss: 3.81; acc: 0.31
Batch: 360; loss: 4.73; acc: 0.27
Batch: 380; loss: 3.46; acc: 0.44
Batch: 400; loss: 4.51; acc: 0.34
Batch: 420; loss: 5.75; acc: 0.33
Batch: 440; loss: 3.28; acc: 0.39
Batch: 460; loss: 4.9; acc: 0.33
Batch: 480; loss: 3.47; acc: 0.41
Batch: 500; loss: 4.44; acc: 0.34
Batch: 520; loss: 3.77; acc: 0.34
Batch: 540; loss: 5.81; acc: 0.27
Batch: 560; loss: 5.74; acc: 0.31
Batch: 580; loss: 5.72; acc: 0.25
Batch: 600; loss: 5.25; acc: 0.28
Batch: 620; loss: 4.92; acc: 0.28
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.15; acc: 0.33
Batch: 20; loss: 6.86; acc: 0.33
Batch: 40; loss: 4.11; acc: 0.45
Batch: 60; loss: 5.2; acc: 0.23
Batch: 80; loss: 4.01; acc: 0.34
Batch: 100; loss: 6.38; acc: 0.27
Batch: 120; loss: 4.31; acc: 0.36
Batch: 140; loss: 5.88; acc: 0.27
Val Epoch over. val_loss: 4.554426944939194; val_accuracy: 0.3331011146496815 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.85; acc: 0.28
Batch: 20; loss: 5.49; acc: 0.31
Batch: 40; loss: 5.74; acc: 0.3
Batch: 60; loss: 4.37; acc: 0.38
Batch: 80; loss: 5.07; acc: 0.39
Batch: 100; loss: 4.4; acc: 0.34
Batch: 120; loss: 4.13; acc: 0.42
Batch: 140; loss: 4.63; acc: 0.33
Batch: 160; loss: 4.52; acc: 0.25
Batch: 180; loss: 3.48; acc: 0.33
Batch: 200; loss: 4.35; acc: 0.38
Batch: 220; loss: 3.6; acc: 0.44
Batch: 240; loss: 4.14; acc: 0.42
Batch: 260; loss: 5.14; acc: 0.23
Batch: 280; loss: 5.37; acc: 0.31
Batch: 300; loss: 5.43; acc: 0.25
Batch: 320; loss: 4.98; acc: 0.41
Batch: 340; loss: 3.81; acc: 0.33
Batch: 360; loss: 4.28; acc: 0.28
Batch: 380; loss: 4.11; acc: 0.33
Batch: 400; loss: 4.6; acc: 0.44
Batch: 420; loss: 4.6; acc: 0.36
Batch: 440; loss: 3.78; acc: 0.39
Batch: 460; loss: 3.8; acc: 0.42
Batch: 480; loss: 4.47; acc: 0.31
Batch: 500; loss: 4.21; acc: 0.39
Batch: 520; loss: 3.3; acc: 0.42
Batch: 540; loss: 4.72; acc: 0.31
Batch: 560; loss: 4.55; acc: 0.33
Batch: 580; loss: 3.97; acc: 0.39
Batch: 600; loss: 4.77; acc: 0.34
Batch: 620; loss: 5.18; acc: 0.25
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.11; acc: 0.36
Batch: 20; loss: 7.0; acc: 0.34
Batch: 40; loss: 3.97; acc: 0.44
Batch: 60; loss: 5.24; acc: 0.2
Batch: 80; loss: 4.08; acc: 0.36
Batch: 100; loss: 6.33; acc: 0.28
Batch: 120; loss: 4.3; acc: 0.38
Batch: 140; loss: 5.93; acc: 0.25
Val Epoch over. val_loss: 4.525597622440119; val_accuracy: 0.3338972929936306 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.2; acc: 0.33
Batch: 20; loss: 4.17; acc: 0.3
Batch: 40; loss: 5.47; acc: 0.28
Batch: 60; loss: 5.29; acc: 0.33
Batch: 80; loss: 3.88; acc: 0.34
Batch: 100; loss: 5.5; acc: 0.33
Batch: 120; loss: 4.68; acc: 0.41
Batch: 140; loss: 4.5; acc: 0.33
Batch: 160; loss: 4.39; acc: 0.38
Batch: 180; loss: 4.53; acc: 0.27
Batch: 200; loss: 6.32; acc: 0.27
Batch: 220; loss: 4.56; acc: 0.34
Batch: 240; loss: 4.17; acc: 0.28
Batch: 260; loss: 3.38; acc: 0.47
Batch: 280; loss: 6.25; acc: 0.25
Batch: 300; loss: 4.61; acc: 0.34
Batch: 320; loss: 4.62; acc: 0.34
Batch: 340; loss: 3.48; acc: 0.38
Batch: 360; loss: 4.59; acc: 0.34
Batch: 380; loss: 4.16; acc: 0.34
Batch: 400; loss: 6.19; acc: 0.25
Batch: 420; loss: 4.14; acc: 0.36
Batch: 440; loss: 4.85; acc: 0.39
Batch: 460; loss: 2.9; acc: 0.47
Batch: 480; loss: 4.73; acc: 0.45
Batch: 500; loss: 5.1; acc: 0.3
Batch: 520; loss: 4.86; acc: 0.25
Batch: 540; loss: 4.32; acc: 0.3
Batch: 560; loss: 5.23; acc: 0.31
Batch: 580; loss: 5.87; acc: 0.27
Batch: 600; loss: 4.09; acc: 0.36
Batch: 620; loss: 5.2; acc: 0.3
Train Epoch over. train_loss: 4.47; train_accuracy: 0.35 

Batch: 0; loss: 4.07; acc: 0.34
Batch: 20; loss: 6.94; acc: 0.33
Batch: 40; loss: 3.93; acc: 0.44
Batch: 60; loss: 5.16; acc: 0.22
Batch: 80; loss: 4.14; acc: 0.34
Batch: 100; loss: 6.41; acc: 0.28
Batch: 120; loss: 4.34; acc: 0.38
Batch: 140; loss: 6.0; acc: 0.23
Val Epoch over. val_loss: 4.540985780157102; val_accuracy: 0.3346934713375796 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.16; acc: 0.47
Batch: 20; loss: 4.07; acc: 0.38
Batch: 40; loss: 4.85; acc: 0.31
Batch: 60; loss: 5.07; acc: 0.34
Batch: 80; loss: 4.84; acc: 0.34
Batch: 100; loss: 4.05; acc: 0.33
Batch: 120; loss: 4.87; acc: 0.34
Batch: 140; loss: 4.83; acc: 0.27
Batch: 160; loss: 4.1; acc: 0.3
Batch: 180; loss: 3.58; acc: 0.45
Batch: 200; loss: 4.28; acc: 0.34
Batch: 220; loss: 5.25; acc: 0.34
Batch: 240; loss: 4.54; acc: 0.34
Batch: 260; loss: 4.79; acc: 0.3
Batch: 280; loss: 3.39; acc: 0.34
Batch: 300; loss: 4.61; acc: 0.33
Batch: 320; loss: 5.0; acc: 0.3
Batch: 340; loss: 4.94; acc: 0.33
Batch: 360; loss: 4.37; acc: 0.36
Batch: 380; loss: 3.93; acc: 0.34
Batch: 400; loss: 4.39; acc: 0.34
Batch: 420; loss: 4.57; acc: 0.41
Batch: 440; loss: 3.85; acc: 0.38
Batch: 460; loss: 4.17; acc: 0.36
Batch: 480; loss: 5.02; acc: 0.33
Batch: 500; loss: 3.38; acc: 0.41
Batch: 520; loss: 4.07; acc: 0.31
Batch: 540; loss: 4.25; acc: 0.39
Batch: 560; loss: 4.58; acc: 0.34
Batch: 580; loss: 5.25; acc: 0.3
Batch: 600; loss: 5.56; acc: 0.31
Batch: 620; loss: 4.87; acc: 0.33
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.08; acc: 0.33
Batch: 20; loss: 6.96; acc: 0.34
Batch: 40; loss: 3.89; acc: 0.44
Batch: 60; loss: 5.17; acc: 0.2
Batch: 80; loss: 4.16; acc: 0.34
Batch: 100; loss: 6.47; acc: 0.27
Batch: 120; loss: 4.38; acc: 0.38
Batch: 140; loss: 6.09; acc: 0.22
Val Epoch over. val_loss: 4.557250290159967; val_accuracy: 0.33320063694267515 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.61; acc: 0.36
Batch: 20; loss: 5.23; acc: 0.27
Batch: 40; loss: 4.2; acc: 0.36
Batch: 60; loss: 5.69; acc: 0.36
Batch: 80; loss: 4.62; acc: 0.39
Batch: 100; loss: 5.35; acc: 0.38
Batch: 120; loss: 5.08; acc: 0.25
Batch: 140; loss: 4.46; acc: 0.42
Batch: 160; loss: 4.91; acc: 0.38
Batch: 180; loss: 4.11; acc: 0.33
Batch: 200; loss: 4.43; acc: 0.31
Batch: 220; loss: 4.72; acc: 0.31
Batch: 240; loss: 4.45; acc: 0.36
Batch: 260; loss: 3.65; acc: 0.36
Batch: 280; loss: 3.88; acc: 0.38
Batch: 300; loss: 5.02; acc: 0.33
Batch: 320; loss: 4.96; acc: 0.31
Batch: 340; loss: 3.52; acc: 0.41
Batch: 360; loss: 5.07; acc: 0.41
Batch: 380; loss: 5.89; acc: 0.25
Batch: 400; loss: 4.73; acc: 0.33
Batch: 420; loss: 4.21; acc: 0.38
Batch: 440; loss: 3.44; acc: 0.47
Batch: 460; loss: 3.93; acc: 0.42
Batch: 480; loss: 4.31; acc: 0.41
Batch: 500; loss: 4.8; acc: 0.3
Batch: 520; loss: 4.92; acc: 0.3
Batch: 540; loss: 4.13; acc: 0.3
Batch: 560; loss: 4.27; acc: 0.34
Batch: 580; loss: 4.57; acc: 0.31
Batch: 600; loss: 4.76; acc: 0.34
Batch: 620; loss: 4.7; acc: 0.33
Train Epoch over. train_loss: 4.48; train_accuracy: 0.35 

Batch: 0; loss: 4.07; acc: 0.34
Batch: 20; loss: 6.96; acc: 0.31
Batch: 40; loss: 3.86; acc: 0.42
Batch: 60; loss: 5.13; acc: 0.22
Batch: 80; loss: 4.24; acc: 0.31
Batch: 100; loss: 6.48; acc: 0.28
Batch: 120; loss: 4.43; acc: 0.36
Batch: 140; loss: 6.19; acc: 0.23
Val Epoch over. val_loss: 4.569140040950411; val_accuracy: 0.3338972929936306 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.74; acc: 0.27
Batch: 20; loss: 3.88; acc: 0.3
Batch: 40; loss: 3.92; acc: 0.44
Batch: 60; loss: 4.57; acc: 0.25
Batch: 80; loss: 5.29; acc: 0.39
Batch: 100; loss: 6.68; acc: 0.25
Batch: 120; loss: 4.51; acc: 0.3
Batch: 140; loss: 3.6; acc: 0.31
Batch: 160; loss: 3.49; acc: 0.45
Batch: 180; loss: 4.26; acc: 0.34
Batch: 200; loss: 5.4; acc: 0.36
Batch: 220; loss: 4.69; acc: 0.28
Batch: 240; loss: 5.06; acc: 0.31
Batch: 260; loss: 4.57; acc: 0.33
Batch: 280; loss: 5.14; acc: 0.33
Batch: 300; loss: 4.16; acc: 0.36
Batch: 320; loss: 4.12; acc: 0.38
Batch: 340; loss: 4.81; acc: 0.27
Batch: 360; loss: 4.35; acc: 0.34
Batch: 380; loss: 4.3; acc: 0.28
Batch: 400; loss: 4.09; acc: 0.38
Batch: 420; loss: 3.89; acc: 0.42
Batch: 440; loss: 4.25; acc: 0.3
Batch: 460; loss: 4.26; acc: 0.34
Batch: 480; loss: 3.8; acc: 0.42
Batch: 500; loss: 3.46; acc: 0.44
Batch: 520; loss: 4.85; acc: 0.33
Batch: 540; loss: 4.02; acc: 0.34
Batch: 560; loss: 4.95; acc: 0.33
Batch: 580; loss: 4.33; acc: 0.39
Batch: 600; loss: 3.96; acc: 0.31
Batch: 620; loss: 5.68; acc: 0.25
Train Epoch over. train_loss: 4.49; train_accuracy: 0.35 

Batch: 0; loss: 4.09; acc: 0.36
Batch: 20; loss: 6.93; acc: 0.3
Batch: 40; loss: 3.83; acc: 0.42
Batch: 60; loss: 5.16; acc: 0.22
Batch: 80; loss: 4.28; acc: 0.33
Batch: 100; loss: 6.46; acc: 0.28
Batch: 120; loss: 4.48; acc: 0.36
Batch: 140; loss: 6.24; acc: 0.25
Val Epoch over. val_loss: 4.574649657413459; val_accuracy: 0.336484872611465 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.58; acc: 0.28
Batch: 20; loss: 5.03; acc: 0.28
Batch: 40; loss: 4.49; acc: 0.28
Batch: 60; loss: 5.19; acc: 0.3
Batch: 80; loss: 4.78; acc: 0.33
Batch: 100; loss: 4.91; acc: 0.28
Batch: 120; loss: 4.8; acc: 0.36
Batch: 140; loss: 3.75; acc: 0.5
Batch: 160; loss: 4.2; acc: 0.41
Batch: 180; loss: 3.4; acc: 0.41
Batch: 200; loss: 3.33; acc: 0.44
Batch: 220; loss: 4.04; acc: 0.27
Batch: 240; loss: 3.99; acc: 0.42
Batch: 260; loss: 4.65; acc: 0.19
Batch: 280; loss: 4.23; acc: 0.36
Batch: 300; loss: 5.14; acc: 0.36
Batch: 320; loss: 3.96; acc: 0.44
Batch: 340; loss: 3.92; acc: 0.38
Batch: 360; loss: 4.16; acc: 0.41
Batch: 380; loss: 3.95; acc: 0.47
Batch: 400; loss: 4.71; acc: 0.33
Batch: 420; loss: 5.51; acc: 0.27
Batch: 440; loss: 6.52; acc: 0.3
Batch: 460; loss: 5.01; acc: 0.31
Batch: 480; loss: 5.07; acc: 0.41
Batch: 500; loss: 4.51; acc: 0.31
Batch: 520; loss: 4.28; acc: 0.38
Batch: 540; loss: 5.24; acc: 0.27
Batch: 560; loss: 5.92; acc: 0.33
Batch: 580; loss: 4.79; acc: 0.34
Batch: 600; loss: 4.5; acc: 0.34
Batch: 620; loss: 4.74; acc: 0.36
Train Epoch over. train_loss: 4.5; train_accuracy: 0.35 

Batch: 0; loss: 4.11; acc: 0.34
Batch: 20; loss: 6.96; acc: 0.31
Batch: 40; loss: 3.83; acc: 0.41
Batch: 60; loss: 5.12; acc: 0.23
Batch: 80; loss: 4.32; acc: 0.34
Batch: 100; loss: 6.51; acc: 0.27
Batch: 120; loss: 4.49; acc: 0.36
Batch: 140; loss: 6.29; acc: 0.25
Val Epoch over. val_loss: 4.593140641595148; val_accuracy: 0.3369824840764331 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.56; acc: 0.36
Batch: 20; loss: 3.87; acc: 0.42
Batch: 40; loss: 4.5; acc: 0.44
Batch: 60; loss: 5.73; acc: 0.3
Batch: 80; loss: 4.97; acc: 0.28
Batch: 100; loss: 3.88; acc: 0.38
Batch: 120; loss: 5.02; acc: 0.39
Batch: 140; loss: 4.64; acc: 0.34
Batch: 160; loss: 4.57; acc: 0.33
Batch: 180; loss: 5.03; acc: 0.31
Batch: 200; loss: 4.18; acc: 0.41
Batch: 220; loss: 3.64; acc: 0.41
Batch: 240; loss: 3.46; acc: 0.36
Batch: 260; loss: 3.73; acc: 0.34
Batch: 280; loss: 4.77; acc: 0.33
Batch: 300; loss: 5.04; acc: 0.28
Batch: 320; loss: 4.26; acc: 0.42
Batch: 340; loss: 4.7; acc: 0.33
Batch: 360; loss: 3.92; acc: 0.41
Batch: 380; loss: 4.47; acc: 0.38
Batch: 400; loss: 4.84; acc: 0.3
Batch: 420; loss: 4.11; acc: 0.38
Batch: 440; loss: 4.45; acc: 0.3
Batch: 460; loss: 3.47; acc: 0.41
Batch: 480; loss: 4.85; acc: 0.3
Batch: 500; loss: 4.93; acc: 0.33
Batch: 520; loss: 4.64; acc: 0.34
Batch: 540; loss: 5.52; acc: 0.28
Batch: 560; loss: 5.63; acc: 0.25
Batch: 580; loss: 4.4; acc: 0.28
Batch: 600; loss: 4.6; acc: 0.31
Batch: 620; loss: 5.08; acc: 0.39
Train Epoch over. train_loss: 4.51; train_accuracy: 0.35 

Batch: 0; loss: 4.1; acc: 0.31
Batch: 20; loss: 6.93; acc: 0.33
Batch: 40; loss: 3.83; acc: 0.39
Batch: 60; loss: 5.1; acc: 0.23
Batch: 80; loss: 4.37; acc: 0.33
Batch: 100; loss: 6.5; acc: 0.25
Batch: 120; loss: 4.53; acc: 0.34
Batch: 140; loss: 6.35; acc: 0.23
Val Epoch over. val_loss: 4.6064126218200485; val_accuracy: 0.3362858280254777 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.85; acc: 0.31
Batch: 20; loss: 5.43; acc: 0.31
Batch: 40; loss: 3.48; acc: 0.39
Batch: 60; loss: 4.63; acc: 0.39
Batch: 80; loss: 3.5; acc: 0.44
Batch: 100; loss: 4.22; acc: 0.31
Batch: 120; loss: 3.87; acc: 0.42
Batch: 140; loss: 3.3; acc: 0.41
Batch: 160; loss: 4.87; acc: 0.25
Batch: 180; loss: 4.22; acc: 0.34
Batch: 200; loss: 3.71; acc: 0.36
Batch: 220; loss: 5.1; acc: 0.34
Batch: 240; loss: 3.3; acc: 0.44
Batch: 260; loss: 4.46; acc: 0.38
Batch: 280; loss: 4.78; acc: 0.31
Batch: 300; loss: 4.48; acc: 0.44
Batch: 320; loss: 3.42; acc: 0.44
Batch: 340; loss: 3.93; acc: 0.39
Batch: 360; loss: 4.87; acc: 0.34
Batch: 380; loss: 4.35; acc: 0.33
Batch: 400; loss: 4.13; acc: 0.34
Batch: 420; loss: 4.67; acc: 0.34
Batch: 440; loss: 5.49; acc: 0.33
Batch: 460; loss: 5.25; acc: 0.31
Batch: 480; loss: 4.31; acc: 0.36
Batch: 500; loss: 4.51; acc: 0.38
Batch: 520; loss: 4.77; acc: 0.31
Batch: 540; loss: 3.96; acc: 0.42
Batch: 560; loss: 4.28; acc: 0.36
Batch: 580; loss: 5.35; acc: 0.31
Batch: 600; loss: 6.02; acc: 0.28
Batch: 620; loss: 3.58; acc: 0.34
Train Epoch over. train_loss: 4.51; train_accuracy: 0.35 

Batch: 0; loss: 4.13; acc: 0.31
Batch: 20; loss: 6.95; acc: 0.31
Batch: 40; loss: 3.83; acc: 0.42
Batch: 60; loss: 5.13; acc: 0.23
Batch: 80; loss: 4.4; acc: 0.31
Batch: 100; loss: 6.51; acc: 0.23
Batch: 120; loss: 4.52; acc: 0.36
Batch: 140; loss: 6.38; acc: 0.23
Val Epoch over. val_loss: 4.607831525195176; val_accuracy: 0.33767914012738853 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.74; acc: 0.31
Batch: 20; loss: 4.64; acc: 0.33
Batch: 40; loss: 3.63; acc: 0.38
Batch: 60; loss: 4.33; acc: 0.3
Batch: 80; loss: 4.33; acc: 0.39
Batch: 100; loss: 3.36; acc: 0.41
Batch: 120; loss: 3.82; acc: 0.41
Batch: 140; loss: 5.25; acc: 0.22
Batch: 160; loss: 4.03; acc: 0.34
Batch: 180; loss: 4.29; acc: 0.33
Batch: 200; loss: 4.49; acc: 0.33
Batch: 220; loss: 3.46; acc: 0.41
Batch: 240; loss: 4.87; acc: 0.3
Batch: 260; loss: 4.48; acc: 0.36
Batch: 280; loss: 4.16; acc: 0.31
Batch: 300; loss: 4.5; acc: 0.28
Batch: 320; loss: 5.08; acc: 0.25
Batch: 340; loss: 3.23; acc: 0.42
Batch: 360; loss: 4.63; acc: 0.39
Batch: 380; loss: 5.01; acc: 0.28
Batch: 400; loss: 5.46; acc: 0.27
Batch: 420; loss: 4.5; acc: 0.38
Batch: 440; loss: 3.45; acc: 0.42
Batch: 460; loss: 4.12; acc: 0.41
Batch: 480; loss: 4.2; acc: 0.34
Batch: 500; loss: 4.74; acc: 0.33
Batch: 520; loss: 4.69; acc: 0.27
Batch: 540; loss: 4.77; acc: 0.36
Batch: 560; loss: 4.85; acc: 0.34
Batch: 580; loss: 4.27; acc: 0.38
Batch: 600; loss: 4.11; acc: 0.33
Batch: 620; loss: 3.56; acc: 0.41
Train Epoch over. train_loss: 4.52; train_accuracy: 0.35 

Batch: 0; loss: 4.13; acc: 0.31
Batch: 20; loss: 6.91; acc: 0.31
Batch: 40; loss: 3.83; acc: 0.42
Batch: 60; loss: 5.14; acc: 0.25
Batch: 80; loss: 4.44; acc: 0.31
Batch: 100; loss: 6.5; acc: 0.22
Batch: 120; loss: 4.52; acc: 0.34
Batch: 140; loss: 6.41; acc: 0.23
Val Epoch over. val_loss: 4.615526009517111; val_accuracy: 0.3368829617834395 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 5.25; acc: 0.31
Batch: 20; loss: 5.09; acc: 0.3
Batch: 40; loss: 5.28; acc: 0.34
Batch: 60; loss: 4.85; acc: 0.3
Batch: 80; loss: 5.62; acc: 0.27
Batch: 100; loss: 5.05; acc: 0.34
Batch: 120; loss: 4.32; acc: 0.36
Batch: 140; loss: 3.93; acc: 0.38
Batch: 160; loss: 4.95; acc: 0.39
Batch: 180; loss: 4.04; acc: 0.36
Batch: 200; loss: 4.28; acc: 0.38
Batch: 220; loss: 3.7; acc: 0.45
Batch: 240; loss: 4.32; acc: 0.41
Batch: 260; loss: 4.33; acc: 0.34
Batch: 280; loss: 5.32; acc: 0.33
Batch: 300; loss: 5.64; acc: 0.28
Batch: 320; loss: 5.1; acc: 0.3
Batch: 340; loss: 4.1; acc: 0.33
Batch: 360; loss: 4.54; acc: 0.33
Batch: 380; loss: 4.58; acc: 0.41
Batch: 400; loss: 4.27; acc: 0.3
Batch: 420; loss: 4.74; acc: 0.33
Batch: 440; loss: 4.82; acc: 0.39
Batch: 460; loss: 4.22; acc: 0.36
Batch: 480; loss: 3.56; acc: 0.45
Batch: 500; loss: 5.98; acc: 0.28
Batch: 520; loss: 4.98; acc: 0.38
Batch: 540; loss: 5.02; acc: 0.23
Batch: 560; loss: 4.32; acc: 0.41
Batch: 580; loss: 4.0; acc: 0.28
Batch: 600; loss: 5.02; acc: 0.3
Batch: 620; loss: 4.78; acc: 0.23
Train Epoch over. train_loss: 4.52; train_accuracy: 0.35 

Batch: 0; loss: 4.15; acc: 0.33
Batch: 20; loss: 6.95; acc: 0.31
Batch: 40; loss: 3.83; acc: 0.42
Batch: 60; loss: 5.11; acc: 0.25
Batch: 80; loss: 4.49; acc: 0.31
Batch: 100; loss: 6.52; acc: 0.22
Batch: 120; loss: 4.56; acc: 0.34
Batch: 140; loss: 6.47; acc: 0.25
Val Epoch over. val_loss: 4.6275533111232106; val_accuracy: 0.3385748407643312 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 5.97; acc: 0.33
Batch: 20; loss: 4.46; acc: 0.42
Batch: 40; loss: 4.58; acc: 0.36
Batch: 60; loss: 3.66; acc: 0.47
Batch: 80; loss: 4.76; acc: 0.27
Batch: 100; loss: 5.26; acc: 0.3
Batch: 120; loss: 3.96; acc: 0.34
Batch: 140; loss: 4.27; acc: 0.34
Batch: 160; loss: 3.87; acc: 0.45
Batch: 180; loss: 3.96; acc: 0.42
Batch: 200; loss: 3.31; acc: 0.44
Batch: 220; loss: 4.29; acc: 0.39
Batch: 240; loss: 3.59; acc: 0.45
Batch: 260; loss: 4.17; acc: 0.39
Batch: 280; loss: 4.85; acc: 0.36
Batch: 300; loss: 4.18; acc: 0.36
Batch: 320; loss: 5.75; acc: 0.3
Batch: 340; loss: 5.68; acc: 0.33
Batch: 360; loss: 5.01; acc: 0.33
Batch: 380; loss: 5.0; acc: 0.31
Batch: 400; loss: 5.48; acc: 0.28
Batch: 420; loss: 5.24; acc: 0.33
Batch: 440; loss: 3.64; acc: 0.39
Batch: 460; loss: 5.33; acc: 0.42
Batch: 480; loss: 4.48; acc: 0.41
Batch: 500; loss: 5.2; acc: 0.3
Batch: 520; loss: 4.9; acc: 0.23
Batch: 540; loss: 3.98; acc: 0.42
Batch: 560; loss: 5.44; acc: 0.25
Batch: 580; loss: 3.68; acc: 0.31
Batch: 600; loss: 4.87; acc: 0.3
Batch: 620; loss: 4.19; acc: 0.38
Train Epoch over. train_loss: 4.53; train_accuracy: 0.35 

Batch: 0; loss: 4.15; acc: 0.31
Batch: 20; loss: 6.91; acc: 0.31
Batch: 40; loss: 3.85; acc: 0.44
Batch: 60; loss: 5.13; acc: 0.23
Batch: 80; loss: 4.5; acc: 0.31
Batch: 100; loss: 6.52; acc: 0.23
Batch: 120; loss: 4.52; acc: 0.38
Batch: 140; loss: 6.53; acc: 0.27
Val Epoch over. val_loss: 4.638778660707413; val_accuracy: 0.3375796178343949 

plots/subspace_training/lenet/2020-01-10 14:00:56/d_dim_50_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 20871
elements in E: 4442600
fraction nonzero: 0.004697924638725071
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.99; acc: 0.39
Batch: 40; loss: 3.93; acc: 0.31
Batch: 60; loss: 3.02; acc: 0.41
Batch: 80; loss: 2.86; acc: 0.39
Batch: 100; loss: 2.07; acc: 0.5
Batch: 120; loss: 2.67; acc: 0.44
Batch: 140; loss: 2.56; acc: 0.45
Batch: 160; loss: 2.66; acc: 0.44
Batch: 180; loss: 2.5; acc: 0.52
Batch: 200; loss: 2.53; acc: 0.5
Batch: 220; loss: 2.69; acc: 0.47
Batch: 240; loss: 2.6; acc: 0.48
Batch: 260; loss: 2.89; acc: 0.38
Batch: 280; loss: 1.68; acc: 0.58
Batch: 300; loss: 1.48; acc: 0.58
Batch: 320; loss: 2.92; acc: 0.52
Batch: 340; loss: 2.13; acc: 0.52
Batch: 360; loss: 2.07; acc: 0.55
Batch: 380; loss: 2.94; acc: 0.45
Batch: 400; loss: 2.4; acc: 0.44
Batch: 420; loss: 2.42; acc: 0.52
Batch: 440; loss: 2.23; acc: 0.56
Batch: 460; loss: 1.77; acc: 0.61
Batch: 480; loss: 2.28; acc: 0.52
Batch: 500; loss: 2.98; acc: 0.45
Batch: 520; loss: 3.22; acc: 0.41
Batch: 540; loss: 1.61; acc: 0.59
Batch: 560; loss: 1.71; acc: 0.52
Batch: 580; loss: 1.93; acc: 0.61
Batch: 600; loss: 2.72; acc: 0.42
Batch: 620; loss: 2.45; acc: 0.44
Train Epoch over. train_loss: 2.59; train_accuracy: 0.48 

Batch: 0; loss: 2.25; acc: 0.47
Batch: 20; loss: 2.65; acc: 0.52
Batch: 40; loss: 2.26; acc: 0.52
Batch: 60; loss: 1.56; acc: 0.69
Batch: 80; loss: 2.13; acc: 0.52
Batch: 100; loss: 2.12; acc: 0.47
Batch: 120; loss: 1.29; acc: 0.72
Batch: 140; loss: 3.66; acc: 0.28
Val Epoch over. val_loss: 2.4170201113269587; val_accuracy: 0.5101512738853503 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 2.68; acc: 0.45
Batch: 20; loss: 2.43; acc: 0.56
Batch: 40; loss: 2.17; acc: 0.47
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.85; acc: 0.58
Batch: 100; loss: 2.09; acc: 0.53
Batch: 120; loss: 2.16; acc: 0.53
Batch: 140; loss: 1.8; acc: 0.53
Batch: 160; loss: 2.34; acc: 0.5
Batch: 180; loss: 2.25; acc: 0.58
Batch: 200; loss: 3.15; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.59
Batch: 240; loss: 3.04; acc: 0.58
Batch: 260; loss: 1.47; acc: 0.58
Batch: 280; loss: 2.5; acc: 0.52
Batch: 300; loss: 1.97; acc: 0.55
Batch: 320; loss: 1.63; acc: 0.62
Batch: 340; loss: 2.23; acc: 0.53
Batch: 360; loss: 3.09; acc: 0.48
Batch: 380; loss: 1.54; acc: 0.58
Batch: 400; loss: 1.91; acc: 0.62
Batch: 420; loss: 2.45; acc: 0.53
Batch: 440; loss: 1.71; acc: 0.61
Batch: 460; loss: 2.8; acc: 0.47
Batch: 480; loss: 2.14; acc: 0.56
Batch: 500; loss: 2.0; acc: 0.59
Batch: 520; loss: 2.49; acc: 0.58
Batch: 540; loss: 1.85; acc: 0.58
Batch: 560; loss: 1.6; acc: 0.58
Batch: 580; loss: 2.05; acc: 0.48
Batch: 600; loss: 2.36; acc: 0.55
Batch: 620; loss: 2.8; acc: 0.47
Train Epoch over. train_loss: 2.18; train_accuracy: 0.54 

Batch: 0; loss: 2.44; acc: 0.48
Batch: 20; loss: 2.3; acc: 0.48
Batch: 40; loss: 2.0; acc: 0.52
Batch: 60; loss: 1.32; acc: 0.66
Batch: 80; loss: 1.67; acc: 0.62
Batch: 100; loss: 1.93; acc: 0.59
Batch: 120; loss: 1.16; acc: 0.69
Batch: 140; loss: 3.52; acc: 0.39
Val Epoch over. val_loss: 2.058642658458394; val_accuracy: 0.5569267515923567 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 1.68; acc: 0.56
Batch: 20; loss: 2.22; acc: 0.47
Batch: 40; loss: 1.72; acc: 0.53
Batch: 60; loss: 3.06; acc: 0.52
Batch: 80; loss: 2.22; acc: 0.59
Batch: 100; loss: 2.21; acc: 0.58
Batch: 120; loss: 2.49; acc: 0.47
Batch: 140; loss: 2.51; acc: 0.45
Batch: 160; loss: 1.71; acc: 0.56
Batch: 180; loss: 2.41; acc: 0.5
Batch: 200; loss: 2.08; acc: 0.53
Batch: 220; loss: 2.53; acc: 0.48
Batch: 240; loss: 2.67; acc: 0.45
Batch: 260; loss: 1.85; acc: 0.58
Batch: 280; loss: 2.68; acc: 0.44
Batch: 300; loss: 1.79; acc: 0.58
Batch: 320; loss: 3.07; acc: 0.47
Batch: 340; loss: 2.91; acc: 0.52
Batch: 360; loss: 1.72; acc: 0.72
Batch: 380; loss: 2.22; acc: 0.56
Batch: 400; loss: 2.09; acc: 0.53
Batch: 420; loss: 2.76; acc: 0.44
Batch: 440; loss: 0.81; acc: 0.7
Batch: 460; loss: 2.57; acc: 0.52
Batch: 480; loss: 2.59; acc: 0.56
Batch: 500; loss: 2.16; acc: 0.64
Batch: 520; loss: 1.87; acc: 0.59
Batch: 540; loss: 1.46; acc: 0.61
Batch: 560; loss: 1.94; acc: 0.67
Batch: 580; loss: 2.01; acc: 0.5
Batch: 600; loss: 1.46; acc: 0.61
Batch: 620; loss: 2.02; acc: 0.53
Train Epoch over. train_loss: 2.2; train_accuracy: 0.55 

Batch: 0; loss: 2.84; acc: 0.53
Batch: 20; loss: 2.37; acc: 0.52
Batch: 40; loss: 1.53; acc: 0.58
Batch: 60; loss: 1.41; acc: 0.66
Batch: 80; loss: 2.14; acc: 0.58
Batch: 100; loss: 2.13; acc: 0.56
Batch: 120; loss: 1.32; acc: 0.72
Batch: 140; loss: 3.86; acc: 0.34
Val Epoch over. val_loss: 2.256589327648187; val_accuracy: 0.5574243630573248 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 2.23; acc: 0.66
Batch: 20; loss: 3.1; acc: 0.47
Batch: 40; loss: 2.1; acc: 0.53
Batch: 60; loss: 1.86; acc: 0.56
Batch: 80; loss: 2.18; acc: 0.58
Batch: 100; loss: 2.06; acc: 0.52
Batch: 120; loss: 1.96; acc: 0.61
Batch: 140; loss: 2.48; acc: 0.53
Batch: 160; loss: 2.56; acc: 0.58
Batch: 180; loss: 2.27; acc: 0.5
Batch: 200; loss: 1.76; acc: 0.64
Batch: 220; loss: 2.5; acc: 0.53
Batch: 240; loss: 2.87; acc: 0.53
Batch: 260; loss: 2.01; acc: 0.61
Batch: 280; loss: 1.8; acc: 0.66
Batch: 300; loss: 2.14; acc: 0.53
Batch: 320; loss: 2.33; acc: 0.56
Batch: 340; loss: 2.42; acc: 0.53
Batch: 360; loss: 2.11; acc: 0.53
Batch: 380; loss: 2.37; acc: 0.56
Batch: 400; loss: 1.54; acc: 0.66
Batch: 420; loss: 2.06; acc: 0.5
Batch: 440; loss: 2.01; acc: 0.55
Batch: 460; loss: 1.94; acc: 0.59
Batch: 480; loss: 3.06; acc: 0.48
Batch: 500; loss: 2.47; acc: 0.55
Batch: 520; loss: 3.29; acc: 0.39
Batch: 540; loss: 2.04; acc: 0.59
Batch: 560; loss: 3.01; acc: 0.47
Batch: 580; loss: 1.65; acc: 0.56
Batch: 600; loss: 2.42; acc: 0.42
Batch: 620; loss: 1.91; acc: 0.64
Train Epoch over. train_loss: 2.2; train_accuracy: 0.55 

Batch: 0; loss: 2.53; acc: 0.5
Batch: 20; loss: 2.31; acc: 0.47
Batch: 40; loss: 1.65; acc: 0.56
Batch: 60; loss: 1.54; acc: 0.61
Batch: 80; loss: 1.93; acc: 0.58
Batch: 100; loss: 1.82; acc: 0.59
Batch: 120; loss: 1.09; acc: 0.69
Batch: 140; loss: 3.66; acc: 0.34
Val Epoch over. val_loss: 2.0928171516224077; val_accuracy: 0.5509554140127388 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 1.9; acc: 0.52
Batch: 20; loss: 2.14; acc: 0.55
Batch: 40; loss: 1.98; acc: 0.58
Batch: 60; loss: 1.41; acc: 0.59
Batch: 80; loss: 2.73; acc: 0.48
Batch: 100; loss: 2.48; acc: 0.53
Batch: 120; loss: 1.62; acc: 0.69
Batch: 140; loss: 2.37; acc: 0.55
Batch: 160; loss: 2.44; acc: 0.56
Batch: 180; loss: 2.18; acc: 0.48
Batch: 200; loss: 1.84; acc: 0.53
Batch: 220; loss: 2.68; acc: 0.53
Batch: 240; loss: 3.12; acc: 0.5
Batch: 260; loss: 2.01; acc: 0.62
Batch: 280; loss: 2.52; acc: 0.55
Batch: 300; loss: 1.45; acc: 0.59
Batch: 320; loss: 2.91; acc: 0.53
Batch: 340; loss: 3.04; acc: 0.47
Batch: 360; loss: 2.97; acc: 0.42
Batch: 380; loss: 1.13; acc: 0.77
Batch: 400; loss: 1.43; acc: 0.7
Batch: 420; loss: 2.32; acc: 0.61
Batch: 440; loss: 1.8; acc: 0.53
Batch: 460; loss: 2.16; acc: 0.52
Batch: 480; loss: 1.31; acc: 0.62
Batch: 500; loss: 3.15; acc: 0.5
Batch: 520; loss: 1.97; acc: 0.62
Batch: 540; loss: 2.97; acc: 0.39
Batch: 560; loss: 1.76; acc: 0.59
Batch: 580; loss: 2.02; acc: 0.58
Batch: 600; loss: 3.0; acc: 0.5
Batch: 620; loss: 2.02; acc: 0.56
Train Epoch over. train_loss: 2.18; train_accuracy: 0.55 

Batch: 0; loss: 3.46; acc: 0.45
Batch: 20; loss: 2.63; acc: 0.44
Batch: 40; loss: 2.33; acc: 0.52
Batch: 60; loss: 2.4; acc: 0.53
Batch: 80; loss: 2.85; acc: 0.47
Batch: 100; loss: 2.37; acc: 0.45
Batch: 120; loss: 1.78; acc: 0.56
Batch: 140; loss: 4.5; acc: 0.38
Val Epoch over. val_loss: 2.8580395089592905; val_accuracy: 0.4836783439490446 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 2.5; acc: 0.52
Batch: 20; loss: 2.32; acc: 0.47
Batch: 40; loss: 2.44; acc: 0.42
Batch: 60; loss: 2.29; acc: 0.48
Batch: 80; loss: 3.01; acc: 0.58
Batch: 100; loss: 2.13; acc: 0.62
Batch: 120; loss: 2.29; acc: 0.58
Batch: 140; loss: 1.77; acc: 0.61
Batch: 160; loss: 2.23; acc: 0.45
Batch: 180; loss: 2.79; acc: 0.53
Batch: 200; loss: 2.86; acc: 0.44
Batch: 220; loss: 1.57; acc: 0.67
Batch: 240; loss: 2.72; acc: 0.42
Batch: 260; loss: 2.37; acc: 0.48
Batch: 280; loss: 2.24; acc: 0.53
Batch: 300; loss: 1.5; acc: 0.59
Batch: 320; loss: 1.92; acc: 0.61
Batch: 340; loss: 1.79; acc: 0.61
Batch: 360; loss: 1.75; acc: 0.56
Batch: 380; loss: 1.63; acc: 0.67
Batch: 400; loss: 1.5; acc: 0.67
Batch: 420; loss: 2.69; acc: 0.55
Batch: 440; loss: 2.02; acc: 0.64
Batch: 460; loss: 1.15; acc: 0.67
Batch: 480; loss: 2.34; acc: 0.5
Batch: 500; loss: 1.8; acc: 0.61
Batch: 520; loss: 2.32; acc: 0.5
Batch: 540; loss: 2.25; acc: 0.45
Batch: 560; loss: 1.62; acc: 0.59
Batch: 580; loss: 2.57; acc: 0.5
Batch: 600; loss: 2.04; acc: 0.58
Batch: 620; loss: 2.11; acc: 0.58
Train Epoch over. train_loss: 2.21; train_accuracy: 0.56 

Batch: 0; loss: 2.8; acc: 0.48
Batch: 20; loss: 2.29; acc: 0.52
Batch: 40; loss: 1.75; acc: 0.56
Batch: 60; loss: 1.82; acc: 0.58
Batch: 80; loss: 1.96; acc: 0.61
Batch: 100; loss: 1.94; acc: 0.62
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 3.45; acc: 0.33
Val Epoch over. val_loss: 2.1757229593149416; val_accuracy: 0.5535429936305732 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 1.77; acc: 0.62
Batch: 20; loss: 2.91; acc: 0.5
Batch: 40; loss: 2.7; acc: 0.47
Batch: 60; loss: 2.11; acc: 0.5
Batch: 80; loss: 2.28; acc: 0.59
Batch: 100; loss: 1.71; acc: 0.59
Batch: 120; loss: 2.21; acc: 0.59
Batch: 140; loss: 2.19; acc: 0.56
Batch: 160; loss: 2.12; acc: 0.64
Batch: 180; loss: 1.63; acc: 0.56
Batch: 200; loss: 3.54; acc: 0.5
Batch: 220; loss: 1.94; acc: 0.62
Batch: 240; loss: 2.36; acc: 0.52
Batch: 260; loss: 1.77; acc: 0.58
Batch: 280; loss: 1.7; acc: 0.66
Batch: 300; loss: 2.05; acc: 0.61
Batch: 320; loss: 3.41; acc: 0.5
Batch: 340; loss: 2.3; acc: 0.52
Batch: 360; loss: 2.25; acc: 0.55
Batch: 380; loss: 2.18; acc: 0.62
Batch: 400; loss: 2.32; acc: 0.53
Batch: 420; loss: 2.43; acc: 0.59
Batch: 440; loss: 2.08; acc: 0.5
Batch: 460; loss: 2.4; acc: 0.52
Batch: 480; loss: 2.18; acc: 0.64
Batch: 500; loss: 1.67; acc: 0.58
Batch: 520; loss: 2.92; acc: 0.48
Batch: 540; loss: 1.68; acc: 0.56
Batch: 560; loss: 2.61; acc: 0.47
Batch: 580; loss: 1.73; acc: 0.55
Batch: 600; loss: 1.73; acc: 0.64
Batch: 620; loss: 2.09; acc: 0.58
Train Epoch over. train_loss: 2.22; train_accuracy: 0.55 

Batch: 0; loss: 2.64; acc: 0.47
Batch: 20; loss: 2.78; acc: 0.42
Batch: 40; loss: 1.77; acc: 0.56
Batch: 60; loss: 1.59; acc: 0.64
Batch: 80; loss: 2.04; acc: 0.52
Batch: 100; loss: 1.92; acc: 0.55
Batch: 120; loss: 1.04; acc: 0.7
Batch: 140; loss: 3.5; acc: 0.36
Val Epoch over. val_loss: 2.2081433211921886; val_accuracy: 0.5573248407643312 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 2.46; acc: 0.5
Batch: 20; loss: 1.96; acc: 0.56
Batch: 40; loss: 1.95; acc: 0.55
Batch: 60; loss: 2.12; acc: 0.59
Batch: 80; loss: 2.41; acc: 0.58
Batch: 100; loss: 1.77; acc: 0.59
Batch: 120; loss: 3.41; acc: 0.48
Batch: 140; loss: 1.6; acc: 0.62
Batch: 160; loss: 1.58; acc: 0.66
Batch: 180; loss: 1.83; acc: 0.58
Batch: 200; loss: 2.3; acc: 0.58
Batch: 220; loss: 1.98; acc: 0.58
Batch: 240; loss: 2.81; acc: 0.47
Batch: 260; loss: 1.42; acc: 0.62
Batch: 280; loss: 1.97; acc: 0.58
Batch: 300; loss: 1.66; acc: 0.62
Batch: 320; loss: 1.94; acc: 0.61
Batch: 340; loss: 1.67; acc: 0.61
Batch: 360; loss: 2.28; acc: 0.58
Batch: 380; loss: 2.52; acc: 0.53
Batch: 400; loss: 2.08; acc: 0.5
Batch: 420; loss: 2.18; acc: 0.52
Batch: 440; loss: 3.21; acc: 0.53
Batch: 460; loss: 3.18; acc: 0.48
Batch: 480; loss: 1.82; acc: 0.66
Batch: 500; loss: 2.27; acc: 0.48
Batch: 520; loss: 2.2; acc: 0.52
Batch: 540; loss: 3.46; acc: 0.48
Batch: 560; loss: 1.81; acc: 0.62
Batch: 580; loss: 2.02; acc: 0.55
Batch: 600; loss: 2.88; acc: 0.52
Batch: 620; loss: 2.61; acc: 0.56
Train Epoch over. train_loss: 2.2; train_accuracy: 0.56 

Batch: 0; loss: 2.65; acc: 0.44
Batch: 20; loss: 2.6; acc: 0.45
Batch: 40; loss: 1.84; acc: 0.5
Batch: 60; loss: 1.47; acc: 0.62
Batch: 80; loss: 1.91; acc: 0.62
Batch: 100; loss: 2.06; acc: 0.64
Batch: 120; loss: 1.29; acc: 0.7
Batch: 140; loss: 3.47; acc: 0.39
Val Epoch over. val_loss: 2.1575654858996156; val_accuracy: 0.5547372611464968 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.8; acc: 0.62
Batch: 20; loss: 2.36; acc: 0.55
Batch: 40; loss: 2.18; acc: 0.61
Batch: 60; loss: 1.51; acc: 0.58
Batch: 80; loss: 2.24; acc: 0.58
Batch: 100; loss: 2.16; acc: 0.53
Batch: 120; loss: 1.88; acc: 0.62
Batch: 140; loss: 2.74; acc: 0.5
Batch: 160; loss: 2.39; acc: 0.52
Batch: 180; loss: 3.03; acc: 0.47
Batch: 200; loss: 2.57; acc: 0.53
Batch: 220; loss: 2.54; acc: 0.48
Batch: 240; loss: 2.78; acc: 0.62
Batch: 260; loss: 2.16; acc: 0.55
Batch: 280; loss: 2.11; acc: 0.59
Batch: 300; loss: 1.92; acc: 0.61
Batch: 320; loss: 1.84; acc: 0.59
Batch: 340; loss: 2.64; acc: 0.5
Batch: 360; loss: 3.06; acc: 0.55
Batch: 380; loss: 2.13; acc: 0.56
Batch: 400; loss: 2.2; acc: 0.55
Batch: 420; loss: 1.6; acc: 0.61
Batch: 440; loss: 2.13; acc: 0.66
Batch: 460; loss: 1.83; acc: 0.59
Batch: 480; loss: 1.52; acc: 0.67
Batch: 500; loss: 1.49; acc: 0.61
Batch: 520; loss: 2.8; acc: 0.45
Batch: 540; loss: 2.1; acc: 0.59
Batch: 560; loss: 3.19; acc: 0.55
Batch: 580; loss: 2.04; acc: 0.56
Batch: 600; loss: 2.34; acc: 0.47
Batch: 620; loss: 3.16; acc: 0.5
Train Epoch over. train_loss: 2.21; train_accuracy: 0.55 

Batch: 0; loss: 2.5; acc: 0.5
Batch: 20; loss: 2.2; acc: 0.55
Batch: 40; loss: 1.93; acc: 0.53
Batch: 60; loss: 1.43; acc: 0.7
Batch: 80; loss: 1.97; acc: 0.52
Batch: 100; loss: 1.95; acc: 0.56
Batch: 120; loss: 1.26; acc: 0.69
Batch: 140; loss: 3.44; acc: 0.42
Val Epoch over. val_loss: 2.127260624982749; val_accuracy: 0.5438893312101911 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 2.22; acc: 0.52
Batch: 20; loss: 2.41; acc: 0.5
Batch: 40; loss: 2.43; acc: 0.52
Batch: 60; loss: 2.49; acc: 0.52
Batch: 80; loss: 1.92; acc: 0.55
Batch: 100; loss: 1.91; acc: 0.61
Batch: 120; loss: 1.96; acc: 0.58
Batch: 140; loss: 2.75; acc: 0.52
Batch: 160; loss: 2.4; acc: 0.52
Batch: 180; loss: 2.81; acc: 0.48
Batch: 200; loss: 2.35; acc: 0.5
Batch: 220; loss: 2.14; acc: 0.58
Batch: 240; loss: 2.88; acc: 0.41
Batch: 260; loss: 2.06; acc: 0.62
Batch: 280; loss: 2.17; acc: 0.47
Batch: 300; loss: 2.78; acc: 0.56
Batch: 320; loss: 2.01; acc: 0.59
Batch: 340; loss: 2.43; acc: 0.55
Batch: 360; loss: 1.6; acc: 0.61
Batch: 380; loss: 1.72; acc: 0.66
Batch: 400; loss: 1.41; acc: 0.64
Batch: 420; loss: 1.68; acc: 0.62
Batch: 440; loss: 2.35; acc: 0.67
Batch: 460; loss: 2.39; acc: 0.56
Batch: 480; loss: 2.15; acc: 0.58
Batch: 500; loss: 2.22; acc: 0.52
Batch: 520; loss: 2.11; acc: 0.56
Batch: 540; loss: 2.24; acc: 0.53
Batch: 560; loss: 3.28; acc: 0.39
Batch: 580; loss: 2.22; acc: 0.61
Batch: 600; loss: 2.25; acc: 0.55
Batch: 620; loss: 2.11; acc: 0.52
Train Epoch over. train_loss: 2.2; train_accuracy: 0.55 

Batch: 0; loss: 2.86; acc: 0.45
Batch: 20; loss: 2.82; acc: 0.45
Batch: 40; loss: 1.7; acc: 0.55
Batch: 60; loss: 1.9; acc: 0.55
Batch: 80; loss: 2.23; acc: 0.55
Batch: 100; loss: 2.11; acc: 0.55
Batch: 120; loss: 1.02; acc: 0.7
Batch: 140; loss: 3.88; acc: 0.38
Val Epoch over. val_loss: 2.3482091388884623; val_accuracy: 0.5214968152866242 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.77; acc: 0.45
Batch: 20; loss: 2.56; acc: 0.47
Batch: 40; loss: 2.01; acc: 0.59
Batch: 60; loss: 1.48; acc: 0.66
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.92; acc: 0.64
Batch: 120; loss: 2.81; acc: 0.47
Batch: 140; loss: 1.84; acc: 0.66
Batch: 160; loss: 2.17; acc: 0.58
Batch: 180; loss: 1.57; acc: 0.58
Batch: 200; loss: 1.57; acc: 0.72
Batch: 220; loss: 2.94; acc: 0.52
Batch: 240; loss: 1.5; acc: 0.72
Batch: 260; loss: 2.06; acc: 0.56
Batch: 280; loss: 1.8; acc: 0.59
Batch: 300; loss: 1.48; acc: 0.67
Batch: 320; loss: 1.69; acc: 0.58
Batch: 340; loss: 2.29; acc: 0.55
Batch: 360; loss: 1.48; acc: 0.72
Batch: 380; loss: 1.67; acc: 0.53
Batch: 400; loss: 1.85; acc: 0.58
Batch: 420; loss: 1.83; acc: 0.58
Batch: 440; loss: 1.02; acc: 0.69
Batch: 460; loss: 1.94; acc: 0.61
Batch: 480; loss: 2.1; acc: 0.52
Batch: 500; loss: 2.25; acc: 0.52
Batch: 520; loss: 1.92; acc: 0.55
Batch: 540; loss: 2.53; acc: 0.53
Batch: 560; loss: 1.57; acc: 0.55
Batch: 580; loss: 1.6; acc: 0.67
Batch: 600; loss: 2.11; acc: 0.56
Batch: 620; loss: 1.46; acc: 0.66
Train Epoch over. train_loss: 1.89; train_accuracy: 0.59 

Batch: 0; loss: 2.22; acc: 0.52
Batch: 20; loss: 2.31; acc: 0.5
Batch: 40; loss: 1.65; acc: 0.58
Batch: 60; loss: 1.32; acc: 0.7
Batch: 80; loss: 1.63; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.62
Batch: 120; loss: 0.92; acc: 0.8
Batch: 140; loss: 3.31; acc: 0.41
Val Epoch over. val_loss: 1.921867277591851; val_accuracy: 0.5814092356687898 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.84; acc: 0.47
Batch: 20; loss: 2.41; acc: 0.56
Batch: 40; loss: 1.69; acc: 0.61
Batch: 60; loss: 2.66; acc: 0.44
Batch: 80; loss: 1.62; acc: 0.58
Batch: 100; loss: 1.86; acc: 0.62
Batch: 120; loss: 2.34; acc: 0.53
Batch: 140; loss: 1.92; acc: 0.53
Batch: 160; loss: 1.65; acc: 0.56
Batch: 180; loss: 1.98; acc: 0.55
Batch: 200; loss: 1.52; acc: 0.73
Batch: 220; loss: 1.55; acc: 0.69
Batch: 240; loss: 1.58; acc: 0.64
Batch: 260; loss: 1.98; acc: 0.52
Batch: 280; loss: 2.44; acc: 0.5
Batch: 300; loss: 2.69; acc: 0.5
Batch: 320; loss: 1.87; acc: 0.62
Batch: 340; loss: 1.91; acc: 0.61
Batch: 360; loss: 2.12; acc: 0.66
Batch: 380; loss: 2.54; acc: 0.41
Batch: 400; loss: 1.53; acc: 0.67
Batch: 420; loss: 1.88; acc: 0.56
Batch: 440; loss: 1.01; acc: 0.72
Batch: 460; loss: 1.16; acc: 0.7
Batch: 480; loss: 2.01; acc: 0.64
Batch: 500; loss: 1.68; acc: 0.58
Batch: 520; loss: 1.5; acc: 0.72
Batch: 540; loss: 2.06; acc: 0.56
Batch: 560; loss: 1.99; acc: 0.62
Batch: 580; loss: 2.07; acc: 0.5
Batch: 600; loss: 1.82; acc: 0.62
Batch: 620; loss: 1.71; acc: 0.55
Train Epoch over. train_loss: 1.87; train_accuracy: 0.59 

Batch: 0; loss: 2.27; acc: 0.5
Batch: 20; loss: 2.32; acc: 0.52
Batch: 40; loss: 1.78; acc: 0.56
Batch: 60; loss: 1.31; acc: 0.73
Batch: 80; loss: 1.66; acc: 0.56
Batch: 100; loss: 1.87; acc: 0.59
Batch: 120; loss: 0.91; acc: 0.81
Batch: 140; loss: 3.37; acc: 0.39
Val Epoch over. val_loss: 1.9503910697189866; val_accuracy: 0.5742436305732485 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.07; acc: 0.55
Batch: 20; loss: 1.91; acc: 0.62
Batch: 40; loss: 1.87; acc: 0.56
Batch: 60; loss: 1.97; acc: 0.55
Batch: 80; loss: 2.37; acc: 0.62
Batch: 100; loss: 1.54; acc: 0.64
Batch: 120; loss: 2.25; acc: 0.59
Batch: 140; loss: 2.12; acc: 0.55
Batch: 160; loss: 1.15; acc: 0.69
Batch: 180; loss: 1.5; acc: 0.56
Batch: 200; loss: 2.21; acc: 0.64
Batch: 220; loss: 1.88; acc: 0.59
Batch: 240; loss: 1.82; acc: 0.56
Batch: 260; loss: 1.82; acc: 0.59
Batch: 280; loss: 1.58; acc: 0.67
Batch: 300; loss: 2.01; acc: 0.58
Batch: 320; loss: 1.92; acc: 0.55
Batch: 340; loss: 1.98; acc: 0.61
Batch: 360; loss: 1.51; acc: 0.64
Batch: 380; loss: 1.93; acc: 0.58
Batch: 400; loss: 1.05; acc: 0.67
Batch: 420; loss: 1.88; acc: 0.61
Batch: 440; loss: 1.55; acc: 0.61
Batch: 460; loss: 2.07; acc: 0.62
Batch: 480; loss: 1.12; acc: 0.7
Batch: 500; loss: 1.92; acc: 0.59
Batch: 520; loss: 1.69; acc: 0.61
Batch: 540; loss: 1.65; acc: 0.64
Batch: 560; loss: 2.83; acc: 0.53
Batch: 580; loss: 1.66; acc: 0.69
Batch: 600; loss: 1.26; acc: 0.67
Batch: 620; loss: 1.71; acc: 0.62
Train Epoch over. train_loss: 1.87; train_accuracy: 0.59 

Batch: 0; loss: 2.39; acc: 0.5
Batch: 20; loss: 2.26; acc: 0.52
Batch: 40; loss: 1.68; acc: 0.56
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.61; acc: 0.62
Batch: 100; loss: 1.77; acc: 0.61
Batch: 120; loss: 0.97; acc: 0.78
Batch: 140; loss: 3.24; acc: 0.42
Val Epoch over. val_loss: 1.9659183894752696; val_accuracy: 0.5799164012738853 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.12; acc: 0.56
Batch: 20; loss: 2.04; acc: 0.61
Batch: 40; loss: 2.25; acc: 0.5
Batch: 60; loss: 1.95; acc: 0.66
Batch: 80; loss: 2.38; acc: 0.53
Batch: 100; loss: 2.33; acc: 0.53
Batch: 120; loss: 2.15; acc: 0.53
Batch: 140; loss: 0.94; acc: 0.78
Batch: 160; loss: 2.35; acc: 0.55
Batch: 180; loss: 1.44; acc: 0.56
Batch: 200; loss: 1.52; acc: 0.64
Batch: 220; loss: 2.65; acc: 0.52
Batch: 240; loss: 1.98; acc: 0.67
Batch: 260; loss: 1.27; acc: 0.64
Batch: 280; loss: 1.84; acc: 0.52
Batch: 300; loss: 2.12; acc: 0.53
Batch: 320; loss: 2.08; acc: 0.62
Batch: 340; loss: 2.23; acc: 0.61
Batch: 360; loss: 1.56; acc: 0.62
Batch: 380; loss: 1.55; acc: 0.69
Batch: 400; loss: 2.16; acc: 0.62
Batch: 420; loss: 1.96; acc: 0.52
Batch: 440; loss: 1.67; acc: 0.69
Batch: 460; loss: 1.69; acc: 0.61
Batch: 480; loss: 2.1; acc: 0.53
Batch: 500; loss: 2.55; acc: 0.59
Batch: 520; loss: 1.88; acc: 0.67
Batch: 540; loss: 1.72; acc: 0.64
Batch: 560; loss: 2.14; acc: 0.56
Batch: 580; loss: 1.68; acc: 0.64
Batch: 600; loss: 2.07; acc: 0.61
Batch: 620; loss: 1.86; acc: 0.52
Train Epoch over. train_loss: 1.87; train_accuracy: 0.59 

Batch: 0; loss: 2.37; acc: 0.5
Batch: 20; loss: 2.34; acc: 0.47
Batch: 40; loss: 1.73; acc: 0.59
Batch: 60; loss: 1.34; acc: 0.72
Batch: 80; loss: 1.67; acc: 0.59
Batch: 100; loss: 1.86; acc: 0.62
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 3.31; acc: 0.38
Val Epoch over. val_loss: 1.9571271648832187; val_accuracy: 0.5784235668789809 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.71; acc: 0.5
Batch: 20; loss: 2.46; acc: 0.5
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.79; acc: 0.59
Batch: 80; loss: 2.21; acc: 0.53
Batch: 100; loss: 2.28; acc: 0.61
Batch: 120; loss: 2.52; acc: 0.5
Batch: 140; loss: 1.81; acc: 0.62
Batch: 160; loss: 1.77; acc: 0.59
Batch: 180; loss: 2.05; acc: 0.61
Batch: 200; loss: 1.95; acc: 0.61
Batch: 220; loss: 1.82; acc: 0.61
Batch: 240; loss: 2.32; acc: 0.5
Batch: 260; loss: 3.05; acc: 0.55
Batch: 280; loss: 1.4; acc: 0.67
Batch: 300; loss: 1.97; acc: 0.58
Batch: 320; loss: 1.76; acc: 0.58
Batch: 340; loss: 1.74; acc: 0.66
Batch: 360; loss: 2.09; acc: 0.52
Batch: 380; loss: 1.79; acc: 0.66
Batch: 400; loss: 2.33; acc: 0.48
Batch: 420; loss: 1.09; acc: 0.73
Batch: 440; loss: 2.19; acc: 0.58
Batch: 460; loss: 1.54; acc: 0.66
Batch: 480; loss: 1.77; acc: 0.56
Batch: 500; loss: 2.5; acc: 0.48
Batch: 520; loss: 2.34; acc: 0.53
Batch: 540; loss: 2.38; acc: 0.59
Batch: 560; loss: 2.36; acc: 0.53
Batch: 580; loss: 2.13; acc: 0.55
Batch: 600; loss: 1.48; acc: 0.64
Batch: 620; loss: 2.61; acc: 0.5
Train Epoch over. train_loss: 1.87; train_accuracy: 0.6 

Batch: 0; loss: 2.22; acc: 0.52
Batch: 20; loss: 2.29; acc: 0.53
Batch: 40; loss: 1.77; acc: 0.56
Batch: 60; loss: 1.36; acc: 0.66
Batch: 80; loss: 1.64; acc: 0.59
Batch: 100; loss: 1.83; acc: 0.64
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 3.26; acc: 0.41
Val Epoch over. val_loss: 1.9217650324675688; val_accuracy: 0.5830015923566879 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.22; acc: 0.58
Batch: 20; loss: 1.56; acc: 0.64
Batch: 40; loss: 1.44; acc: 0.64
Batch: 60; loss: 1.89; acc: 0.58
Batch: 80; loss: 1.97; acc: 0.56
Batch: 100; loss: 1.8; acc: 0.64
Batch: 120; loss: 1.59; acc: 0.56
Batch: 140; loss: 2.3; acc: 0.56
Batch: 160; loss: 1.82; acc: 0.53
Batch: 180; loss: 2.16; acc: 0.61
Batch: 200; loss: 2.39; acc: 0.59
Batch: 220; loss: 1.57; acc: 0.61
Batch: 240; loss: 1.54; acc: 0.62
Batch: 260; loss: 1.61; acc: 0.66
Batch: 280; loss: 1.5; acc: 0.59
Batch: 300; loss: 2.23; acc: 0.53
Batch: 320; loss: 1.58; acc: 0.67
Batch: 340; loss: 1.93; acc: 0.56
Batch: 360; loss: 1.78; acc: 0.61
Batch: 380; loss: 2.41; acc: 0.56
Batch: 400; loss: 2.37; acc: 0.53
Batch: 420; loss: 2.06; acc: 0.66
Batch: 440; loss: 1.31; acc: 0.66
Batch: 460; loss: 1.3; acc: 0.69
Batch: 480; loss: 1.57; acc: 0.61
Batch: 500; loss: 2.12; acc: 0.59
Batch: 520; loss: 1.5; acc: 0.7
Batch: 540; loss: 1.37; acc: 0.69
Batch: 560; loss: 1.95; acc: 0.53
Batch: 580; loss: 2.22; acc: 0.53
Batch: 600; loss: 1.88; acc: 0.62
Batch: 620; loss: 1.53; acc: 0.64
Train Epoch over. train_loss: 1.87; train_accuracy: 0.6 

Batch: 0; loss: 2.34; acc: 0.52
Batch: 20; loss: 2.48; acc: 0.47
Batch: 40; loss: 1.71; acc: 0.62
Batch: 60; loss: 1.41; acc: 0.62
Batch: 80; loss: 1.69; acc: 0.56
Batch: 100; loss: 1.86; acc: 0.62
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 3.35; acc: 0.38
Val Epoch over. val_loss: 1.9552866964583184; val_accuracy: 0.5822054140127388 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.12; acc: 0.52
Batch: 20; loss: 1.57; acc: 0.64
Batch: 40; loss: 1.49; acc: 0.64
Batch: 60; loss: 1.98; acc: 0.55
Batch: 80; loss: 1.44; acc: 0.66
Batch: 100; loss: 1.81; acc: 0.53
Batch: 120; loss: 2.26; acc: 0.58
Batch: 140; loss: 1.91; acc: 0.61
Batch: 160; loss: 1.92; acc: 0.56
Batch: 180; loss: 1.72; acc: 0.55
Batch: 200; loss: 1.83; acc: 0.62
Batch: 220; loss: 2.01; acc: 0.64
Batch: 240; loss: 1.62; acc: 0.67
Batch: 260; loss: 1.71; acc: 0.58
Batch: 280; loss: 2.43; acc: 0.52
Batch: 300; loss: 1.66; acc: 0.59
Batch: 320; loss: 1.5; acc: 0.69
Batch: 340; loss: 2.43; acc: 0.61
Batch: 360; loss: 1.8; acc: 0.55
Batch: 380; loss: 1.48; acc: 0.59
Batch: 400; loss: 1.41; acc: 0.7
Batch: 420; loss: 1.66; acc: 0.59
Batch: 440; loss: 1.27; acc: 0.59
Batch: 460; loss: 2.43; acc: 0.5
Batch: 480; loss: 1.99; acc: 0.52
Batch: 500; loss: 1.34; acc: 0.66
Batch: 520; loss: 2.24; acc: 0.58
Batch: 540; loss: 2.17; acc: 0.53
Batch: 560; loss: 1.54; acc: 0.59
Batch: 580; loss: 1.58; acc: 0.59
Batch: 600; loss: 1.25; acc: 0.66
Batch: 620; loss: 2.03; acc: 0.61
Train Epoch over. train_loss: 1.87; train_accuracy: 0.6 

Batch: 0; loss: 2.19; acc: 0.5
Batch: 20; loss: 2.39; acc: 0.5
Batch: 40; loss: 1.7; acc: 0.58
Batch: 60; loss: 1.37; acc: 0.67
Batch: 80; loss: 1.61; acc: 0.62
Batch: 100; loss: 1.86; acc: 0.64
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 3.27; acc: 0.41
Val Epoch over. val_loss: 1.9261950713813685; val_accuracy: 0.5862858280254777 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.06; acc: 0.5
Batch: 20; loss: 1.86; acc: 0.58
Batch: 40; loss: 1.31; acc: 0.7
Batch: 60; loss: 1.84; acc: 0.61
Batch: 80; loss: 1.43; acc: 0.69
Batch: 100; loss: 1.21; acc: 0.67
Batch: 120; loss: 1.96; acc: 0.53
Batch: 140; loss: 1.87; acc: 0.56
Batch: 160; loss: 1.33; acc: 0.67
Batch: 180; loss: 2.51; acc: 0.59
Batch: 200; loss: 2.33; acc: 0.52
Batch: 220; loss: 2.14; acc: 0.58
Batch: 240; loss: 2.33; acc: 0.59
Batch: 260; loss: 2.33; acc: 0.64
Batch: 280; loss: 2.01; acc: 0.56
Batch: 300; loss: 1.97; acc: 0.62
Batch: 320; loss: 2.26; acc: 0.58
Batch: 340; loss: 2.43; acc: 0.44
Batch: 360; loss: 1.54; acc: 0.61
Batch: 380; loss: 2.15; acc: 0.47
Batch: 400; loss: 1.3; acc: 0.72
Batch: 420; loss: 1.9; acc: 0.61
Batch: 440; loss: 1.22; acc: 0.62
Batch: 460; loss: 2.02; acc: 0.61
Batch: 480; loss: 1.49; acc: 0.64
Batch: 500; loss: 1.33; acc: 0.67
Batch: 520; loss: 1.86; acc: 0.59
Batch: 540; loss: 1.69; acc: 0.66
Batch: 560; loss: 1.64; acc: 0.59
Batch: 580; loss: 2.7; acc: 0.41
Batch: 600; loss: 1.18; acc: 0.69
Batch: 620; loss: 1.7; acc: 0.64
Train Epoch over. train_loss: 1.87; train_accuracy: 0.6 

Batch: 0; loss: 2.21; acc: 0.52
Batch: 20; loss: 2.42; acc: 0.47
Batch: 40; loss: 1.84; acc: 0.61
Batch: 60; loss: 1.42; acc: 0.69
Batch: 80; loss: 1.71; acc: 0.59
Batch: 100; loss: 1.78; acc: 0.64
Batch: 120; loss: 0.92; acc: 0.75
Batch: 140; loss: 3.29; acc: 0.39
Val Epoch over. val_loss: 1.960494487148941; val_accuracy: 0.5782245222929936 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.68; acc: 0.69
Batch: 20; loss: 1.46; acc: 0.64
Batch: 40; loss: 1.57; acc: 0.58
Batch: 60; loss: 1.58; acc: 0.58
Batch: 80; loss: 2.04; acc: 0.61
Batch: 100; loss: 1.88; acc: 0.56
Batch: 120; loss: 2.36; acc: 0.55
Batch: 140; loss: 2.48; acc: 0.5
Batch: 160; loss: 1.72; acc: 0.64
Batch: 180; loss: 1.96; acc: 0.59
Batch: 200; loss: 1.96; acc: 0.58
Batch: 220; loss: 1.5; acc: 0.67
Batch: 240; loss: 1.68; acc: 0.62
Batch: 260; loss: 1.96; acc: 0.59
Batch: 280; loss: 1.88; acc: 0.56
Batch: 300; loss: 1.62; acc: 0.55
Batch: 320; loss: 1.98; acc: 0.53
Batch: 340; loss: 1.77; acc: 0.61
Batch: 360; loss: 1.96; acc: 0.52
Batch: 380; loss: 1.33; acc: 0.62
Batch: 400; loss: 2.28; acc: 0.58
Batch: 420; loss: 1.87; acc: 0.62
Batch: 440; loss: 1.55; acc: 0.64
Batch: 460; loss: 2.2; acc: 0.66
Batch: 480; loss: 1.76; acc: 0.59
Batch: 500; loss: 1.49; acc: 0.62
Batch: 520; loss: 2.1; acc: 0.56
Batch: 540; loss: 2.61; acc: 0.44
Batch: 560; loss: 2.1; acc: 0.62
Batch: 580; loss: 1.79; acc: 0.56
Batch: 600; loss: 1.34; acc: 0.62
Batch: 620; loss: 2.3; acc: 0.58
Train Epoch over. train_loss: 1.87; train_accuracy: 0.6 

Batch: 0; loss: 2.23; acc: 0.55
Batch: 20; loss: 2.42; acc: 0.47
Batch: 40; loss: 1.82; acc: 0.59
Batch: 60; loss: 1.46; acc: 0.64
Batch: 80; loss: 1.69; acc: 0.56
Batch: 100; loss: 1.83; acc: 0.59
Batch: 120; loss: 0.99; acc: 0.69
Batch: 140; loss: 3.23; acc: 0.39
Val Epoch over. val_loss: 1.964112567673823; val_accuracy: 0.5801154458598726 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.85; acc: 0.59
Batch: 20; loss: 2.17; acc: 0.59
Batch: 40; loss: 2.13; acc: 0.5
Batch: 60; loss: 1.59; acc: 0.66
Batch: 80; loss: 1.9; acc: 0.58
Batch: 100; loss: 2.16; acc: 0.53
Batch: 120; loss: 1.75; acc: 0.61
Batch: 140; loss: 1.89; acc: 0.64
Batch: 160; loss: 2.44; acc: 0.5
Batch: 180; loss: 1.9; acc: 0.59
Batch: 200; loss: 1.94; acc: 0.64
Batch: 220; loss: 2.3; acc: 0.59
Batch: 240; loss: 2.05; acc: 0.61
Batch: 260; loss: 2.33; acc: 0.52
Batch: 280; loss: 1.54; acc: 0.59
Batch: 300; loss: 2.27; acc: 0.55
Batch: 320; loss: 2.16; acc: 0.53
Batch: 340; loss: 1.39; acc: 0.66
Batch: 360; loss: 1.34; acc: 0.73
Batch: 380; loss: 2.17; acc: 0.55
Batch: 400; loss: 2.13; acc: 0.56
Batch: 420; loss: 1.18; acc: 0.73
Batch: 440; loss: 1.92; acc: 0.48
Batch: 460; loss: 1.73; acc: 0.55
Batch: 480; loss: 2.12; acc: 0.55
Batch: 500; loss: 2.18; acc: 0.53
Batch: 520; loss: 1.48; acc: 0.7
Batch: 540; loss: 2.98; acc: 0.55
Batch: 560; loss: 1.93; acc: 0.61
Batch: 580; loss: 1.58; acc: 0.66
Batch: 600; loss: 1.56; acc: 0.55
Batch: 620; loss: 1.64; acc: 0.62
Train Epoch over. train_loss: 1.87; train_accuracy: 0.6 

Batch: 0; loss: 2.22; acc: 0.52
Batch: 20; loss: 2.39; acc: 0.5
Batch: 40; loss: 1.86; acc: 0.61
Batch: 60; loss: 1.46; acc: 0.66
Batch: 80; loss: 1.69; acc: 0.58
Batch: 100; loss: 1.81; acc: 0.62
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 3.18; acc: 0.42
Val Epoch over. val_loss: 1.9576144366507318; val_accuracy: 0.5805135350318471 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.43; acc: 0.61
Batch: 20; loss: 2.04; acc: 0.59
Batch: 40; loss: 1.79; acc: 0.55
Batch: 60; loss: 2.92; acc: 0.58
Batch: 80; loss: 2.0; acc: 0.56
Batch: 100; loss: 2.27; acc: 0.55
Batch: 120; loss: 2.07; acc: 0.5
Batch: 140; loss: 1.36; acc: 0.67
Batch: 160; loss: 1.44; acc: 0.53
Batch: 180; loss: 1.25; acc: 0.72
Batch: 200; loss: 1.95; acc: 0.64
Batch: 220; loss: 1.55; acc: 0.59
Batch: 240; loss: 2.26; acc: 0.5
Batch: 260; loss: 1.76; acc: 0.67
Batch: 280; loss: 2.25; acc: 0.52
Batch: 300; loss: 1.51; acc: 0.59
Batch: 320; loss: 1.97; acc: 0.53
Batch: 340; loss: 2.8; acc: 0.55
Batch: 360; loss: 1.62; acc: 0.66
Batch: 380; loss: 1.36; acc: 0.7
Batch: 400; loss: 2.39; acc: 0.5
Batch: 420; loss: 2.57; acc: 0.52
Batch: 440; loss: 1.9; acc: 0.55
Batch: 460; loss: 1.06; acc: 0.73
Batch: 480; loss: 1.69; acc: 0.58
Batch: 500; loss: 2.33; acc: 0.5
Batch: 520; loss: 1.95; acc: 0.55
Batch: 540; loss: 1.74; acc: 0.64
Batch: 560; loss: 1.83; acc: 0.61
Batch: 580; loss: 2.49; acc: 0.52
Batch: 600; loss: 1.48; acc: 0.62
Batch: 620; loss: 1.91; acc: 0.48
Train Epoch over. train_loss: 1.86; train_accuracy: 0.6 

Batch: 0; loss: 2.21; acc: 0.5
Batch: 20; loss: 2.44; acc: 0.5
Batch: 40; loss: 1.85; acc: 0.59
Batch: 60; loss: 1.53; acc: 0.66
Batch: 80; loss: 1.74; acc: 0.55
Batch: 100; loss: 1.85; acc: 0.59
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 3.12; acc: 0.39
Val Epoch over. val_loss: 1.9619776879905895; val_accuracy: 0.5807125796178344 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.75; acc: 0.62
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 1.86; acc: 0.55
Batch: 60; loss: 1.91; acc: 0.55
Batch: 80; loss: 2.03; acc: 0.61
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 2.28; acc: 0.5
Batch: 140; loss: 1.55; acc: 0.56
Batch: 160; loss: 1.98; acc: 0.59
Batch: 180; loss: 2.43; acc: 0.48
Batch: 200; loss: 1.41; acc: 0.61
Batch: 220; loss: 1.64; acc: 0.62
Batch: 240; loss: 2.22; acc: 0.62
Batch: 260; loss: 1.83; acc: 0.52
Batch: 280; loss: 1.7; acc: 0.59
Batch: 300; loss: 2.26; acc: 0.55
Batch: 320; loss: 1.79; acc: 0.55
Batch: 340; loss: 2.28; acc: 0.59
Batch: 360; loss: 1.83; acc: 0.58
Batch: 380; loss: 1.67; acc: 0.58
Batch: 400; loss: 2.16; acc: 0.53
Batch: 420; loss: 2.07; acc: 0.53
Batch: 440; loss: 2.05; acc: 0.64
Batch: 460; loss: 2.28; acc: 0.55
Batch: 480; loss: 1.04; acc: 0.72
Batch: 500; loss: 1.6; acc: 0.69
Batch: 520; loss: 1.75; acc: 0.59
Batch: 540; loss: 1.43; acc: 0.69
Batch: 560; loss: 2.29; acc: 0.55
Batch: 580; loss: 2.07; acc: 0.52
Batch: 600; loss: 2.24; acc: 0.52
Batch: 620; loss: 1.05; acc: 0.8
Train Epoch over. train_loss: 1.88; train_accuracy: 0.6 

Batch: 0; loss: 2.26; acc: 0.5
Batch: 20; loss: 2.58; acc: 0.48
Batch: 40; loss: 1.84; acc: 0.61
Batch: 60; loss: 1.59; acc: 0.64
Batch: 80; loss: 1.81; acc: 0.58
Batch: 100; loss: 1.9; acc: 0.59
Batch: 120; loss: 0.96; acc: 0.66
Batch: 140; loss: 3.09; acc: 0.42
Val Epoch over. val_loss: 1.9918385744094849; val_accuracy: 0.5766321656050956 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.82; acc: 0.62
Batch: 20; loss: 1.74; acc: 0.62
Batch: 40; loss: 1.91; acc: 0.53
Batch: 60; loss: 1.46; acc: 0.61
Batch: 80; loss: 1.9; acc: 0.56
Batch: 100; loss: 1.88; acc: 0.67
Batch: 120; loss: 2.14; acc: 0.53
Batch: 140; loss: 1.84; acc: 0.55
Batch: 160; loss: 2.0; acc: 0.59
Batch: 180; loss: 1.84; acc: 0.62
Batch: 200; loss: 1.88; acc: 0.64
Batch: 220; loss: 2.64; acc: 0.48
Batch: 240; loss: 1.92; acc: 0.56
Batch: 260; loss: 1.74; acc: 0.58
Batch: 280; loss: 2.29; acc: 0.53
Batch: 300; loss: 2.27; acc: 0.53
Batch: 320; loss: 1.98; acc: 0.67
Batch: 340; loss: 1.96; acc: 0.58
Batch: 360; loss: 1.51; acc: 0.61
Batch: 380; loss: 1.93; acc: 0.59
Batch: 400; loss: 2.41; acc: 0.47
Batch: 420; loss: 2.55; acc: 0.5
Batch: 440; loss: 1.46; acc: 0.64
Batch: 460; loss: 1.75; acc: 0.64
Batch: 480; loss: 2.56; acc: 0.56
Batch: 500; loss: 1.85; acc: 0.61
Batch: 520; loss: 2.18; acc: 0.59
Batch: 540; loss: 2.35; acc: 0.52
Batch: 560; loss: 1.47; acc: 0.66
Batch: 580; loss: 2.16; acc: 0.53
Batch: 600; loss: 2.81; acc: 0.55
Batch: 620; loss: 2.14; acc: 0.59
Train Epoch over. train_loss: 1.91; train_accuracy: 0.59 

Batch: 0; loss: 2.26; acc: 0.52
Batch: 20; loss: 2.67; acc: 0.45
Batch: 40; loss: 1.89; acc: 0.61
Batch: 60; loss: 1.64; acc: 0.66
Batch: 80; loss: 1.87; acc: 0.58
Batch: 100; loss: 1.94; acc: 0.56
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 3.08; acc: 0.42
Val Epoch over. val_loss: 2.024305814390729; val_accuracy: 0.5730493630573248 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.09; acc: 0.58
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 2.33; acc: 0.48
Batch: 60; loss: 2.66; acc: 0.52
Batch: 80; loss: 1.95; acc: 0.58
Batch: 100; loss: 3.45; acc: 0.41
Batch: 120; loss: 1.93; acc: 0.61
Batch: 140; loss: 1.51; acc: 0.69
Batch: 160; loss: 1.51; acc: 0.59
Batch: 180; loss: 1.7; acc: 0.64
Batch: 200; loss: 2.44; acc: 0.5
Batch: 220; loss: 1.94; acc: 0.58
Batch: 240; loss: 2.36; acc: 0.61
Batch: 260; loss: 0.99; acc: 0.69
Batch: 280; loss: 1.85; acc: 0.56
Batch: 300; loss: 1.87; acc: 0.62
Batch: 320; loss: 1.47; acc: 0.66
Batch: 340; loss: 1.72; acc: 0.56
Batch: 360; loss: 1.82; acc: 0.53
Batch: 380; loss: 1.42; acc: 0.66
Batch: 400; loss: 1.92; acc: 0.58
Batch: 420; loss: 2.68; acc: 0.56
Batch: 440; loss: 2.19; acc: 0.53
Batch: 460; loss: 1.65; acc: 0.67
Batch: 480; loss: 1.77; acc: 0.62
Batch: 500; loss: 1.9; acc: 0.59
Batch: 520; loss: 1.93; acc: 0.61
Batch: 540; loss: 1.67; acc: 0.62
Batch: 560; loss: 1.8; acc: 0.64
Batch: 580; loss: 2.13; acc: 0.5
Batch: 600; loss: 1.85; acc: 0.67
Batch: 620; loss: 2.6; acc: 0.52
Train Epoch over. train_loss: 1.93; train_accuracy: 0.59 

Batch: 0; loss: 2.29; acc: 0.48
Batch: 20; loss: 2.76; acc: 0.45
Batch: 40; loss: 1.93; acc: 0.59
Batch: 60; loss: 1.67; acc: 0.66
Batch: 80; loss: 1.91; acc: 0.58
Batch: 100; loss: 1.98; acc: 0.56
Batch: 120; loss: 1.02; acc: 0.69
Batch: 140; loss: 3.06; acc: 0.44
Val Epoch over. val_loss: 2.0512802262974392; val_accuracy: 0.5698646496815286 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.58; acc: 0.53
Batch: 20; loss: 1.96; acc: 0.55
Batch: 40; loss: 2.25; acc: 0.47
Batch: 60; loss: 2.29; acc: 0.53
Batch: 80; loss: 1.8; acc: 0.59
Batch: 100; loss: 2.17; acc: 0.55
Batch: 120; loss: 2.0; acc: 0.53
Batch: 140; loss: 1.34; acc: 0.67
Batch: 160; loss: 2.01; acc: 0.58
Batch: 180; loss: 1.63; acc: 0.7
Batch: 200; loss: 1.78; acc: 0.62
Batch: 220; loss: 1.58; acc: 0.66
Batch: 240; loss: 1.84; acc: 0.56
Batch: 260; loss: 2.82; acc: 0.56
Batch: 280; loss: 1.56; acc: 0.62
Batch: 300; loss: 1.75; acc: 0.62
Batch: 320; loss: 1.77; acc: 0.62
Batch: 340; loss: 1.89; acc: 0.61
Batch: 360; loss: 1.65; acc: 0.66
Batch: 380; loss: 1.57; acc: 0.58
Batch: 400; loss: 1.65; acc: 0.69
Batch: 420; loss: 2.5; acc: 0.45
Batch: 440; loss: 2.14; acc: 0.61
Batch: 460; loss: 2.2; acc: 0.56
Batch: 480; loss: 1.84; acc: 0.66
Batch: 500; loss: 1.77; acc: 0.5
Batch: 520; loss: 2.53; acc: 0.48
Batch: 540; loss: 3.16; acc: 0.44
Batch: 560; loss: 1.69; acc: 0.66
Batch: 580; loss: 1.97; acc: 0.58
Batch: 600; loss: 1.9; acc: 0.59
Batch: 620; loss: 1.38; acc: 0.64
Train Epoch over. train_loss: 1.95; train_accuracy: 0.59 

Batch: 0; loss: 2.33; acc: 0.47
Batch: 20; loss: 2.85; acc: 0.42
Batch: 40; loss: 1.96; acc: 0.59
Batch: 60; loss: 1.72; acc: 0.66
Batch: 80; loss: 1.95; acc: 0.58
Batch: 100; loss: 2.0; acc: 0.55
Batch: 120; loss: 1.05; acc: 0.69
Batch: 140; loss: 3.03; acc: 0.44
Val Epoch over. val_loss: 2.081132206187886; val_accuracy: 0.5676751592356688 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.53; acc: 0.53
Batch: 20; loss: 1.68; acc: 0.62
Batch: 40; loss: 2.32; acc: 0.58
Batch: 60; loss: 2.52; acc: 0.53
Batch: 80; loss: 2.48; acc: 0.53
Batch: 100; loss: 2.11; acc: 0.56
Batch: 120; loss: 2.18; acc: 0.56
Batch: 140; loss: 2.29; acc: 0.52
Batch: 160; loss: 1.64; acc: 0.62
Batch: 180; loss: 2.59; acc: 0.55
Batch: 200; loss: 1.83; acc: 0.58
Batch: 220; loss: 2.11; acc: 0.56
Batch: 240; loss: 1.9; acc: 0.58
Batch: 260; loss: 1.75; acc: 0.59
Batch: 280; loss: 1.59; acc: 0.58
Batch: 300; loss: 2.59; acc: 0.55
Batch: 320; loss: 2.49; acc: 0.62
Batch: 340; loss: 1.46; acc: 0.61
Batch: 360; loss: 2.02; acc: 0.55
Batch: 380; loss: 1.47; acc: 0.69
Batch: 400; loss: 1.05; acc: 0.7
Batch: 420; loss: 2.7; acc: 0.53
Batch: 440; loss: 2.29; acc: 0.59
Batch: 460; loss: 2.0; acc: 0.53
Batch: 480; loss: 1.74; acc: 0.58
Batch: 500; loss: 1.85; acc: 0.56
Batch: 520; loss: 2.16; acc: 0.59
Batch: 540; loss: 1.38; acc: 0.7
Batch: 560; loss: 2.07; acc: 0.64
Batch: 580; loss: 1.94; acc: 0.61
Batch: 600; loss: 2.3; acc: 0.56
Batch: 620; loss: 1.68; acc: 0.58
Train Epoch over. train_loss: 1.98; train_accuracy: 0.58 

Batch: 0; loss: 2.33; acc: 0.47
Batch: 20; loss: 2.92; acc: 0.39
Batch: 40; loss: 1.97; acc: 0.59
Batch: 60; loss: 1.74; acc: 0.66
Batch: 80; loss: 1.99; acc: 0.56
Batch: 100; loss: 2.05; acc: 0.55
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 3.02; acc: 0.44
Val Epoch over. val_loss: 2.1049110874248917; val_accuracy: 0.5657842356687898 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.08; acc: 0.61
Batch: 20; loss: 2.58; acc: 0.47
Batch: 40; loss: 1.82; acc: 0.58
Batch: 60; loss: 1.68; acc: 0.69
Batch: 80; loss: 1.86; acc: 0.59
Batch: 100; loss: 1.44; acc: 0.69
Batch: 120; loss: 1.69; acc: 0.61
Batch: 140; loss: 1.85; acc: 0.56
Batch: 160; loss: 1.6; acc: 0.66
Batch: 180; loss: 2.31; acc: 0.61
Batch: 200; loss: 2.06; acc: 0.69
Batch: 220; loss: 1.62; acc: 0.62
Batch: 240; loss: 2.0; acc: 0.59
Batch: 260; loss: 1.36; acc: 0.64
Batch: 280; loss: 1.94; acc: 0.56
Batch: 300; loss: 2.23; acc: 0.56
Batch: 320; loss: 1.63; acc: 0.62
Batch: 340; loss: 2.01; acc: 0.58
Batch: 360; loss: 2.43; acc: 0.52
Batch: 380; loss: 1.63; acc: 0.59
Batch: 400; loss: 2.12; acc: 0.5
Batch: 420; loss: 2.39; acc: 0.52
Batch: 440; loss: 2.0; acc: 0.56
Batch: 460; loss: 1.48; acc: 0.69
Batch: 480; loss: 2.24; acc: 0.59
Batch: 500; loss: 1.96; acc: 0.59
Batch: 520; loss: 1.69; acc: 0.66
Batch: 540; loss: 1.89; acc: 0.55
Batch: 560; loss: 1.48; acc: 0.53
Batch: 580; loss: 1.87; acc: 0.59
Batch: 600; loss: 2.59; acc: 0.52
Batch: 620; loss: 1.42; acc: 0.66
Train Epoch over. train_loss: 2.0; train_accuracy: 0.58 

Batch: 0; loss: 2.36; acc: 0.47
Batch: 20; loss: 3.01; acc: 0.41
Batch: 40; loss: 2.0; acc: 0.58
Batch: 60; loss: 1.76; acc: 0.66
Batch: 80; loss: 2.01; acc: 0.56
Batch: 100; loss: 2.1; acc: 0.53
Batch: 120; loss: 1.11; acc: 0.67
Batch: 140; loss: 3.0; acc: 0.42
Val Epoch over. val_loss: 2.1266343768235223; val_accuracy: 0.56359474522293 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.68; acc: 0.64
Batch: 20; loss: 1.52; acc: 0.61
Batch: 40; loss: 1.68; acc: 0.61
Batch: 60; loss: 1.71; acc: 0.64
Batch: 80; loss: 1.86; acc: 0.67
Batch: 100; loss: 2.27; acc: 0.61
Batch: 120; loss: 1.98; acc: 0.61
Batch: 140; loss: 2.28; acc: 0.48
Batch: 160; loss: 2.05; acc: 0.58
Batch: 180; loss: 1.79; acc: 0.56
Batch: 200; loss: 1.57; acc: 0.56
Batch: 220; loss: 1.91; acc: 0.58
Batch: 240; loss: 1.21; acc: 0.69
Batch: 260; loss: 1.93; acc: 0.56
Batch: 280; loss: 2.63; acc: 0.52
Batch: 300; loss: 2.48; acc: 0.55
Batch: 320; loss: 2.09; acc: 0.56
Batch: 340; loss: 2.59; acc: 0.5
Batch: 360; loss: 2.05; acc: 0.58
Batch: 380; loss: 1.79; acc: 0.58
Batch: 400; loss: 2.25; acc: 0.58
Batch: 420; loss: 1.72; acc: 0.67
Batch: 440; loss: 1.73; acc: 0.62
Batch: 460; loss: 2.81; acc: 0.53
Batch: 480; loss: 1.9; acc: 0.61
Batch: 500; loss: 2.03; acc: 0.62
Batch: 520; loss: 1.56; acc: 0.72
Batch: 540; loss: 1.53; acc: 0.66
Batch: 560; loss: 1.9; acc: 0.66
Batch: 580; loss: 1.67; acc: 0.66
Batch: 600; loss: 1.69; acc: 0.64
Batch: 620; loss: 1.86; acc: 0.61
Train Epoch over. train_loss: 2.01; train_accuracy: 0.58 

Batch: 0; loss: 2.36; acc: 0.48
Batch: 20; loss: 3.13; acc: 0.44
Batch: 40; loss: 2.02; acc: 0.56
Batch: 60; loss: 1.77; acc: 0.66
Batch: 80; loss: 2.03; acc: 0.56
Batch: 100; loss: 2.14; acc: 0.53
Batch: 120; loss: 1.11; acc: 0.67
Batch: 140; loss: 3.02; acc: 0.39
Val Epoch over. val_loss: 2.1448967973138116; val_accuracy: 0.5638933121019108 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.76; acc: 0.58
Batch: 20; loss: 2.25; acc: 0.45
Batch: 40; loss: 2.67; acc: 0.58
Batch: 60; loss: 2.5; acc: 0.5
Batch: 80; loss: 1.92; acc: 0.47
Batch: 100; loss: 2.37; acc: 0.58
Batch: 120; loss: 1.81; acc: 0.56
Batch: 140; loss: 1.85; acc: 0.66
Batch: 160; loss: 1.62; acc: 0.61
Batch: 180; loss: 1.85; acc: 0.73
Batch: 200; loss: 2.13; acc: 0.59
Batch: 220; loss: 2.41; acc: 0.5
Batch: 240; loss: 1.08; acc: 0.7
Batch: 260; loss: 1.55; acc: 0.62
Batch: 280; loss: 2.2; acc: 0.61
Batch: 300; loss: 1.61; acc: 0.61
Batch: 320; loss: 2.59; acc: 0.48
Batch: 340; loss: 2.09; acc: 0.64
Batch: 360; loss: 2.19; acc: 0.55
Batch: 380; loss: 1.57; acc: 0.66
Batch: 400; loss: 1.75; acc: 0.67
Batch: 420; loss: 2.59; acc: 0.59
Batch: 440; loss: 2.23; acc: 0.55
Batch: 460; loss: 1.64; acc: 0.67
Batch: 480; loss: 1.13; acc: 0.7
Batch: 500; loss: 2.06; acc: 0.58
Batch: 520; loss: 2.08; acc: 0.45
Batch: 540; loss: 2.44; acc: 0.55
Batch: 560; loss: 1.95; acc: 0.58
Batch: 580; loss: 1.65; acc: 0.62
Batch: 600; loss: 2.53; acc: 0.47
Batch: 620; loss: 2.23; acc: 0.56
Train Epoch over. train_loss: 2.03; train_accuracy: 0.58 

Batch: 0; loss: 2.39; acc: 0.48
Batch: 20; loss: 3.23; acc: 0.38
Batch: 40; loss: 2.03; acc: 0.55
Batch: 60; loss: 1.82; acc: 0.66
Batch: 80; loss: 2.08; acc: 0.55
Batch: 100; loss: 2.16; acc: 0.55
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 3.04; acc: 0.41
Val Epoch over. val_loss: 2.176992939535979; val_accuracy: 0.5603105095541401 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.98; acc: 0.55
Batch: 20; loss: 2.99; acc: 0.5
Batch: 40; loss: 1.83; acc: 0.52
Batch: 60; loss: 1.83; acc: 0.59
Batch: 80; loss: 1.81; acc: 0.58
Batch: 100; loss: 1.51; acc: 0.7
Batch: 120; loss: 2.07; acc: 0.59
Batch: 140; loss: 1.81; acc: 0.56
Batch: 160; loss: 1.38; acc: 0.69
Batch: 180; loss: 1.82; acc: 0.62
Batch: 200; loss: 2.18; acc: 0.58
Batch: 220; loss: 2.08; acc: 0.56
Batch: 240; loss: 1.89; acc: 0.61
Batch: 260; loss: 1.99; acc: 0.64
Batch: 280; loss: 2.36; acc: 0.59
Batch: 300; loss: 2.09; acc: 0.64
Batch: 320; loss: 2.12; acc: 0.58
Batch: 340; loss: 1.94; acc: 0.56
Batch: 360; loss: 2.35; acc: 0.47
Batch: 380; loss: 2.49; acc: 0.52
Batch: 400; loss: 2.04; acc: 0.5
Batch: 420; loss: 2.06; acc: 0.55
Batch: 440; loss: 2.06; acc: 0.55
Batch: 460; loss: 2.14; acc: 0.53
Batch: 480; loss: 1.56; acc: 0.59
Batch: 500; loss: 1.94; acc: 0.61
Batch: 520; loss: 2.21; acc: 0.48
Batch: 540; loss: 2.32; acc: 0.53
Batch: 560; loss: 2.51; acc: 0.59
Batch: 580; loss: 2.62; acc: 0.45
Batch: 600; loss: 2.61; acc: 0.52
Batch: 620; loss: 1.65; acc: 0.64
Train Epoch over. train_loss: 2.05; train_accuracy: 0.58 

Batch: 0; loss: 2.45; acc: 0.52
Batch: 20; loss: 3.31; acc: 0.36
Batch: 40; loss: 2.03; acc: 0.53
Batch: 60; loss: 1.84; acc: 0.66
Batch: 80; loss: 2.1; acc: 0.55
Batch: 100; loss: 2.19; acc: 0.55
Batch: 120; loss: 1.15; acc: 0.72
Batch: 140; loss: 3.03; acc: 0.41
Val Epoch over. val_loss: 2.187069690910874; val_accuracy: 0.5608081210191083 

plots/subspace_training/lenet/2020-01-10 14:00:56/d_dim_100_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 42075
elements in E: 8885200
fraction nonzero: 0.004735402692117229
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.01; acc: 0.47
Batch: 40; loss: 2.07; acc: 0.52
Batch: 60; loss: 2.03; acc: 0.5
Batch: 80; loss: 1.46; acc: 0.56
Batch: 100; loss: 1.16; acc: 0.67
Batch: 120; loss: 1.09; acc: 0.56
Batch: 140; loss: 1.43; acc: 0.62
Batch: 160; loss: 2.14; acc: 0.59
Batch: 180; loss: 1.4; acc: 0.61
Batch: 200; loss: 1.23; acc: 0.7
Batch: 220; loss: 2.11; acc: 0.58
Batch: 240; loss: 1.77; acc: 0.62
Batch: 260; loss: 1.91; acc: 0.53
Batch: 280; loss: 1.18; acc: 0.75
Batch: 300; loss: 1.66; acc: 0.59
Batch: 320; loss: 0.87; acc: 0.7
Batch: 340; loss: 1.23; acc: 0.73
Batch: 360; loss: 0.85; acc: 0.77
Batch: 380; loss: 2.0; acc: 0.64
Batch: 400; loss: 1.11; acc: 0.69
Batch: 420; loss: 1.07; acc: 0.73
Batch: 440; loss: 1.28; acc: 0.77
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.9; acc: 0.73
Batch: 500; loss: 1.23; acc: 0.7
Batch: 520; loss: 1.36; acc: 0.66
Batch: 540; loss: 1.18; acc: 0.73
Batch: 560; loss: 0.78; acc: 0.84
Batch: 580; loss: 0.89; acc: 0.78
Batch: 600; loss: 1.21; acc: 0.7
Batch: 620; loss: 1.9; acc: 0.53
Train Epoch over. train_loss: 1.58; train_accuracy: 0.63 

Batch: 0; loss: 1.29; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 0.9; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 1.41; acc: 0.66
Batch: 100; loss: 1.76; acc: 0.64
Batch: 120; loss: 1.48; acc: 0.69
Batch: 140; loss: 1.75; acc: 0.59
Val Epoch over. val_loss: 1.3349610214020795; val_accuracy: 0.6711783439490446 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 1.85; acc: 0.58
Batch: 20; loss: 1.26; acc: 0.67
Batch: 40; loss: 1.41; acc: 0.61
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 1.0; acc: 0.73
Batch: 100; loss: 1.43; acc: 0.66
Batch: 120; loss: 1.86; acc: 0.62
Batch: 140; loss: 1.91; acc: 0.61
Batch: 160; loss: 1.34; acc: 0.7
Batch: 180; loss: 0.99; acc: 0.66
Batch: 200; loss: 1.65; acc: 0.69
Batch: 220; loss: 0.67; acc: 0.73
Batch: 240; loss: 1.07; acc: 0.75
Batch: 260; loss: 1.41; acc: 0.59
Batch: 280; loss: 0.92; acc: 0.67
Batch: 300; loss: 1.87; acc: 0.67
Batch: 320; loss: 1.02; acc: 0.77
Batch: 340; loss: 1.29; acc: 0.7
Batch: 360; loss: 2.15; acc: 0.62
Batch: 380; loss: 0.81; acc: 0.83
Batch: 400; loss: 1.57; acc: 0.58
Batch: 420; loss: 1.29; acc: 0.67
Batch: 440; loss: 1.34; acc: 0.66
Batch: 460; loss: 0.89; acc: 0.8
Batch: 480; loss: 1.83; acc: 0.67
Batch: 500; loss: 1.46; acc: 0.64
Batch: 520; loss: 1.11; acc: 0.72
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 1.36; acc: 0.69
Batch: 580; loss: 0.83; acc: 0.77
Batch: 600; loss: 1.5; acc: 0.69
Batch: 620; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 1.25; train_accuracy: 0.7 

Batch: 0; loss: 1.09; acc: 0.78
Batch: 20; loss: 1.57; acc: 0.61
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.34; acc: 0.64
Batch: 100; loss: 1.43; acc: 0.73
Batch: 120; loss: 1.46; acc: 0.78
Batch: 140; loss: 1.79; acc: 0.56
Val Epoch over. val_loss: 1.2869183011115737; val_accuracy: 0.7055135350318471 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 1.3; acc: 0.69
Batch: 20; loss: 1.18; acc: 0.75
Batch: 40; loss: 1.14; acc: 0.73
Batch: 60; loss: 1.16; acc: 0.73
Batch: 80; loss: 1.19; acc: 0.75
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 1.9; acc: 0.59
Batch: 140; loss: 1.28; acc: 0.69
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 1.27; acc: 0.7
Batch: 200; loss: 0.8; acc: 0.83
Batch: 220; loss: 1.59; acc: 0.69
Batch: 240; loss: 1.8; acc: 0.66
Batch: 260; loss: 1.29; acc: 0.69
Batch: 280; loss: 1.52; acc: 0.66
Batch: 300; loss: 1.18; acc: 0.67
Batch: 320; loss: 1.19; acc: 0.77
Batch: 340; loss: 1.04; acc: 0.75
Batch: 360; loss: 0.76; acc: 0.73
Batch: 380; loss: 1.3; acc: 0.72
Batch: 400; loss: 1.56; acc: 0.69
Batch: 420; loss: 1.25; acc: 0.67
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 1.06; acc: 0.7
Batch: 480; loss: 2.04; acc: 0.55
Batch: 500; loss: 1.37; acc: 0.64
Batch: 520; loss: 1.4; acc: 0.7
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 1.53; acc: 0.67
Batch: 600; loss: 0.9; acc: 0.73
Batch: 620; loss: 1.11; acc: 0.75
Train Epoch over. train_loss: 1.19; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.8
Batch: 20; loss: 2.49; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.77
Batch: 60; loss: 1.25; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.72
Batch: 100; loss: 1.85; acc: 0.66
Batch: 120; loss: 0.99; acc: 0.8
Batch: 140; loss: 2.2; acc: 0.5
Val Epoch over. val_loss: 1.349682911945756; val_accuracy: 0.7082006369426752 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.94; acc: 0.66
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 1.41; acc: 0.7
Batch: 80; loss: 1.23; acc: 0.73
Batch: 100; loss: 1.25; acc: 0.83
Batch: 120; loss: 1.16; acc: 0.75
Batch: 140; loss: 0.95; acc: 0.72
Batch: 160; loss: 1.2; acc: 0.73
Batch: 180; loss: 0.78; acc: 0.83
Batch: 200; loss: 1.0; acc: 0.73
Batch: 220; loss: 1.05; acc: 0.73
Batch: 240; loss: 1.3; acc: 0.72
Batch: 260; loss: 0.82; acc: 0.81
Batch: 280; loss: 1.16; acc: 0.72
Batch: 300; loss: 1.42; acc: 0.59
Batch: 320; loss: 1.27; acc: 0.72
Batch: 340; loss: 1.12; acc: 0.72
Batch: 360; loss: 1.22; acc: 0.75
Batch: 380; loss: 1.34; acc: 0.69
Batch: 400; loss: 1.06; acc: 0.8
Batch: 420; loss: 1.43; acc: 0.73
Batch: 440; loss: 1.61; acc: 0.72
Batch: 460; loss: 1.18; acc: 0.73
Batch: 480; loss: 1.65; acc: 0.67
Batch: 500; loss: 1.07; acc: 0.77
Batch: 520; loss: 1.0; acc: 0.8
Batch: 540; loss: 0.96; acc: 0.7
Batch: 560; loss: 0.99; acc: 0.77
Batch: 580; loss: 0.84; acc: 0.7
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 1.0; acc: 0.75
Train Epoch over. train_loss: 1.14; train_accuracy: 0.73 

Batch: 0; loss: 0.93; acc: 0.78
Batch: 20; loss: 2.28; acc: 0.62
Batch: 40; loss: 0.92; acc: 0.72
Batch: 60; loss: 1.47; acc: 0.67
Batch: 80; loss: 1.06; acc: 0.72
Batch: 100; loss: 1.68; acc: 0.7
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 1.73; acc: 0.61
Val Epoch over. val_loss: 1.2246621577603043; val_accuracy: 0.7216361464968153 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 1.31; acc: 0.72
Batch: 20; loss: 1.37; acc: 0.75
Batch: 40; loss: 1.02; acc: 0.72
Batch: 60; loss: 1.78; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 1.44; acc: 0.67
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 1.69; acc: 0.75
Batch: 160; loss: 0.96; acc: 0.72
Batch: 180; loss: 0.99; acc: 0.77
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 1.12; acc: 0.73
Batch: 240; loss: 1.01; acc: 0.8
Batch: 260; loss: 1.25; acc: 0.73
Batch: 280; loss: 1.04; acc: 0.73
Batch: 300; loss: 1.05; acc: 0.78
Batch: 320; loss: 1.03; acc: 0.75
Batch: 340; loss: 1.51; acc: 0.67
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.97; acc: 0.78
Batch: 400; loss: 0.99; acc: 0.75
Batch: 420; loss: 0.69; acc: 0.75
Batch: 440; loss: 0.68; acc: 0.77
Batch: 460; loss: 1.31; acc: 0.7
Batch: 480; loss: 1.33; acc: 0.77
Batch: 500; loss: 1.24; acc: 0.67
Batch: 520; loss: 0.58; acc: 0.78
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 1.32; acc: 0.81
Batch: 580; loss: 0.94; acc: 0.75
Batch: 600; loss: 1.14; acc: 0.7
Batch: 620; loss: 1.36; acc: 0.75
Train Epoch over. train_loss: 1.12; train_accuracy: 0.74 

Batch: 0; loss: 0.89; acc: 0.81
Batch: 20; loss: 1.65; acc: 0.61
Batch: 40; loss: 0.89; acc: 0.83
Batch: 60; loss: 1.42; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.77
Batch: 100; loss: 1.51; acc: 0.73
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 2.0; acc: 0.59
Val Epoch over. val_loss: 1.1933886311995756; val_accuracy: 0.7234275477707006 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 1.48; acc: 0.75
Batch: 20; loss: 1.6; acc: 0.64
Batch: 40; loss: 1.07; acc: 0.78
Batch: 60; loss: 1.81; acc: 0.7
Batch: 80; loss: 1.25; acc: 0.78
Batch: 100; loss: 1.45; acc: 0.66
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 1.92; acc: 0.62
Batch: 160; loss: 1.76; acc: 0.62
Batch: 180; loss: 1.1; acc: 0.72
Batch: 200; loss: 1.22; acc: 0.67
Batch: 220; loss: 0.73; acc: 0.78
Batch: 240; loss: 1.19; acc: 0.64
Batch: 260; loss: 0.6; acc: 0.78
Batch: 280; loss: 1.09; acc: 0.78
Batch: 300; loss: 0.8; acc: 0.81
Batch: 320; loss: 1.38; acc: 0.7
Batch: 340; loss: 1.52; acc: 0.66
Batch: 360; loss: 1.38; acc: 0.72
Batch: 380; loss: 1.19; acc: 0.69
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.88; acc: 0.77
Batch: 460; loss: 0.89; acc: 0.75
Batch: 480; loss: 0.94; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.83
Batch: 520; loss: 1.25; acc: 0.75
Batch: 540; loss: 0.69; acc: 0.83
Batch: 560; loss: 0.94; acc: 0.81
Batch: 580; loss: 1.41; acc: 0.72
Batch: 600; loss: 1.24; acc: 0.73
Batch: 620; loss: 1.01; acc: 0.8
Train Epoch over. train_loss: 1.11; train_accuracy: 0.75 

Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 2.21; acc: 0.64
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 1.93; acc: 0.64
Batch: 80; loss: 1.59; acc: 0.58
Batch: 100; loss: 1.81; acc: 0.69
Batch: 120; loss: 1.18; acc: 0.73
Batch: 140; loss: 2.37; acc: 0.67
Val Epoch over. val_loss: 1.5515763622939966; val_accuracy: 0.690187101910828 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 1.31; acc: 0.7
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 1.07; acc: 0.73
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 1.11; acc: 0.64
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 1.33; acc: 0.8
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 1.09; acc: 0.75
Batch: 180; loss: 1.32; acc: 0.73
Batch: 200; loss: 1.03; acc: 0.73
Batch: 220; loss: 1.12; acc: 0.73
Batch: 240; loss: 1.79; acc: 0.7
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.93; acc: 0.81
Batch: 300; loss: 1.22; acc: 0.78
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 1.13; acc: 0.77
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.92; acc: 0.81
Batch: 400; loss: 0.7; acc: 0.78
Batch: 420; loss: 1.54; acc: 0.72
Batch: 440; loss: 0.67; acc: 0.75
Batch: 460; loss: 1.08; acc: 0.72
Batch: 480; loss: 1.48; acc: 0.73
Batch: 500; loss: 1.17; acc: 0.78
Batch: 520; loss: 1.36; acc: 0.67
Batch: 540; loss: 0.9; acc: 0.78
Batch: 560; loss: 1.16; acc: 0.72
Batch: 580; loss: 1.21; acc: 0.73
Batch: 600; loss: 1.1; acc: 0.75
Batch: 620; loss: 1.14; acc: 0.75
Train Epoch over. train_loss: 1.08; train_accuracy: 0.76 

Batch: 0; loss: 0.95; acc: 0.8
Batch: 20; loss: 1.96; acc: 0.59
Batch: 40; loss: 1.03; acc: 0.8
Batch: 60; loss: 1.59; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.73
Batch: 100; loss: 1.49; acc: 0.67
Batch: 120; loss: 0.82; acc: 0.81
Batch: 140; loss: 2.03; acc: 0.66
Val Epoch over. val_loss: 1.2232837346708698; val_accuracy: 0.7350716560509554 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 1.24; acc: 0.66
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 1.28; acc: 0.69
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 1.27; acc: 0.69
Batch: 100; loss: 1.14; acc: 0.83
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 1.44; acc: 0.78
Batch: 180; loss: 1.15; acc: 0.77
Batch: 200; loss: 1.27; acc: 0.67
Batch: 220; loss: 0.9; acc: 0.78
Batch: 240; loss: 1.27; acc: 0.75
Batch: 260; loss: 0.99; acc: 0.75
Batch: 280; loss: 0.88; acc: 0.77
Batch: 300; loss: 0.64; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.8
Batch: 340; loss: 0.95; acc: 0.78
Batch: 360; loss: 0.99; acc: 0.81
Batch: 380; loss: 0.94; acc: 0.72
Batch: 400; loss: 0.99; acc: 0.75
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 1.46; acc: 0.72
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 1.08; acc: 0.77
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 1.03; acc: 0.83
Batch: 540; loss: 1.5; acc: 0.73
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.81
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 1.04; train_accuracy: 0.76 

Batch: 0; loss: 1.24; acc: 0.73
Batch: 20; loss: 1.89; acc: 0.66
Batch: 40; loss: 1.15; acc: 0.75
Batch: 60; loss: 1.66; acc: 0.7
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.17; acc: 0.77
Batch: 120; loss: 0.92; acc: 0.81
Batch: 140; loss: 2.05; acc: 0.64
Val Epoch over. val_loss: 1.34369500731207; val_accuracy: 0.7151671974522293 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.15; acc: 0.7
Batch: 20; loss: 1.29; acc: 0.73
Batch: 40; loss: 1.43; acc: 0.7
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 1.45; acc: 0.7
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 1.28; acc: 0.67
Batch: 160; loss: 1.3; acc: 0.78
Batch: 180; loss: 1.38; acc: 0.66
Batch: 200; loss: 0.7; acc: 0.81
Batch: 220; loss: 1.02; acc: 0.75
Batch: 240; loss: 1.37; acc: 0.69
Batch: 260; loss: 0.94; acc: 0.77
Batch: 280; loss: 0.94; acc: 0.8
Batch: 300; loss: 1.89; acc: 0.67
Batch: 320; loss: 0.84; acc: 0.7
Batch: 340; loss: 0.82; acc: 0.8
Batch: 360; loss: 1.31; acc: 0.72
Batch: 380; loss: 1.04; acc: 0.78
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 1.02; acc: 0.8
Batch: 440; loss: 0.87; acc: 0.81
Batch: 460; loss: 0.79; acc: 0.84
Batch: 480; loss: 0.92; acc: 0.75
Batch: 500; loss: 1.21; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 1.16; acc: 0.77
Batch: 580; loss: 1.12; acc: 0.77
Batch: 600; loss: 1.36; acc: 0.77
Batch: 620; loss: 1.0; acc: 0.8
Train Epoch over. train_loss: 1.02; train_accuracy: 0.77 

Batch: 0; loss: 1.09; acc: 0.77
Batch: 20; loss: 1.98; acc: 0.67
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 2.03; acc: 0.59
Batch: 80; loss: 1.51; acc: 0.75
Batch: 100; loss: 1.07; acc: 0.8
Batch: 120; loss: 1.1; acc: 0.77
Batch: 140; loss: 1.79; acc: 0.7
Val Epoch over. val_loss: 1.4160203903344026; val_accuracy: 0.7183519108280255 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 1.39; acc: 0.75
Batch: 20; loss: 1.44; acc: 0.7
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 1.38; acc: 0.73
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 1.17; acc: 0.77
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.79; acc: 0.83
Batch: 200; loss: 0.84; acc: 0.8
Batch: 220; loss: 0.79; acc: 0.77
Batch: 240; loss: 0.71; acc: 0.75
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 1.05; acc: 0.77
Batch: 300; loss: 1.16; acc: 0.69
Batch: 320; loss: 1.11; acc: 0.77
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 0.96; acc: 0.73
Batch: 380; loss: 1.12; acc: 0.75
Batch: 400; loss: 0.85; acc: 0.72
Batch: 420; loss: 1.49; acc: 0.72
Batch: 440; loss: 0.94; acc: 0.8
Batch: 460; loss: 1.53; acc: 0.72
Batch: 480; loss: 0.73; acc: 0.8
Batch: 500; loss: 0.9; acc: 0.8
Batch: 520; loss: 0.99; acc: 0.81
Batch: 540; loss: 1.21; acc: 0.77
Batch: 560; loss: 2.39; acc: 0.62
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.86; acc: 0.78
Batch: 620; loss: 0.82; acc: 0.77
Train Epoch over. train_loss: 1.01; train_accuracy: 0.77 

Batch: 0; loss: 1.32; acc: 0.72
Batch: 20; loss: 2.49; acc: 0.66
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 1.51; acc: 0.72
Batch: 80; loss: 1.65; acc: 0.73
Batch: 100; loss: 1.34; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.91
Batch: 140; loss: 1.91; acc: 0.69
Val Epoch over. val_loss: 1.2642114282034005; val_accuracy: 0.7363654458598726 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.76; acc: 0.7
Batch: 20; loss: 1.31; acc: 0.72
Batch: 40; loss: 1.37; acc: 0.67
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.87; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.89
Batch: 120; loss: 1.25; acc: 0.75
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 1.16; acc: 0.77
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.74; acc: 0.77
Batch: 220; loss: 1.35; acc: 0.72
Batch: 240; loss: 0.89; acc: 0.89
Batch: 260; loss: 0.95; acc: 0.83
Batch: 280; loss: 0.73; acc: 0.8
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.85; acc: 0.78
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.85; acc: 0.78
Batch: 400; loss: 1.07; acc: 0.75
Batch: 420; loss: 1.01; acc: 0.75
Batch: 440; loss: 0.78; acc: 0.81
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.84; acc: 0.7
Batch: 500; loss: 1.33; acc: 0.75
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.79; acc: 0.84
Batch: 560; loss: 0.91; acc: 0.78
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.91; acc: 0.81
Batch: 20; loss: 2.13; acc: 0.66
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 1.44; acc: 0.72
Batch: 80; loss: 1.24; acc: 0.78
Batch: 100; loss: 0.97; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.88
Batch: 140; loss: 1.77; acc: 0.66
Val Epoch over. val_loss: 1.0512937865439493; val_accuracy: 0.7732882165605095 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 1.3; acc: 0.73
Batch: 40; loss: 1.06; acc: 0.78
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 1.08; acc: 0.81
Batch: 100; loss: 1.27; acc: 0.7
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.86; acc: 0.84
Batch: 160; loss: 0.74; acc: 0.77
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 1.39; acc: 0.73
Batch: 260; loss: 0.78; acc: 0.86
Batch: 280; loss: 1.19; acc: 0.7
Batch: 300; loss: 0.93; acc: 0.8
Batch: 320; loss: 1.17; acc: 0.81
Batch: 340; loss: 1.27; acc: 0.77
Batch: 360; loss: 0.79; acc: 0.88
Batch: 380; loss: 1.27; acc: 0.69
Batch: 400; loss: 0.84; acc: 0.84
Batch: 420; loss: 1.49; acc: 0.75
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.82; acc: 0.84
Batch: 500; loss: 0.73; acc: 0.77
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 1.04; acc: 0.77
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 0.68; acc: 0.81
Batch: 600; loss: 1.09; acc: 0.75
Batch: 620; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.82; train_accuracy: 0.81 

Batch: 0; loss: 0.99; acc: 0.81
Batch: 20; loss: 2.21; acc: 0.64
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 1.45; acc: 0.73
Batch: 80; loss: 1.16; acc: 0.78
Batch: 100; loss: 1.01; acc: 0.75
Batch: 120; loss: 0.84; acc: 0.88
Batch: 140; loss: 1.83; acc: 0.66
Val Epoch over. val_loss: 1.0843723234097669; val_accuracy: 0.7698049363057324 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.08; acc: 0.77
Batch: 20; loss: 1.17; acc: 0.81
Batch: 40; loss: 0.97; acc: 0.77
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 1.0; acc: 0.8
Batch: 100; loss: 0.86; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 0.75; acc: 0.77
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.99; acc: 0.75
Batch: 240; loss: 0.85; acc: 0.77
Batch: 260; loss: 0.77; acc: 0.88
Batch: 280; loss: 0.92; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 1.1; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.94
Batch: 380; loss: 1.04; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.83
Batch: 420; loss: 0.92; acc: 0.84
Batch: 440; loss: 0.83; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 1.1; acc: 0.73
Batch: 540; loss: 0.76; acc: 0.83
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.78; acc: 0.84
Train Epoch over. train_loss: 0.82; train_accuracy: 0.81 

Batch: 0; loss: 0.94; acc: 0.81
Batch: 20; loss: 2.25; acc: 0.64
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 1.44; acc: 0.75
Batch: 80; loss: 1.16; acc: 0.8
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.84; acc: 0.88
Batch: 140; loss: 1.79; acc: 0.66
Val Epoch over. val_loss: 1.0859925682377662; val_accuracy: 0.7723925159235668 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.05; acc: 0.78
Batch: 20; loss: 0.93; acc: 0.86
Batch: 40; loss: 0.99; acc: 0.72
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.85; acc: 0.81
Batch: 120; loss: 1.07; acc: 0.77
Batch: 140; loss: 0.76; acc: 0.83
Batch: 160; loss: 1.24; acc: 0.77
Batch: 180; loss: 0.83; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.78
Batch: 220; loss: 0.83; acc: 0.8
Batch: 240; loss: 0.68; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 1.02; acc: 0.8
Batch: 320; loss: 0.77; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.69; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.78
Batch: 400; loss: 0.9; acc: 0.81
Batch: 420; loss: 0.9; acc: 0.81
Batch: 440; loss: 1.21; acc: 0.78
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.88
Batch: 500; loss: 0.88; acc: 0.77
Batch: 520; loss: 1.37; acc: 0.75
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 1.38; acc: 0.73
Batch: 580; loss: 0.74; acc: 0.86
Batch: 600; loss: 1.12; acc: 0.78
Batch: 620; loss: 0.85; acc: 0.8
Train Epoch over. train_loss: 0.82; train_accuracy: 0.81 

Batch: 0; loss: 0.93; acc: 0.83
Batch: 20; loss: 2.25; acc: 0.66
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 1.46; acc: 0.7
Batch: 80; loss: 1.14; acc: 0.78
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.88
Batch: 140; loss: 1.84; acc: 0.67
Val Epoch over. val_loss: 1.0647957763474458; val_accuracy: 0.7752786624203821 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.13; acc: 0.75
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 1.51; acc: 0.72
Batch: 120; loss: 1.37; acc: 0.7
Batch: 140; loss: 1.0; acc: 0.75
Batch: 160; loss: 0.87; acc: 0.78
Batch: 180; loss: 0.77; acc: 0.84
Batch: 200; loss: 0.88; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 1.52; acc: 0.73
Batch: 260; loss: 0.98; acc: 0.75
Batch: 280; loss: 0.89; acc: 0.83
Batch: 300; loss: 1.11; acc: 0.77
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.79; acc: 0.8
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.83
Batch: 500; loss: 1.15; acc: 0.77
Batch: 520; loss: 1.3; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.88
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.83; acc: 0.77
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 1.35; acc: 0.77
Train Epoch over. train_loss: 0.83; train_accuracy: 0.81 

Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 2.36; acc: 0.67
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 1.45; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.78
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 0.82; acc: 0.86
Batch: 140; loss: 1.93; acc: 0.64
Val Epoch over. val_loss: 1.076507169160114; val_accuracy: 0.7730891719745223 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.14; acc: 0.72
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 1.35; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.81
Batch: 100; loss: 1.18; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.97; acc: 0.8
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.75; acc: 0.77
Batch: 200; loss: 1.55; acc: 0.67
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.73; acc: 0.84
Batch: 260; loss: 0.91; acc: 0.83
Batch: 280; loss: 0.81; acc: 0.78
Batch: 300; loss: 0.84; acc: 0.81
Batch: 320; loss: 0.74; acc: 0.8
Batch: 340; loss: 0.73; acc: 0.86
Batch: 360; loss: 0.92; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.96; acc: 0.78
Batch: 440; loss: 1.08; acc: 0.78
Batch: 460; loss: 1.16; acc: 0.81
Batch: 480; loss: 0.66; acc: 0.78
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.73; acc: 0.84
Batch: 560; loss: 0.87; acc: 0.78
Batch: 580; loss: 1.28; acc: 0.7
Batch: 600; loss: 0.89; acc: 0.84
Batch: 620; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.83; train_accuracy: 0.81 

Batch: 0; loss: 0.9; acc: 0.83
Batch: 20; loss: 2.42; acc: 0.64
Batch: 40; loss: 0.71; acc: 0.83
Batch: 60; loss: 1.51; acc: 0.7
Batch: 80; loss: 1.14; acc: 0.8
Batch: 100; loss: 0.86; acc: 0.78
Batch: 120; loss: 0.82; acc: 0.89
Batch: 140; loss: 1.9; acc: 0.61
Val Epoch over. val_loss: 1.1038842546712062; val_accuracy: 0.7697054140127388 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.8; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 1.1; acc: 0.77
Batch: 60; loss: 1.11; acc: 0.77
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 1.15; acc: 0.75
Batch: 160; loss: 0.75; acc: 0.75
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 1.29; acc: 0.8
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.95; acc: 0.72
Batch: 280; loss: 1.28; acc: 0.69
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 1.0; acc: 0.8
Batch: 360; loss: 1.48; acc: 0.75
Batch: 380; loss: 0.89; acc: 0.81
Batch: 400; loss: 1.14; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 1.01; acc: 0.67
Batch: 480; loss: 1.03; acc: 0.8
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 1.14; acc: 0.8
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 1.06; acc: 0.8
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.82; acc: 0.8
Train Epoch over. train_loss: 0.83; train_accuracy: 0.81 

Batch: 0; loss: 0.91; acc: 0.8
Batch: 20; loss: 2.43; acc: 0.64
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 1.44; acc: 0.73
Batch: 80; loss: 1.13; acc: 0.78
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 0.86; acc: 0.86
Batch: 140; loss: 1.96; acc: 0.64
Val Epoch over. val_loss: 1.1217811437928753; val_accuracy: 0.7681130573248408 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 1.15; acc: 0.84
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 1.27; acc: 0.72
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 1.36; acc: 0.8
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 0.99; acc: 0.73
Batch: 240; loss: 1.07; acc: 0.69
Batch: 260; loss: 1.31; acc: 0.75
Batch: 280; loss: 0.88; acc: 0.78
Batch: 300; loss: 0.94; acc: 0.78
Batch: 320; loss: 0.8; acc: 0.86
Batch: 340; loss: 1.26; acc: 0.81
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.84; acc: 0.84
Batch: 420; loss: 1.0; acc: 0.8
Batch: 440; loss: 0.71; acc: 0.86
Batch: 460; loss: 1.03; acc: 0.78
Batch: 480; loss: 0.35; acc: 0.95
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 1.12; acc: 0.8
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.73; acc: 0.81
Batch: 580; loss: 0.88; acc: 0.81
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.84; train_accuracy: 0.81 

Batch: 0; loss: 0.87; acc: 0.83
Batch: 20; loss: 2.38; acc: 0.64
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 1.46; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.77
Batch: 100; loss: 0.85; acc: 0.78
Batch: 120; loss: 0.88; acc: 0.86
Batch: 140; loss: 2.04; acc: 0.62
Val Epoch over. val_loss: 1.1296404155955952; val_accuracy: 0.7669187898089171 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.37; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.8; acc: 0.81
Batch: 60; loss: 1.32; acc: 0.77
Batch: 80; loss: 1.04; acc: 0.78
Batch: 100; loss: 1.44; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 0.7; acc: 0.75
Batch: 160; loss: 1.2; acc: 0.78
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.9; acc: 0.77
Batch: 240; loss: 1.48; acc: 0.8
Batch: 260; loss: 0.76; acc: 0.81
Batch: 280; loss: 0.82; acc: 0.81
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.94; acc: 0.8
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.51; acc: 0.92
Batch: 400; loss: 0.93; acc: 0.77
Batch: 420; loss: 0.98; acc: 0.84
Batch: 440; loss: 1.07; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.92; acc: 0.77
Batch: 540; loss: 0.59; acc: 0.78
Batch: 560; loss: 1.07; acc: 0.75
Batch: 580; loss: 0.84; acc: 0.83
Batch: 600; loss: 0.96; acc: 0.77
Batch: 620; loss: 0.73; acc: 0.86
Train Epoch over. train_loss: 0.84; train_accuracy: 0.81 

Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 2.4; acc: 0.66
Batch: 40; loss: 0.75; acc: 0.83
Batch: 60; loss: 1.41; acc: 0.72
Batch: 80; loss: 1.13; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.84
Batch: 140; loss: 2.05; acc: 0.62
Val Epoch over. val_loss: 1.140980522154243; val_accuracy: 0.7673168789808917 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.26; acc: 0.77
Batch: 20; loss: 1.03; acc: 0.78
Batch: 40; loss: 1.56; acc: 0.77
Batch: 60; loss: 1.58; acc: 0.78
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 1.01; acc: 0.78
Batch: 140; loss: 0.73; acc: 0.81
Batch: 160; loss: 1.13; acc: 0.78
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.91; acc: 0.84
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 1.56; acc: 0.78
Batch: 260; loss: 1.2; acc: 0.75
Batch: 280; loss: 1.24; acc: 0.77
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.89; acc: 0.77
Batch: 360; loss: 0.77; acc: 0.84
Batch: 380; loss: 1.04; acc: 0.72
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 1.07; acc: 0.77
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.86; acc: 0.78
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 1.24; acc: 0.77
Batch: 560; loss: 1.33; acc: 0.73
Batch: 580; loss: 0.67; acc: 0.78
Batch: 600; loss: 0.6; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.81
Train Epoch over. train_loss: 0.84; train_accuracy: 0.81 

Batch: 0; loss: 0.9; acc: 0.81
Batch: 20; loss: 2.31; acc: 0.66
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 1.38; acc: 0.72
Batch: 80; loss: 1.15; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.86
Batch: 140; loss: 2.14; acc: 0.62
Val Epoch over. val_loss: 1.107616161844533; val_accuracy: 0.7725915605095541 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.84; acc: 0.83
Batch: 20; loss: 1.2; acc: 0.77
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.9; acc: 0.83
Batch: 80; loss: 0.72; acc: 0.84
Batch: 100; loss: 0.83; acc: 0.84
Batch: 120; loss: 1.13; acc: 0.8
Batch: 140; loss: 0.88; acc: 0.84
Batch: 160; loss: 0.74; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.83; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.88
Batch: 240; loss: 1.08; acc: 0.8
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 1.53; acc: 0.75
Batch: 300; loss: 0.95; acc: 0.78
Batch: 320; loss: 0.82; acc: 0.77
Batch: 340; loss: 0.93; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.84
Batch: 380; loss: 0.78; acc: 0.78
Batch: 400; loss: 1.07; acc: 0.75
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.79; acc: 0.84
Batch: 460; loss: 0.94; acc: 0.84
Batch: 480; loss: 0.94; acc: 0.83
Batch: 500; loss: 1.42; acc: 0.75
Batch: 520; loss: 0.75; acc: 0.84
Batch: 540; loss: 1.26; acc: 0.75
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.97; acc: 0.83
Batch: 600; loss: 0.76; acc: 0.8
Batch: 620; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.88; train_accuracy: 0.81 

Batch: 0; loss: 1.13; acc: 0.77
Batch: 20; loss: 2.67; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.81
Batch: 60; loss: 1.51; acc: 0.69
Batch: 80; loss: 1.21; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 0.89; acc: 0.88
Batch: 140; loss: 2.44; acc: 0.59
Val Epoch over. val_loss: 1.2781483176027892; val_accuracy: 0.7527866242038217 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 1.71; acc: 0.7
Batch: 60; loss: 0.95; acc: 0.75
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 0.9; acc: 0.86
Batch: 120; loss: 1.36; acc: 0.8
Batch: 140; loss: 1.27; acc: 0.75
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 1.03; acc: 0.81
Batch: 200; loss: 0.92; acc: 0.83
Batch: 220; loss: 1.16; acc: 0.81
Batch: 240; loss: 0.95; acc: 0.77
Batch: 260; loss: 0.66; acc: 0.81
Batch: 280; loss: 1.43; acc: 0.73
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 1.1; acc: 0.81
Batch: 340; loss: 1.28; acc: 0.67
Batch: 360; loss: 1.25; acc: 0.72
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 1.08; acc: 0.83
Batch: 420; loss: 0.72; acc: 0.8
Batch: 440; loss: 0.79; acc: 0.78
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.84; acc: 0.77
Batch: 500; loss: 1.07; acc: 0.8
Batch: 520; loss: 0.83; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 1.01; acc: 0.75
Batch: 580; loss: 0.97; acc: 0.84
Batch: 600; loss: 1.74; acc: 0.73
Batch: 620; loss: 1.15; acc: 0.78
Train Epoch over. train_loss: 0.97; train_accuracy: 0.8 

Batch: 0; loss: 1.31; acc: 0.73
Batch: 20; loss: 3.02; acc: 0.64
Batch: 40; loss: 0.99; acc: 0.78
Batch: 60; loss: 1.7; acc: 0.7
Batch: 80; loss: 1.35; acc: 0.75
Batch: 100; loss: 1.01; acc: 0.77
Batch: 120; loss: 0.95; acc: 0.83
Batch: 140; loss: 2.72; acc: 0.59
Val Epoch over. val_loss: 1.4361290764656796; val_accuracy: 0.7388535031847133 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.6; acc: 0.69
Batch: 20; loss: 1.46; acc: 0.78
Batch: 40; loss: 1.5; acc: 0.73
Batch: 60; loss: 1.08; acc: 0.77
Batch: 80; loss: 0.99; acc: 0.8
Batch: 100; loss: 1.06; acc: 0.78
Batch: 120; loss: 1.8; acc: 0.75
Batch: 140; loss: 1.62; acc: 0.72
Batch: 160; loss: 2.07; acc: 0.72
Batch: 180; loss: 1.03; acc: 0.81
Batch: 200; loss: 0.97; acc: 0.81
Batch: 220; loss: 1.2; acc: 0.73
Batch: 240; loss: 1.6; acc: 0.7
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.96; acc: 0.81
Batch: 300; loss: 1.24; acc: 0.72
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 1.06; acc: 0.75
Batch: 360; loss: 0.87; acc: 0.72
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 1.29; acc: 0.72
Batch: 420; loss: 1.43; acc: 0.75
Batch: 440; loss: 1.14; acc: 0.75
Batch: 460; loss: 0.86; acc: 0.78
Batch: 480; loss: 1.37; acc: 0.81
Batch: 500; loss: 0.86; acc: 0.83
Batch: 520; loss: 2.31; acc: 0.64
Batch: 540; loss: 1.69; acc: 0.73
Batch: 560; loss: 1.39; acc: 0.78
Batch: 580; loss: 1.31; acc: 0.81
Batch: 600; loss: 1.33; acc: 0.78
Batch: 620; loss: 1.67; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.78 

Batch: 0; loss: 1.52; acc: 0.72
Batch: 20; loss: 3.44; acc: 0.62
Batch: 40; loss: 1.14; acc: 0.78
Batch: 60; loss: 1.86; acc: 0.7
Batch: 80; loss: 1.51; acc: 0.75
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.02; acc: 0.81
Batch: 140; loss: 2.92; acc: 0.59
Val Epoch over. val_loss: 1.631310116713214; val_accuracy: 0.722531847133758 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.08; acc: 0.72
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 2.12; acc: 0.75
Batch: 60; loss: 1.6; acc: 0.72
Batch: 80; loss: 1.92; acc: 0.73
Batch: 100; loss: 1.78; acc: 0.75
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 1.01; acc: 0.84
Batch: 180; loss: 1.32; acc: 0.8
Batch: 200; loss: 2.38; acc: 0.69
Batch: 220; loss: 1.63; acc: 0.77
Batch: 240; loss: 1.3; acc: 0.78
Batch: 260; loss: 0.8; acc: 0.83
Batch: 280; loss: 0.92; acc: 0.78
Batch: 300; loss: 0.99; acc: 0.75
Batch: 320; loss: 0.81; acc: 0.84
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 2.03; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.72
Batch: 400; loss: 0.8; acc: 0.84
Batch: 420; loss: 1.27; acc: 0.8
Batch: 440; loss: 1.16; acc: 0.77
Batch: 460; loss: 1.06; acc: 0.75
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 1.22; acc: 0.77
Batch: 520; loss: 1.18; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.83
Batch: 560; loss: 1.09; acc: 0.78
Batch: 580; loss: 0.81; acc: 0.83
Batch: 600; loss: 1.41; acc: 0.83
Batch: 620; loss: 1.18; acc: 0.8
Train Epoch over. train_loss: 1.21; train_accuracy: 0.77 

Batch: 0; loss: 1.73; acc: 0.75
Batch: 20; loss: 3.78; acc: 0.61
Batch: 40; loss: 1.27; acc: 0.77
Batch: 60; loss: 2.0; acc: 0.67
Batch: 80; loss: 1.64; acc: 0.75
Batch: 100; loss: 1.17; acc: 0.73
Batch: 120; loss: 1.06; acc: 0.81
Batch: 140; loss: 3.11; acc: 0.58
Val Epoch over. val_loss: 1.805634709300509; val_accuracy: 0.7131767515923567 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.71; acc: 0.66
Batch: 20; loss: 1.09; acc: 0.78
Batch: 40; loss: 1.51; acc: 0.75
Batch: 60; loss: 2.11; acc: 0.72
Batch: 80; loss: 1.2; acc: 0.8
Batch: 100; loss: 1.66; acc: 0.77
Batch: 120; loss: 1.48; acc: 0.78
Batch: 140; loss: 0.9; acc: 0.83
Batch: 160; loss: 1.57; acc: 0.73
Batch: 180; loss: 1.15; acc: 0.77
Batch: 200; loss: 0.9; acc: 0.81
Batch: 220; loss: 1.23; acc: 0.77
Batch: 240; loss: 1.66; acc: 0.72
Batch: 260; loss: 1.56; acc: 0.7
Batch: 280; loss: 1.11; acc: 0.83
Batch: 300; loss: 1.12; acc: 0.8
Batch: 320; loss: 1.2; acc: 0.75
Batch: 340; loss: 0.99; acc: 0.84
Batch: 360; loss: 0.96; acc: 0.84
Batch: 380; loss: 1.11; acc: 0.77
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 1.26; acc: 0.81
Batch: 440; loss: 1.62; acc: 0.77
Batch: 460; loss: 1.23; acc: 0.72
Batch: 480; loss: 1.27; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.81
Batch: 520; loss: 1.46; acc: 0.77
Batch: 540; loss: 0.71; acc: 0.84
Batch: 560; loss: 1.74; acc: 0.67
Batch: 580; loss: 1.01; acc: 0.77
Batch: 600; loss: 1.36; acc: 0.72
Batch: 620; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 1.35; train_accuracy: 0.76 

Batch: 0; loss: 1.93; acc: 0.73
Batch: 20; loss: 4.13; acc: 0.61
Batch: 40; loss: 1.47; acc: 0.75
Batch: 60; loss: 2.19; acc: 0.67
Batch: 80; loss: 1.73; acc: 0.75
Batch: 100; loss: 1.28; acc: 0.73
Batch: 120; loss: 1.15; acc: 0.83
Batch: 140; loss: 3.4; acc: 0.55
Val Epoch over. val_loss: 2.015916000126274; val_accuracy: 0.7019307324840764 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.38; acc: 0.69
Batch: 20; loss: 1.53; acc: 0.78
Batch: 40; loss: 2.27; acc: 0.7
Batch: 60; loss: 2.78; acc: 0.67
Batch: 80; loss: 1.55; acc: 0.77
Batch: 100; loss: 1.3; acc: 0.77
Batch: 120; loss: 1.32; acc: 0.8
Batch: 140; loss: 2.3; acc: 0.62
Batch: 160; loss: 1.33; acc: 0.75
Batch: 180; loss: 2.15; acc: 0.73
Batch: 200; loss: 1.11; acc: 0.77
Batch: 220; loss: 1.14; acc: 0.77
Batch: 240; loss: 1.31; acc: 0.75
Batch: 260; loss: 1.19; acc: 0.86
Batch: 280; loss: 1.56; acc: 0.75
Batch: 300; loss: 1.19; acc: 0.78
Batch: 320; loss: 1.41; acc: 0.77
Batch: 340; loss: 1.69; acc: 0.77
Batch: 360; loss: 1.62; acc: 0.78
Batch: 380; loss: 1.72; acc: 0.8
Batch: 400; loss: 1.87; acc: 0.73
Batch: 420; loss: 1.81; acc: 0.72
Batch: 440; loss: 1.36; acc: 0.78
Batch: 460; loss: 1.42; acc: 0.72
Batch: 480; loss: 1.61; acc: 0.7
Batch: 500; loss: 1.61; acc: 0.73
Batch: 520; loss: 2.07; acc: 0.61
Batch: 540; loss: 1.3; acc: 0.77
Batch: 560; loss: 1.53; acc: 0.72
Batch: 580; loss: 1.39; acc: 0.8
Batch: 600; loss: 1.54; acc: 0.72
Batch: 620; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 1.49; train_accuracy: 0.75 

Batch: 0; loss: 2.08; acc: 0.77
Batch: 20; loss: 4.37; acc: 0.59
Batch: 40; loss: 1.64; acc: 0.72
Batch: 60; loss: 2.42; acc: 0.67
Batch: 80; loss: 1.91; acc: 0.69
Batch: 100; loss: 1.37; acc: 0.75
Batch: 120; loss: 1.21; acc: 0.84
Batch: 140; loss: 3.76; acc: 0.53
Val Epoch over. val_loss: 2.218567332264724; val_accuracy: 0.6937699044585988 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.55; acc: 0.75
Batch: 20; loss: 1.56; acc: 0.78
Batch: 40; loss: 1.48; acc: 0.77
Batch: 60; loss: 1.83; acc: 0.69
Batch: 80; loss: 1.57; acc: 0.8
Batch: 100; loss: 1.26; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.78
Batch: 140; loss: 1.32; acc: 0.77
Batch: 160; loss: 1.07; acc: 0.77
Batch: 180; loss: 2.05; acc: 0.75
Batch: 200; loss: 1.03; acc: 0.78
Batch: 220; loss: 1.33; acc: 0.73
Batch: 240; loss: 1.08; acc: 0.8
Batch: 260; loss: 1.4; acc: 0.77
Batch: 280; loss: 1.64; acc: 0.7
Batch: 300; loss: 1.8; acc: 0.77
Batch: 320; loss: 1.79; acc: 0.77
Batch: 340; loss: 1.41; acc: 0.73
Batch: 360; loss: 1.61; acc: 0.8
Batch: 380; loss: 1.1; acc: 0.78
Batch: 400; loss: 1.07; acc: 0.78
Batch: 420; loss: 1.81; acc: 0.72
Batch: 440; loss: 1.49; acc: 0.81
Batch: 460; loss: 1.52; acc: 0.77
Batch: 480; loss: 2.11; acc: 0.72
Batch: 500; loss: 1.92; acc: 0.75
Batch: 520; loss: 2.4; acc: 0.7
Batch: 540; loss: 1.19; acc: 0.8
Batch: 560; loss: 1.3; acc: 0.77
Batch: 580; loss: 1.06; acc: 0.8
Batch: 600; loss: 1.3; acc: 0.77
Batch: 620; loss: 1.33; acc: 0.83
Train Epoch over. train_loss: 1.65; train_accuracy: 0.75 

Batch: 0; loss: 2.23; acc: 0.78
Batch: 20; loss: 4.66; acc: 0.58
Batch: 40; loss: 1.86; acc: 0.7
Batch: 60; loss: 2.63; acc: 0.61
Batch: 80; loss: 2.03; acc: 0.7
Batch: 100; loss: 1.52; acc: 0.67
Batch: 120; loss: 1.34; acc: 0.83
Batch: 140; loss: 4.18; acc: 0.55
Val Epoch over. val_loss: 2.455569972278206; val_accuracy: 0.6838176751592356 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.33; acc: 0.67
Batch: 20; loss: 1.72; acc: 0.69
Batch: 40; loss: 2.79; acc: 0.62
Batch: 60; loss: 1.72; acc: 0.75
Batch: 80; loss: 1.7; acc: 0.73
Batch: 100; loss: 2.6; acc: 0.7
Batch: 120; loss: 1.17; acc: 0.77
Batch: 140; loss: 2.93; acc: 0.61
Batch: 160; loss: 1.57; acc: 0.75
Batch: 180; loss: 0.97; acc: 0.81
Batch: 200; loss: 1.1; acc: 0.8
Batch: 220; loss: 2.57; acc: 0.64
Batch: 240; loss: 0.98; acc: 0.83
Batch: 260; loss: 1.71; acc: 0.72
Batch: 280; loss: 1.77; acc: 0.72
Batch: 300; loss: 1.61; acc: 0.78
Batch: 320; loss: 1.88; acc: 0.7
Batch: 340; loss: 1.52; acc: 0.78
Batch: 360; loss: 1.42; acc: 0.78
Batch: 380; loss: 2.87; acc: 0.61
Batch: 400; loss: 1.38; acc: 0.81
Batch: 420; loss: 1.76; acc: 0.73
Batch: 440; loss: 1.42; acc: 0.77
Batch: 460; loss: 1.83; acc: 0.73
Batch: 480; loss: 2.18; acc: 0.75
Batch: 500; loss: 2.12; acc: 0.75
Batch: 520; loss: 1.57; acc: 0.77
Batch: 540; loss: 1.24; acc: 0.81
Batch: 560; loss: 1.44; acc: 0.81
Batch: 580; loss: 1.52; acc: 0.77
Batch: 600; loss: 1.27; acc: 0.77
Batch: 620; loss: 1.88; acc: 0.69
Train Epoch over. train_loss: 1.82; train_accuracy: 0.74 

Batch: 0; loss: 2.36; acc: 0.77
Batch: 20; loss: 5.0; acc: 0.55
Batch: 40; loss: 2.07; acc: 0.69
Batch: 60; loss: 3.15; acc: 0.59
Batch: 80; loss: 2.2; acc: 0.7
Batch: 100; loss: 1.76; acc: 0.66
Batch: 120; loss: 1.5; acc: 0.81
Batch: 140; loss: 4.57; acc: 0.56
Val Epoch over. val_loss: 2.728388239243987; val_accuracy: 0.6734673566878981 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.7; acc: 0.66
Batch: 20; loss: 2.99; acc: 0.7
Batch: 40; loss: 3.27; acc: 0.59
Batch: 60; loss: 3.07; acc: 0.69
Batch: 80; loss: 1.17; acc: 0.78
Batch: 100; loss: 1.93; acc: 0.7
Batch: 120; loss: 2.39; acc: 0.75
Batch: 140; loss: 1.35; acc: 0.78
Batch: 160; loss: 1.78; acc: 0.75
Batch: 180; loss: 1.59; acc: 0.78
Batch: 200; loss: 2.38; acc: 0.75
Batch: 220; loss: 2.43; acc: 0.67
Batch: 240; loss: 2.93; acc: 0.7
Batch: 260; loss: 2.85; acc: 0.7
Batch: 280; loss: 2.27; acc: 0.72
Batch: 300; loss: 2.44; acc: 0.73
Batch: 320; loss: 1.32; acc: 0.81
Batch: 340; loss: 1.09; acc: 0.84
Batch: 360; loss: 1.64; acc: 0.77
Batch: 380; loss: 1.09; acc: 0.78
Batch: 400; loss: 1.48; acc: 0.78
Batch: 420; loss: 2.22; acc: 0.7
Batch: 440; loss: 2.21; acc: 0.7
Batch: 460; loss: 2.12; acc: 0.77
Batch: 480; loss: 1.55; acc: 0.72
Batch: 500; loss: 1.34; acc: 0.78
Batch: 520; loss: 1.46; acc: 0.8
Batch: 540; loss: 2.18; acc: 0.66
Batch: 560; loss: 1.86; acc: 0.8
Batch: 580; loss: 1.89; acc: 0.75
Batch: 600; loss: 1.01; acc: 0.8
Batch: 620; loss: 1.84; acc: 0.66
Train Epoch over. train_loss: 2.0; train_accuracy: 0.73 

Batch: 0; loss: 2.39; acc: 0.75
Batch: 20; loss: 5.33; acc: 0.55
Batch: 40; loss: 2.19; acc: 0.7
Batch: 60; loss: 3.6; acc: 0.62
Batch: 80; loss: 2.28; acc: 0.72
Batch: 100; loss: 2.03; acc: 0.64
Batch: 120; loss: 1.66; acc: 0.83
Batch: 140; loss: 4.85; acc: 0.55
Val Epoch over. val_loss: 2.9953444254626134; val_accuracy: 0.6670979299363057 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.14; acc: 0.64
Batch: 20; loss: 3.31; acc: 0.7
Batch: 40; loss: 2.27; acc: 0.64
Batch: 60; loss: 2.81; acc: 0.72
Batch: 80; loss: 2.41; acc: 0.66
Batch: 100; loss: 2.54; acc: 0.69
Batch: 120; loss: 1.79; acc: 0.78
Batch: 140; loss: 2.28; acc: 0.7
Batch: 160; loss: 2.2; acc: 0.7
Batch: 180; loss: 1.38; acc: 0.83
Batch: 200; loss: 1.86; acc: 0.75
Batch: 220; loss: 2.57; acc: 0.73
Batch: 240; loss: 1.67; acc: 0.78
Batch: 260; loss: 1.49; acc: 0.75
Batch: 280; loss: 2.25; acc: 0.66
Batch: 300; loss: 3.05; acc: 0.73
Batch: 320; loss: 1.95; acc: 0.7
Batch: 340; loss: 1.91; acc: 0.75
Batch: 360; loss: 3.13; acc: 0.69
Batch: 380; loss: 1.78; acc: 0.75
Batch: 400; loss: 1.82; acc: 0.7
Batch: 420; loss: 1.98; acc: 0.73
Batch: 440; loss: 1.74; acc: 0.7
Batch: 460; loss: 1.97; acc: 0.78
Batch: 480; loss: 1.1; acc: 0.75
Batch: 500; loss: 1.53; acc: 0.77
Batch: 520; loss: 2.36; acc: 0.69
Batch: 540; loss: 1.98; acc: 0.77
Batch: 560; loss: 1.73; acc: 0.72
Batch: 580; loss: 1.19; acc: 0.78
Batch: 600; loss: 1.4; acc: 0.73
Batch: 620; loss: 2.02; acc: 0.8
Train Epoch over. train_loss: 2.18; train_accuracy: 0.72 

Batch: 0; loss: 2.41; acc: 0.77
Batch: 20; loss: 5.6; acc: 0.53
Batch: 40; loss: 2.21; acc: 0.72
Batch: 60; loss: 4.0; acc: 0.61
Batch: 80; loss: 2.51; acc: 0.69
Batch: 100; loss: 2.26; acc: 0.64
Batch: 120; loss: 1.85; acc: 0.81
Batch: 140; loss: 5.12; acc: 0.53
Val Epoch over. val_loss: 3.2737114987555582; val_accuracy: 0.6591361464968153 

plots/subspace_training/lenet/2020-01-10 14:00:56/d_dim_200_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 63998
elements in E: 13327800
fraction nonzero: 0.00480184276474737
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 1.95; acc: 0.36
Batch: 40; loss: 1.44; acc: 0.58
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 1.03; acc: 0.61
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 1.09; acc: 0.59
Batch: 160; loss: 1.64; acc: 0.66
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.87; acc: 0.77
Batch: 240; loss: 0.86; acc: 0.75
Batch: 260; loss: 1.24; acc: 0.66
Batch: 280; loss: 0.63; acc: 0.72
Batch: 300; loss: 1.09; acc: 0.73
Batch: 320; loss: 0.92; acc: 0.8
Batch: 340; loss: 0.62; acc: 0.77
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.96; acc: 0.66
Batch: 400; loss: 0.63; acc: 0.78
Batch: 420; loss: 0.76; acc: 0.72
Batch: 440; loss: 0.86; acc: 0.78
Batch: 460; loss: 1.01; acc: 0.72
Batch: 480; loss: 0.9; acc: 0.81
Batch: 500; loss: 1.07; acc: 0.75
Batch: 520; loss: 1.55; acc: 0.62
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.83
Batch: 580; loss: 1.01; acc: 0.77
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 1.05; train_accuracy: 0.72 

Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 1.19; acc: 0.72
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.77
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 1.23; acc: 0.7
Val Epoch over. val_loss: 0.846426616428764; val_accuracy: 0.7658240445859873 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.73
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.87; acc: 0.84
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.71; acc: 0.75
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 1.14; acc: 0.66
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.7; acc: 0.83
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 0.78; acc: 0.8
Batch: 340; loss: 0.66; acc: 0.77
Batch: 360; loss: 0.79; acc: 0.75
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.96; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.81
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 1.09; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.66; acc: 0.77
Train Epoch over. train_loss: 0.72; train_accuracy: 0.8 

Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 1.03; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 1.62; acc: 0.59
Val Epoch over. val_loss: 0.9937434718487369; val_accuracy: 0.7372611464968153 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.77
Batch: 80; loss: 0.72; acc: 0.77
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.8
Batch: 220; loss: 1.38; acc: 0.73
Batch: 240; loss: 1.01; acc: 0.75
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.81
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.81; acc: 0.8
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 1.06; acc: 0.7
Batch: 420; loss: 0.84; acc: 0.77
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.74; acc: 0.78
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.67; acc: 0.84
Batch: 580; loss: 0.87; acc: 0.75
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.65; acc: 0.83
Train Epoch over. train_loss: 0.69; train_accuracy: 0.81 

Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 1.34; acc: 0.66
Batch: 40; loss: 0.78; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.65; acc: 0.77
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 1.32; acc: 0.62
Val Epoch over. val_loss: 0.9710984829884426; val_accuracy: 0.7527866242038217 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 1.14; acc: 0.73
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.91; acc: 0.88
Batch: 80; loss: 1.65; acc: 0.64
Batch: 100; loss: 0.53; acc: 0.77
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.81
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.75; acc: 0.84
Batch: 200; loss: 0.62; acc: 0.8
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.81
Batch: 300; loss: 0.76; acc: 0.77
Batch: 320; loss: 0.73; acc: 0.77
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.83
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.74; acc: 0.8
Batch: 440; loss: 1.04; acc: 0.75
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 1.06; acc: 0.77
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.82 

Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.89; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.72
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 1.19; acc: 0.67
Val Epoch over. val_loss: 0.6934661819676685; val_accuracy: 0.808718152866242 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.93; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.83
Batch: 140; loss: 1.11; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.78
Batch: 180; loss: 0.66; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 1.17; acc: 0.75
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.93; acc: 0.73
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.64; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.64; acc: 0.77
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 1.23; acc: 0.7
Batch: 520; loss: 0.79; acc: 0.78
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 1.1; acc: 0.75
Batch: 580; loss: 0.28; acc: 0.86
Batch: 600; loss: 0.82; acc: 0.81
Batch: 620; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 1.08; acc: 0.72
Val Epoch over. val_loss: 0.6855699420924399; val_accuracy: 0.8112062101910829 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.81
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.66; acc: 0.88
Batch: 160; loss: 1.28; acc: 0.75
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.96; acc: 0.77
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 1.16; acc: 0.75
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.83
Batch: 340; loss: 0.8; acc: 0.8
Batch: 360; loss: 0.84; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.8
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.78
Batch: 580; loss: 0.95; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.64; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 1.27; acc: 0.7
Val Epoch over. val_loss: 0.7009653498412697; val_accuracy: 0.81359474522293 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.99; acc: 0.77
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.8
Batch: 160; loss: 1.09; acc: 0.75
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.81; acc: 0.8
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.7; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 1.07; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.8
Batch: 440; loss: 0.37; acc: 0.84
Batch: 460; loss: 1.03; acc: 0.77
Batch: 480; loss: 0.71; acc: 0.77
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.82; acc: 0.81
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.6; acc: 0.72
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.98; acc: 0.8
Train Epoch over. train_loss: 0.63; train_accuracy: 0.83 

Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 0.84; acc: 0.84
Batch: 140; loss: 1.38; acc: 0.66
Val Epoch over. val_loss: 0.8445001995297754; val_accuracy: 0.7834394904458599 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 1.01; acc: 0.78
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.82; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 1.21; acc: 0.72
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.67; acc: 0.8
Batch: 380; loss: 0.79; acc: 0.81
Batch: 400; loss: 0.92; acc: 0.84
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.73; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.78; acc: 0.8
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.9; acc: 0.8
Train Epoch over. train_loss: 0.62; train_accuracy: 0.83 

Batch: 0; loss: 0.58; acc: 0.75
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 1.04; acc: 0.69
Val Epoch over. val_loss: 0.7346591183524223; val_accuracy: 0.7950835987261147 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 1.01; acc: 0.77
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.97; acc: 0.72
Batch: 260; loss: 0.65; acc: 0.75
Batch: 280; loss: 1.31; acc: 0.69
Batch: 300; loss: 0.66; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 1.24; acc: 0.69
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 1.03; acc: 0.75
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 1.01; acc: 0.8
Batch: 580; loss: 0.97; acc: 0.8
Batch: 600; loss: 0.75; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.63; train_accuracy: 0.83 

Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 1.21; acc: 0.62
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 0.84; acc: 0.83
Batch: 140; loss: 1.42; acc: 0.66
Val Epoch over. val_loss: 0.8751541349538572; val_accuracy: 0.7836385350318471 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.94; acc: 0.8
Batch: 20; loss: 1.06; acc: 0.7
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.77
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 1.12; acc: 0.75
Batch: 160; loss: 1.02; acc: 0.78
Batch: 180; loss: 0.78; acc: 0.83
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.72; acc: 0.78
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.7; acc: 0.8
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.77
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 1.06; acc: 0.75
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.83 

Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 1.33; acc: 0.66
Batch: 40; loss: 0.7; acc: 0.88
Batch: 60; loss: 0.98; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.65; acc: 0.75
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 1.4; acc: 0.66
Val Epoch over. val_loss: 0.877164909414425; val_accuracy: 0.7654259554140127 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.86; acc: 0.78
Batch: 20; loss: 0.89; acc: 0.8
Batch: 40; loss: 0.9; acc: 0.81
Batch: 60; loss: 0.5; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 1.01; acc: 0.78
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.9; acc: 0.83
Batch: 180; loss: 0.33; acc: 0.84
Batch: 200; loss: 0.73; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.86
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.86
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.83; acc: 0.77
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 1.06; acc: 0.72
Val Epoch over. val_loss: 0.6259408017062837; val_accuracy: 0.8301154458598726 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.91
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.75
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 1.23; acc: 0.7
Val Epoch over. val_loss: 0.6621342006192845; val_accuracy: 0.8173765923566879 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.49; acc: 0.91
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.77
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.67; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.81
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.81; acc: 0.83
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.84; acc: 0.81
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.7; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 1.2; acc: 0.69
Val Epoch over. val_loss: 0.6712883719403273; val_accuracy: 0.8196656050955414 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.39; acc: 0.81
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.85; acc: 0.84
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.81; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.59; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.76; acc: 0.84
Batch: 140; loss: 1.27; acc: 0.7
Val Epoch over. val_loss: 0.7146714570795655; val_accuracy: 0.8107085987261147 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.83
Batch: 100; loss: 0.87; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.89
Batch: 160; loss: 0.92; acc: 0.8
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.83; acc: 0.77
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.72; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.81; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 1.24; acc: 0.69
Batch: 40; loss: 0.62; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.81; acc: 0.83
Batch: 140; loss: 1.36; acc: 0.72
Val Epoch over. val_loss: 0.7532906526592886; val_accuracy: 0.8070262738853503 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.92; acc: 0.75
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 1.11; acc: 0.8
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 1.08; acc: 0.77
Batch: 400; loss: 0.35; acc: 0.95
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.78
Batch: 540; loss: 0.22; acc: 0.88
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.88; acc: 0.8
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 1.13; acc: 0.73
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 1.38; acc: 0.69
Val Epoch over. val_loss: 0.7693355588396643; val_accuracy: 0.8053343949044586 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.78; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.75; acc: 0.75
Batch: 280; loss: 0.63; acc: 0.83
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 1.14; acc: 0.8
Batch: 380; loss: 0.45; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.88
Batch: 560; loss: 1.06; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 1.12; acc: 0.7
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 1.36; acc: 0.66
Val Epoch over. val_loss: 0.7761969321472629; val_accuracy: 0.8058320063694268 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.7; acc: 0.84
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.84; acc: 0.81
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.83; acc: 0.8
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.92
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 0.57; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.83
Batch: 140; loss: 1.32; acc: 0.69
Val Epoch over. val_loss: 0.7523530190158042; val_accuracy: 0.8120023885350318 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.82; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.74; acc: 0.86
Batch: 160; loss: 1.28; acc: 0.7
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.83; acc: 0.8
Batch: 240; loss: 0.84; acc: 0.81
Batch: 260; loss: 0.93; acc: 0.8
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.86; acc: 0.84
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.81
Batch: 460; loss: 0.83; acc: 0.83
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.66; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 1.13; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 1.34; acc: 0.7
Val Epoch over. val_loss: 0.7657884060387399; val_accuracy: 0.8112062101910829 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.78; acc: 0.84
Batch: 60; loss: 0.93; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.65; acc: 0.89
Batch: 260; loss: 0.94; acc: 0.73
Batch: 280; loss: 0.84; acc: 0.8
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.25; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 1.21; acc: 0.73
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.71; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.63; acc: 0.86
Batch: 620; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.86 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.78
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 1.43; acc: 0.7
Val Epoch over. val_loss: 0.79568697521641; val_accuracy: 0.8046377388535032 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.92; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.99; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.8
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.87; acc: 0.83
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.69; acc: 0.84
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.77; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.81; acc: 0.83
Batch: 300; loss: 0.78; acc: 0.78
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.85; acc: 0.8
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.88
Batch: 480; loss: 0.99; acc: 0.83
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.75; acc: 0.77
Batch: 560; loss: 0.6; acc: 0.91
Batch: 580; loss: 0.94; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.6; train_accuracy: 0.85 

Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 1.44; acc: 0.66
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 0.94; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.78
Batch: 140; loss: 1.75; acc: 0.64
Val Epoch over. val_loss: 1.012226066391939; val_accuracy: 0.7713972929936306 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.05; acc: 0.8
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 1.16; acc: 0.77
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 1.14; acc: 0.78
Batch: 100; loss: 0.62; acc: 0.75
Batch: 120; loss: 1.29; acc: 0.75
Batch: 140; loss: 0.98; acc: 0.78
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.93; acc: 0.84
Batch: 220; loss: 1.67; acc: 0.72
Batch: 240; loss: 0.79; acc: 0.72
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.77; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.86
Batch: 360; loss: 0.76; acc: 0.78
Batch: 380; loss: 0.98; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.43; acc: 0.83
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 1.02; acc: 0.75
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.73; acc: 0.89
Batch: 560; loss: 0.87; acc: 0.8
Batch: 580; loss: 1.02; acc: 0.83
Batch: 600; loss: 1.23; acc: 0.77
Batch: 620; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.75; train_accuracy: 0.82 

Batch: 0; loss: 0.99; acc: 0.77
Batch: 20; loss: 2.08; acc: 0.58
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.85; acc: 0.8
Batch: 80; loss: 1.19; acc: 0.72
Batch: 100; loss: 1.43; acc: 0.66
Batch: 120; loss: 1.45; acc: 0.73
Batch: 140; loss: 2.17; acc: 0.59
Val Epoch over. val_loss: 1.2958177339499164; val_accuracy: 0.7372611464968153 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.3; acc: 0.64
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 1.15; acc: 0.81
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 1.24; acc: 0.72
Batch: 100; loss: 1.4; acc: 0.72
Batch: 120; loss: 1.14; acc: 0.75
Batch: 140; loss: 1.42; acc: 0.73
Batch: 160; loss: 1.18; acc: 0.77
Batch: 180; loss: 1.46; acc: 0.75
Batch: 200; loss: 1.06; acc: 0.77
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.87; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.61; acc: 0.75
Batch: 320; loss: 0.67; acc: 0.77
Batch: 340; loss: 0.68; acc: 0.81
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 1.28; acc: 0.73
Batch: 400; loss: 0.87; acc: 0.81
Batch: 420; loss: 0.95; acc: 0.86
Batch: 440; loss: 1.01; acc: 0.77
Batch: 460; loss: 0.72; acc: 0.81
Batch: 480; loss: 0.89; acc: 0.83
Batch: 500; loss: 0.97; acc: 0.81
Batch: 520; loss: 1.44; acc: 0.78
Batch: 540; loss: 1.21; acc: 0.75
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.95; acc: 0.86
Batch: 600; loss: 1.16; acc: 0.73
Batch: 620; loss: 0.99; acc: 0.78
Train Epoch over. train_loss: 0.94; train_accuracy: 0.8 

Batch: 0; loss: 1.42; acc: 0.73
Batch: 20; loss: 2.79; acc: 0.56
Batch: 40; loss: 0.82; acc: 0.84
Batch: 60; loss: 1.02; acc: 0.77
Batch: 80; loss: 1.5; acc: 0.69
Batch: 100; loss: 1.89; acc: 0.62
Batch: 120; loss: 1.7; acc: 0.72
Batch: 140; loss: 2.67; acc: 0.53
Val Epoch over. val_loss: 1.6176645888644419; val_accuracy: 0.70203025477707 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.01; acc: 0.61
Batch: 20; loss: 1.45; acc: 0.73
Batch: 40; loss: 1.7; acc: 0.75
Batch: 60; loss: 1.59; acc: 0.72
Batch: 80; loss: 1.53; acc: 0.69
Batch: 100; loss: 2.71; acc: 0.56
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.96; acc: 0.77
Batch: 200; loss: 1.12; acc: 0.81
Batch: 220; loss: 1.06; acc: 0.78
Batch: 240; loss: 0.83; acc: 0.73
Batch: 260; loss: 1.24; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 1.08; acc: 0.81
Batch: 340; loss: 1.04; acc: 0.78
Batch: 360; loss: 1.64; acc: 0.72
Batch: 380; loss: 0.79; acc: 0.86
Batch: 400; loss: 1.56; acc: 0.8
Batch: 420; loss: 1.11; acc: 0.84
Batch: 440; loss: 0.97; acc: 0.77
Batch: 460; loss: 1.25; acc: 0.75
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 1.47; acc: 0.78
Batch: 520; loss: 0.87; acc: 0.75
Batch: 540; loss: 1.46; acc: 0.78
Batch: 560; loss: 1.68; acc: 0.69
Batch: 580; loss: 1.02; acc: 0.83
Batch: 600; loss: 1.14; acc: 0.75
Batch: 620; loss: 1.57; acc: 0.73
Train Epoch over. train_loss: 1.15; train_accuracy: 0.77 

Batch: 0; loss: 1.87; acc: 0.67
Batch: 20; loss: 3.46; acc: 0.55
Batch: 40; loss: 0.95; acc: 0.81
Batch: 60; loss: 1.24; acc: 0.77
Batch: 80; loss: 1.81; acc: 0.7
Batch: 100; loss: 2.27; acc: 0.62
Batch: 120; loss: 1.96; acc: 0.7
Batch: 140; loss: 3.31; acc: 0.55
Val Epoch over. val_loss: 1.967251005825723; val_accuracy: 0.676453025477707 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.98; acc: 0.69
Batch: 20; loss: 0.98; acc: 0.78
Batch: 40; loss: 2.37; acc: 0.64
Batch: 60; loss: 2.05; acc: 0.66
Batch: 80; loss: 0.85; acc: 0.75
Batch: 100; loss: 2.33; acc: 0.69
Batch: 120; loss: 1.07; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 1.12; acc: 0.83
Batch: 180; loss: 0.94; acc: 0.78
Batch: 200; loss: 1.46; acc: 0.73
Batch: 220; loss: 1.27; acc: 0.77
Batch: 240; loss: 1.27; acc: 0.72
Batch: 260; loss: 1.32; acc: 0.77
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 1.25; acc: 0.77
Batch: 320; loss: 1.06; acc: 0.72
Batch: 340; loss: 0.86; acc: 0.81
Batch: 360; loss: 1.23; acc: 0.83
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.88; acc: 0.81
Batch: 420; loss: 1.84; acc: 0.7
Batch: 440; loss: 1.79; acc: 0.72
Batch: 460; loss: 1.31; acc: 0.75
Batch: 480; loss: 1.17; acc: 0.73
Batch: 500; loss: 1.91; acc: 0.73
Batch: 520; loss: 1.74; acc: 0.78
Batch: 540; loss: 1.0; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.78
Batch: 580; loss: 1.59; acc: 0.77
Batch: 600; loss: 2.03; acc: 0.62
Batch: 620; loss: 1.53; acc: 0.73
Train Epoch over. train_loss: 1.4; train_accuracy: 0.75 

Batch: 0; loss: 2.38; acc: 0.67
Batch: 20; loss: 4.13; acc: 0.55
Batch: 40; loss: 1.2; acc: 0.8
Batch: 60; loss: 1.64; acc: 0.73
Batch: 80; loss: 2.06; acc: 0.7
Batch: 100; loss: 2.69; acc: 0.66
Batch: 120; loss: 2.24; acc: 0.69
Batch: 140; loss: 4.15; acc: 0.48
Val Epoch over. val_loss: 2.3967232901579254; val_accuracy: 0.6474920382165605 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.15; acc: 0.64
Batch: 20; loss: 1.75; acc: 0.69
Batch: 40; loss: 2.55; acc: 0.7
Batch: 60; loss: 2.11; acc: 0.66
Batch: 80; loss: 2.08; acc: 0.66
Batch: 100; loss: 2.47; acc: 0.7
Batch: 120; loss: 2.11; acc: 0.62
Batch: 140; loss: 1.93; acc: 0.72
Batch: 160; loss: 2.13; acc: 0.72
Batch: 180; loss: 1.71; acc: 0.75
Batch: 200; loss: 1.7; acc: 0.75
Batch: 220; loss: 1.03; acc: 0.78
Batch: 240; loss: 1.36; acc: 0.8
Batch: 260; loss: 1.78; acc: 0.77
Batch: 280; loss: 1.7; acc: 0.69
Batch: 300; loss: 2.38; acc: 0.69
Batch: 320; loss: 1.73; acc: 0.75
Batch: 340; loss: 1.13; acc: 0.78
Batch: 360; loss: 1.65; acc: 0.75
Batch: 380; loss: 1.86; acc: 0.8
Batch: 400; loss: 1.63; acc: 0.77
Batch: 420; loss: 2.42; acc: 0.66
Batch: 440; loss: 1.27; acc: 0.75
Batch: 460; loss: 1.51; acc: 0.67
Batch: 480; loss: 1.37; acc: 0.78
Batch: 500; loss: 1.54; acc: 0.75
Batch: 520; loss: 1.06; acc: 0.8
Batch: 540; loss: 1.52; acc: 0.75
Batch: 560; loss: 2.38; acc: 0.62
Batch: 580; loss: 1.52; acc: 0.73
Batch: 600; loss: 1.03; acc: 0.75
Batch: 620; loss: 1.11; acc: 0.81
Train Epoch over. train_loss: 1.68; train_accuracy: 0.72 

Batch: 0; loss: 2.76; acc: 0.66
Batch: 20; loss: 4.82; acc: 0.5
Batch: 40; loss: 1.48; acc: 0.8
Batch: 60; loss: 2.17; acc: 0.67
Batch: 80; loss: 2.22; acc: 0.69
Batch: 100; loss: 3.17; acc: 0.58
Batch: 120; loss: 2.51; acc: 0.64
Batch: 140; loss: 5.05; acc: 0.45
Val Epoch over. val_loss: 2.8639652151970347; val_accuracy: 0.621218152866242 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.1; acc: 0.58
Batch: 20; loss: 2.89; acc: 0.62
Batch: 40; loss: 1.51; acc: 0.73
Batch: 60; loss: 2.47; acc: 0.66
Batch: 80; loss: 1.31; acc: 0.72
Batch: 100; loss: 3.19; acc: 0.64
Batch: 120; loss: 1.93; acc: 0.67
Batch: 140; loss: 1.96; acc: 0.69
Batch: 160; loss: 1.95; acc: 0.69
Batch: 180; loss: 2.41; acc: 0.67
Batch: 200; loss: 1.7; acc: 0.77
Batch: 220; loss: 2.07; acc: 0.73
Batch: 240; loss: 1.68; acc: 0.67
Batch: 260; loss: 1.5; acc: 0.81
Batch: 280; loss: 2.61; acc: 0.66
Batch: 300; loss: 2.29; acc: 0.7
Batch: 320; loss: 2.05; acc: 0.75
Batch: 340; loss: 1.6; acc: 0.73
Batch: 360; loss: 1.96; acc: 0.75
Batch: 380; loss: 1.35; acc: 0.75
Batch: 400; loss: 2.39; acc: 0.64
Batch: 420; loss: 1.51; acc: 0.77
Batch: 440; loss: 1.96; acc: 0.7
Batch: 460; loss: 1.3; acc: 0.78
Batch: 480; loss: 2.66; acc: 0.7
Batch: 500; loss: 1.47; acc: 0.73
Batch: 520; loss: 2.09; acc: 0.77
Batch: 540; loss: 0.95; acc: 0.8
Batch: 560; loss: 1.39; acc: 0.78
Batch: 580; loss: 2.02; acc: 0.72
Batch: 600; loss: 2.22; acc: 0.69
Batch: 620; loss: 1.04; acc: 0.83
Train Epoch over. train_loss: 1.98; train_accuracy: 0.71 

Batch: 0; loss: 3.01; acc: 0.62
Batch: 20; loss: 5.38; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.75
Batch: 60; loss: 2.62; acc: 0.61
Batch: 80; loss: 2.42; acc: 0.72
Batch: 100; loss: 3.74; acc: 0.56
Batch: 120; loss: 2.88; acc: 0.66
Batch: 140; loss: 5.77; acc: 0.45
Val Epoch over. val_loss: 3.3419043926676366; val_accuracy: 0.6042993630573248 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.01; acc: 0.56
Batch: 20; loss: 2.72; acc: 0.66
Batch: 40; loss: 2.73; acc: 0.69
Batch: 60; loss: 1.83; acc: 0.67
Batch: 80; loss: 2.31; acc: 0.67
Batch: 100; loss: 2.78; acc: 0.61
Batch: 120; loss: 2.21; acc: 0.66
Batch: 140; loss: 2.4; acc: 0.69
Batch: 160; loss: 2.38; acc: 0.67
Batch: 180; loss: 2.25; acc: 0.66
Batch: 200; loss: 1.66; acc: 0.72
Batch: 220; loss: 1.43; acc: 0.73
Batch: 240; loss: 1.35; acc: 0.72
Batch: 260; loss: 3.84; acc: 0.64
Batch: 280; loss: 2.35; acc: 0.64
Batch: 300; loss: 2.87; acc: 0.75
Batch: 320; loss: 2.73; acc: 0.66
Batch: 340; loss: 2.05; acc: 0.72
Batch: 360; loss: 2.32; acc: 0.67
Batch: 380; loss: 2.76; acc: 0.67
Batch: 400; loss: 2.71; acc: 0.73
Batch: 420; loss: 2.18; acc: 0.64
Batch: 440; loss: 1.94; acc: 0.78
Batch: 460; loss: 2.23; acc: 0.77
Batch: 480; loss: 1.83; acc: 0.72
Batch: 500; loss: 2.73; acc: 0.67
Batch: 520; loss: 2.16; acc: 0.67
Batch: 540; loss: 1.53; acc: 0.75
Batch: 560; loss: 1.6; acc: 0.67
Batch: 580; loss: 2.74; acc: 0.78
Batch: 600; loss: 1.54; acc: 0.78
Batch: 620; loss: 2.31; acc: 0.77
Train Epoch over. train_loss: 2.33; train_accuracy: 0.69 

Batch: 0; loss: 3.26; acc: 0.59
Batch: 20; loss: 6.09; acc: 0.47
Batch: 40; loss: 2.03; acc: 0.78
Batch: 60; loss: 3.07; acc: 0.59
Batch: 80; loss: 2.9; acc: 0.59
Batch: 100; loss: 4.3; acc: 0.58
Batch: 120; loss: 3.33; acc: 0.66
Batch: 140; loss: 6.39; acc: 0.44
Val Epoch over. val_loss: 3.926339874601668; val_accuracy: 0.585390127388535 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.76; acc: 0.61
Batch: 20; loss: 3.94; acc: 0.52
Batch: 40; loss: 4.79; acc: 0.59
Batch: 60; loss: 4.09; acc: 0.61
Batch: 80; loss: 2.77; acc: 0.62
Batch: 100; loss: 3.44; acc: 0.66
Batch: 120; loss: 3.0; acc: 0.75
Batch: 140; loss: 2.11; acc: 0.67
Batch: 160; loss: 2.39; acc: 0.62
Batch: 180; loss: 2.41; acc: 0.72
Batch: 200; loss: 2.65; acc: 0.67
Batch: 220; loss: 3.7; acc: 0.67
Batch: 240; loss: 2.67; acc: 0.67
Batch: 260; loss: 2.94; acc: 0.64
Batch: 280; loss: 2.76; acc: 0.61
Batch: 300; loss: 2.29; acc: 0.7
Batch: 320; loss: 2.63; acc: 0.7
Batch: 340; loss: 2.54; acc: 0.73
Batch: 360; loss: 2.9; acc: 0.67
Batch: 380; loss: 2.19; acc: 0.73
Batch: 400; loss: 2.45; acc: 0.72
Batch: 420; loss: 2.72; acc: 0.69
Batch: 440; loss: 2.68; acc: 0.61
Batch: 460; loss: 2.8; acc: 0.72
Batch: 480; loss: 1.93; acc: 0.78
Batch: 500; loss: 2.16; acc: 0.72
Batch: 520; loss: 2.42; acc: 0.69
Batch: 540; loss: 2.35; acc: 0.67
Batch: 560; loss: 1.64; acc: 0.8
Batch: 580; loss: 2.2; acc: 0.72
Batch: 600; loss: 1.92; acc: 0.73
Batch: 620; loss: 2.56; acc: 0.64
Train Epoch over. train_loss: 2.7; train_accuracy: 0.67 

Batch: 0; loss: 3.8; acc: 0.61
Batch: 20; loss: 6.74; acc: 0.47
Batch: 40; loss: 2.37; acc: 0.72
Batch: 60; loss: 3.59; acc: 0.59
Batch: 80; loss: 3.72; acc: 0.56
Batch: 100; loss: 4.9; acc: 0.53
Batch: 120; loss: 3.68; acc: 0.67
Batch: 140; loss: 6.99; acc: 0.5
Val Epoch over. val_loss: 4.611320778063148; val_accuracy: 0.5672770700636943 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.35; acc: 0.58
Batch: 20; loss: 4.05; acc: 0.58
Batch: 40; loss: 4.25; acc: 0.58
Batch: 60; loss: 2.63; acc: 0.62
Batch: 80; loss: 5.02; acc: 0.61
Batch: 100; loss: 3.36; acc: 0.61
Batch: 120; loss: 2.65; acc: 0.67
Batch: 140; loss: 3.15; acc: 0.66
Batch: 160; loss: 1.68; acc: 0.78
Batch: 180; loss: 3.79; acc: 0.61
Batch: 200; loss: 2.52; acc: 0.69
Batch: 220; loss: 2.88; acc: 0.64
Batch: 240; loss: 2.44; acc: 0.7
Batch: 260; loss: 3.36; acc: 0.7
Batch: 280; loss: 3.89; acc: 0.59
Batch: 300; loss: 4.86; acc: 0.59
Batch: 320; loss: 2.64; acc: 0.72
Batch: 340; loss: 1.88; acc: 0.7
Batch: 360; loss: 5.48; acc: 0.55
Batch: 380; loss: 2.38; acc: 0.72
Batch: 400; loss: 2.93; acc: 0.69
Batch: 420; loss: 3.81; acc: 0.61
Batch: 440; loss: 2.91; acc: 0.62
Batch: 460; loss: 1.8; acc: 0.77
Batch: 480; loss: 2.3; acc: 0.66
Batch: 500; loss: 3.08; acc: 0.66
Batch: 520; loss: 3.38; acc: 0.55
Batch: 540; loss: 2.17; acc: 0.72
Batch: 560; loss: 3.19; acc: 0.7
Batch: 580; loss: 2.0; acc: 0.72
Batch: 600; loss: 2.96; acc: 0.77
Batch: 620; loss: 2.66; acc: 0.73
Train Epoch over. train_loss: 3.14; train_accuracy: 0.66 

Batch: 0; loss: 4.21; acc: 0.61
Batch: 20; loss: 7.7; acc: 0.47
Batch: 40; loss: 2.88; acc: 0.64
Batch: 60; loss: 4.05; acc: 0.62
Batch: 80; loss: 4.22; acc: 0.56
Batch: 100; loss: 5.51; acc: 0.53
Batch: 120; loss: 4.03; acc: 0.62
Batch: 140; loss: 8.16; acc: 0.44
Val Epoch over. val_loss: 5.414500115024056; val_accuracy: 0.5448845541401274 

plots/subspace_training/lenet/2020-01-10 14:00:56/d_dim_300_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 84064
elements in E: 17770400
fraction nonzero: 0.004730563183721244
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.05; acc: 0.34
Batch: 40; loss: 1.88; acc: 0.44
Batch: 60; loss: 1.42; acc: 0.59
Batch: 80; loss: 1.23; acc: 0.64
Batch: 100; loss: 1.13; acc: 0.62
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 1.12; acc: 0.66
Batch: 160; loss: 1.04; acc: 0.77
Batch: 180; loss: 0.87; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 0.84; acc: 0.72
Batch: 240; loss: 0.8; acc: 0.73
Batch: 260; loss: 1.22; acc: 0.66
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.93; acc: 0.75
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.65; acc: 0.72
Batch: 400; loss: 0.73; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.77
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.84; acc: 0.78
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.79; acc: 0.78
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.99; train_accuracy: 0.72 

Batch: 0; loss: 0.43; acc: 0.8
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.8
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.66; acc: 0.77
Batch: 100; loss: 0.96; acc: 0.78
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 0.6513222558483197; val_accuracy: 0.8057324840764332 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.73; acc: 0.72
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.82; acc: 0.78
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.79; acc: 0.75
Batch: 300; loss: 1.03; acc: 0.77
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.75; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.38; acc: 0.84
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.83; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.56; train_accuracy: 0.83 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.73; acc: 0.77
Val Epoch over. val_loss: 0.5672931239293639; val_accuracy: 0.8351910828025477 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.95
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.91; acc: 0.78
Batch: 240; loss: 0.64; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.78; acc: 0.8
Batch: 420; loss: 0.67; acc: 0.77
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.38; acc: 0.84
Batch: 620; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.85 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.99; acc: 0.78
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.89; acc: 0.78
Val Epoch over. val_loss: 0.6095580582975582; val_accuracy: 0.8326035031847133 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.83
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.86
Batch: 320; loss: 0.67; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.47; acc: 0.8
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.25; acc: 0.88
Batch: 560; loss: 0.91; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.57; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.75
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.82; acc: 0.78
Val Epoch over. val_loss: 0.6266354385075296; val_accuracy: 0.8228503184713376 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.77
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.35; acc: 0.83
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.7; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.8
Batch: 200; loss: 0.83; acc: 0.86
Batch: 220; loss: 0.95; acc: 0.77
Batch: 240; loss: 0.88; acc: 0.83
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.89
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.78
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.8
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.86 

Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.78; acc: 0.78
Val Epoch over. val_loss: 0.6032843399009887; val_accuracy: 0.8236464968152867 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.62; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.85; acc: 0.73
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.72; acc: 0.75
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.88; acc: 0.78
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.69; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.85; acc: 0.78
Val Epoch over. val_loss: 0.5769436280628678; val_accuracy: 0.8416600318471338 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.78
Batch: 120; loss: 0.39; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.9; acc: 0.78
Batch: 180; loss: 0.65; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.81
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.75; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.84
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.84
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 0.86; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.96; acc: 0.77
Val Epoch over. val_loss: 0.7313463680778339; val_accuracy: 0.8118033439490446 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.7; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.68; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.74; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.67; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.65; acc: 0.8
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.92; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.85; acc: 0.81
Val Epoch over. val_loss: 0.5459480076838451; val_accuracy: 0.8484275477707006 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.69; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.82; acc: 0.8
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.81
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.76; acc: 0.86
Val Epoch over. val_loss: 0.5451470331591406; val_accuracy: 0.8480294585987261 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.76; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.77
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.9; acc: 0.83
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.82; acc: 0.8
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.87; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.82; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.76; acc: 0.8
Batch: 100; loss: 0.94; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.81; acc: 0.8
Val Epoch over. val_loss: 0.6619300878351662; val_accuracy: 0.8211584394904459 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.54; acc: 0.77
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.74; acc: 0.78
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.84
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.73; acc: 0.86
Batch: 540; loss: 1.13; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.84; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.77; acc: 0.88
Val Epoch over. val_loss: 0.5591502097097172; val_accuracy: 0.8525079617834395 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.28; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.81
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.8; acc: 0.83
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.83
Batch: 380; loss: 0.27; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.61; acc: 0.84
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.87; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.8; acc: 0.84
Val Epoch over. val_loss: 0.5980593139768406; val_accuracy: 0.841062898089172 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.68; acc: 0.86
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.93; acc: 0.86
Val Epoch over. val_loss: 0.6146524189763768; val_accuracy: 0.8408638535031847 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.67; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.62; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.68; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.99; acc: 0.81
Batch: 120; loss: 0.69; acc: 0.88
Batch: 140; loss: 1.06; acc: 0.83
Val Epoch over. val_loss: 0.6548462229169858; val_accuracy: 0.8351910828025477 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.88
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.8
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.69; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.72; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 1.0; acc: 0.8
Batch: 540; loss: 0.8; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.87 

Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 1.04; acc: 0.81
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 1.19; acc: 0.83
Val Epoch over. val_loss: 0.683353231780848; val_accuracy: 0.8320063694267515 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.83; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.9; acc: 0.84
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.59; acc: 0.92
Batch: 360; loss: 0.6; acc: 0.91
Batch: 380; loss: 0.66; acc: 0.78
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 1.01; acc: 0.73
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 1.11; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.89
Batch: 140; loss: 1.32; acc: 0.8
Val Epoch over. val_loss: 0.7378398983911344; val_accuracy: 0.8260350318471338 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.84; acc: 0.78
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.72; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.84
Batch: 360; loss: 0.93; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.9; acc: 0.86
Batch: 540; loss: 0.9; acc: 0.78
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.91
Batch: 620; loss: 1.02; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.87 

Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.81; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 1.22; acc: 0.8
Batch: 120; loss: 0.7; acc: 0.88
Batch: 140; loss: 1.48; acc: 0.77
Val Epoch over. val_loss: 0.777984824245143; val_accuracy: 0.819765127388535 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.97; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.97; acc: 0.81
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 1.12; acc: 0.8
Batch: 280; loss: 0.55; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.93; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.94; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.98; acc: 0.81
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.8
Batch: 480; loss: 0.78; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.86 

Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 1.4; acc: 0.8
Batch: 120; loss: 0.72; acc: 0.89
Batch: 140; loss: 1.74; acc: 0.72
Val Epoch over. val_loss: 0.8578215727380886; val_accuracy: 0.8086186305732485 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.74; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 0.71; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.73; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 1.07; acc: 0.77
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.88; acc: 0.8
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.73; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.85; acc: 0.83
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.94; acc: 0.8
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 1.14; acc: 0.8
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.86 

Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.88; acc: 0.8
Batch: 80; loss: 0.72; acc: 0.81
Batch: 100; loss: 1.43; acc: 0.78
Batch: 120; loss: 0.84; acc: 0.88
Batch: 140; loss: 1.9; acc: 0.73
Val Epoch over. val_loss: 0.9557298848963087; val_accuracy: 0.7969745222929936 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.16; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.81
Batch: 40; loss: 1.14; acc: 0.83
Batch: 60; loss: 0.82; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.84; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 1.1; acc: 0.78
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 1.33; acc: 0.7
Batch: 280; loss: 0.94; acc: 0.72
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.7; acc: 0.84
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.61; acc: 0.91
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.6; train_accuracy: 0.86 

Batch: 0; loss: 0.78; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.64; acc: 0.8
Batch: 60; loss: 0.87; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.88
Batch: 100; loss: 1.4; acc: 0.78
Batch: 120; loss: 0.91; acc: 0.88
Batch: 140; loss: 1.93; acc: 0.73
Val Epoch over. val_loss: 0.9460272080959029; val_accuracy: 0.8037420382165605 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 1.04; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.75; acc: 0.84
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.76; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 1.07; acc: 0.75
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.91; acc: 0.83
Batch: 300; loss: 0.74; acc: 0.89
Batch: 320; loss: 1.02; acc: 0.83
Batch: 340; loss: 0.78; acc: 0.83
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.7; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.96; acc: 0.81
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.96; acc: 0.77
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.91; acc: 0.78
Batch: 540; loss: 0.85; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.96; acc: 0.8
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.84 

Batch: 0; loss: 1.31; acc: 0.73
Batch: 20; loss: 1.22; acc: 0.77
Batch: 40; loss: 0.88; acc: 0.78
Batch: 60; loss: 1.32; acc: 0.75
Batch: 80; loss: 1.15; acc: 0.8
Batch: 100; loss: 2.05; acc: 0.72
Batch: 120; loss: 1.38; acc: 0.8
Batch: 140; loss: 2.74; acc: 0.64
Val Epoch over. val_loss: 1.4354274686734387; val_accuracy: 0.7420382165605095 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.09; acc: 0.59
Batch: 20; loss: 1.09; acc: 0.77
Batch: 40; loss: 1.59; acc: 0.78
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.82; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 1.71; acc: 0.69
Batch: 140; loss: 1.82; acc: 0.69
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.84
Batch: 200; loss: 1.76; acc: 0.75
Batch: 220; loss: 1.7; acc: 0.77
Batch: 240; loss: 1.65; acc: 0.73
Batch: 260; loss: 0.68; acc: 0.81
Batch: 280; loss: 0.79; acc: 0.8
Batch: 300; loss: 1.13; acc: 0.75
Batch: 320; loss: 0.73; acc: 0.83
Batch: 340; loss: 0.81; acc: 0.81
Batch: 360; loss: 1.48; acc: 0.78
Batch: 380; loss: 1.33; acc: 0.81
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.91
Batch: 440; loss: 1.12; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.84
Batch: 480; loss: 1.29; acc: 0.78
Batch: 500; loss: 1.18; acc: 0.73
Batch: 520; loss: 1.28; acc: 0.72
Batch: 540; loss: 1.45; acc: 0.77
Batch: 560; loss: 0.9; acc: 0.83
Batch: 580; loss: 1.09; acc: 0.84
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 1.4; acc: 0.77
Train Epoch over. train_loss: 1.06; train_accuracy: 0.8 

Batch: 0; loss: 2.53; acc: 0.67
Batch: 20; loss: 2.17; acc: 0.62
Batch: 40; loss: 1.59; acc: 0.73
Batch: 60; loss: 1.89; acc: 0.7
Batch: 80; loss: 1.72; acc: 0.77
Batch: 100; loss: 2.92; acc: 0.62
Batch: 120; loss: 2.3; acc: 0.73
Batch: 140; loss: 4.22; acc: 0.55
Val Epoch over. val_loss: 2.2251150790293504; val_accuracy: 0.6742635350318471 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.9; acc: 0.72
Batch: 20; loss: 2.83; acc: 0.66
Batch: 40; loss: 1.23; acc: 0.77
Batch: 60; loss: 2.12; acc: 0.67
Batch: 80; loss: 1.33; acc: 0.75
Batch: 100; loss: 1.55; acc: 0.78
Batch: 120; loss: 2.9; acc: 0.61
Batch: 140; loss: 1.99; acc: 0.64
Batch: 160; loss: 1.79; acc: 0.72
Batch: 180; loss: 1.69; acc: 0.73
Batch: 200; loss: 1.62; acc: 0.8
Batch: 220; loss: 1.45; acc: 0.77
Batch: 240; loss: 1.33; acc: 0.72
Batch: 260; loss: 1.4; acc: 0.8
Batch: 280; loss: 1.19; acc: 0.73
Batch: 300; loss: 2.22; acc: 0.7
Batch: 320; loss: 1.42; acc: 0.78
Batch: 340; loss: 1.41; acc: 0.78
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 1.23; acc: 0.81
Batch: 400; loss: 1.95; acc: 0.66
Batch: 420; loss: 1.98; acc: 0.73
Batch: 440; loss: 1.13; acc: 0.84
Batch: 460; loss: 1.96; acc: 0.67
Batch: 480; loss: 1.89; acc: 0.7
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 1.39; acc: 0.77
Batch: 540; loss: 1.25; acc: 0.83
Batch: 560; loss: 0.89; acc: 0.81
Batch: 580; loss: 1.75; acc: 0.73
Batch: 600; loss: 1.76; acc: 0.77
Batch: 620; loss: 1.76; acc: 0.69
Train Epoch over. train_loss: 1.53; train_accuracy: 0.75 

Batch: 0; loss: 3.68; acc: 0.64
Batch: 20; loss: 3.54; acc: 0.59
Batch: 40; loss: 2.59; acc: 0.67
Batch: 60; loss: 2.39; acc: 0.67
Batch: 80; loss: 2.13; acc: 0.7
Batch: 100; loss: 3.98; acc: 0.56
Batch: 120; loss: 3.43; acc: 0.69
Batch: 140; loss: 5.85; acc: 0.5
Val Epoch over. val_loss: 3.140353803422041; val_accuracy: 0.6269904458598726 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.79; acc: 0.72
Batch: 20; loss: 2.04; acc: 0.69
Batch: 40; loss: 3.31; acc: 0.69
Batch: 60; loss: 3.02; acc: 0.66
Batch: 80; loss: 2.75; acc: 0.66
Batch: 100; loss: 3.96; acc: 0.62
Batch: 120; loss: 1.92; acc: 0.67
Batch: 140; loss: 3.29; acc: 0.66
Batch: 160; loss: 1.56; acc: 0.75
Batch: 180; loss: 1.56; acc: 0.77
Batch: 200; loss: 2.38; acc: 0.7
Batch: 220; loss: 2.61; acc: 0.69
Batch: 240; loss: 3.11; acc: 0.56
Batch: 260; loss: 2.0; acc: 0.72
Batch: 280; loss: 1.48; acc: 0.75
Batch: 300; loss: 0.9; acc: 0.83
Batch: 320; loss: 1.71; acc: 0.77
Batch: 340; loss: 2.6; acc: 0.64
Batch: 360; loss: 1.9; acc: 0.77
Batch: 380; loss: 2.41; acc: 0.75
Batch: 400; loss: 1.51; acc: 0.75
Batch: 420; loss: 2.11; acc: 0.72
Batch: 440; loss: 1.76; acc: 0.75
Batch: 460; loss: 1.55; acc: 0.72
Batch: 480; loss: 1.17; acc: 0.78
Batch: 500; loss: 1.94; acc: 0.81
Batch: 520; loss: 1.32; acc: 0.69
Batch: 540; loss: 1.58; acc: 0.81
Batch: 560; loss: 1.65; acc: 0.75
Batch: 580; loss: 2.22; acc: 0.7
Batch: 600; loss: 2.58; acc: 0.67
Batch: 620; loss: 1.63; acc: 0.73
Train Epoch over. train_loss: 2.12; train_accuracy: 0.71 

Batch: 0; loss: 4.95; acc: 0.58
Batch: 20; loss: 5.41; acc: 0.48
Batch: 40; loss: 4.15; acc: 0.64
Batch: 60; loss: 3.1; acc: 0.64
Batch: 80; loss: 3.55; acc: 0.59
Batch: 100; loss: 5.23; acc: 0.5
Batch: 120; loss: 5.14; acc: 0.61
Batch: 140; loss: 7.99; acc: 0.41
Val Epoch over. val_loss: 4.369189948033375; val_accuracy: 0.5810111464968153 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.92; acc: 0.67
Batch: 20; loss: 4.63; acc: 0.61
Batch: 40; loss: 4.92; acc: 0.61
Batch: 60; loss: 4.22; acc: 0.64
Batch: 80; loss: 2.53; acc: 0.67
Batch: 100; loss: 1.57; acc: 0.78
Batch: 120; loss: 3.83; acc: 0.56
Batch: 140; loss: 2.02; acc: 0.7
Batch: 160; loss: 1.69; acc: 0.7
Batch: 180; loss: 3.57; acc: 0.69
Batch: 200; loss: 2.76; acc: 0.64
Batch: 220; loss: 2.55; acc: 0.7
Batch: 240; loss: 3.07; acc: 0.62
Batch: 260; loss: 2.8; acc: 0.67
Batch: 280; loss: 2.26; acc: 0.62
Batch: 300; loss: 2.82; acc: 0.67
Batch: 320; loss: 3.23; acc: 0.61
Batch: 340; loss: 2.93; acc: 0.66
Batch: 360; loss: 2.82; acc: 0.77
Batch: 380; loss: 2.78; acc: 0.72
Batch: 400; loss: 1.53; acc: 0.8
Batch: 420; loss: 2.05; acc: 0.8
Batch: 440; loss: 2.86; acc: 0.61
Batch: 460; loss: 2.36; acc: 0.7
Batch: 480; loss: 3.04; acc: 0.7
Batch: 500; loss: 3.0; acc: 0.64
Batch: 520; loss: 3.55; acc: 0.52
Batch: 540; loss: 1.8; acc: 0.83
Batch: 560; loss: 2.97; acc: 0.64
Batch: 580; loss: 3.06; acc: 0.64
Batch: 600; loss: 2.35; acc: 0.69
Batch: 620; loss: 1.58; acc: 0.8
Train Epoch over. train_loss: 2.85; train_accuracy: 0.68 

Batch: 0; loss: 6.02; acc: 0.59
Batch: 20; loss: 7.36; acc: 0.42
Batch: 40; loss: 5.87; acc: 0.56
Batch: 60; loss: 3.9; acc: 0.58
Batch: 80; loss: 5.4; acc: 0.53
Batch: 100; loss: 6.58; acc: 0.47
Batch: 120; loss: 6.97; acc: 0.56
Batch: 140; loss: 10.83; acc: 0.38
Val Epoch over. val_loss: 5.783217156768605; val_accuracy: 0.5435907643312102 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 7.11; acc: 0.39
Batch: 20; loss: 4.0; acc: 0.56
Batch: 40; loss: 4.49; acc: 0.59
Batch: 60; loss: 4.44; acc: 0.64
Batch: 80; loss: 5.47; acc: 0.52
Batch: 100; loss: 3.64; acc: 0.61
Batch: 120; loss: 4.0; acc: 0.59
Batch: 140; loss: 4.49; acc: 0.66
Batch: 160; loss: 4.57; acc: 0.61
Batch: 180; loss: 4.9; acc: 0.62
Batch: 200; loss: 2.19; acc: 0.72
Batch: 220; loss: 5.02; acc: 0.67
Batch: 240; loss: 5.16; acc: 0.56
Batch: 260; loss: 2.8; acc: 0.7
Batch: 280; loss: 2.13; acc: 0.66
Batch: 300; loss: 2.75; acc: 0.73
Batch: 320; loss: 4.95; acc: 0.69
Batch: 340; loss: 2.69; acc: 0.67
Batch: 360; loss: 3.13; acc: 0.64
Batch: 380; loss: 2.48; acc: 0.75
Batch: 400; loss: 3.75; acc: 0.64
Batch: 420; loss: 3.07; acc: 0.66
Batch: 440; loss: 2.9; acc: 0.61
Batch: 460; loss: 3.81; acc: 0.62
Batch: 480; loss: 4.86; acc: 0.61
Batch: 500; loss: 3.97; acc: 0.59
Batch: 520; loss: 2.64; acc: 0.64
Batch: 540; loss: 3.42; acc: 0.72
Batch: 560; loss: 5.18; acc: 0.59
Batch: 580; loss: 3.37; acc: 0.72
Batch: 600; loss: 3.69; acc: 0.61
Batch: 620; loss: 1.56; acc: 0.75
Train Epoch over. train_loss: 3.73; train_accuracy: 0.64 

Batch: 0; loss: 7.23; acc: 0.53
Batch: 20; loss: 10.68; acc: 0.39
Batch: 40; loss: 8.24; acc: 0.53
Batch: 60; loss: 5.13; acc: 0.56
Batch: 80; loss: 7.07; acc: 0.56
Batch: 100; loss: 8.59; acc: 0.44
Batch: 120; loss: 9.28; acc: 0.5
Batch: 140; loss: 13.85; acc: 0.41
Val Epoch over. val_loss: 7.587930150852082; val_accuracy: 0.5116441082802548 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 7.76; acc: 0.53
Batch: 20; loss: 5.15; acc: 0.59
Batch: 40; loss: 5.51; acc: 0.61
Batch: 60; loss: 5.72; acc: 0.58
Batch: 80; loss: 5.3; acc: 0.52
Batch: 100; loss: 3.59; acc: 0.7
Batch: 120; loss: 5.76; acc: 0.56
Batch: 140; loss: 4.17; acc: 0.66
Batch: 160; loss: 4.26; acc: 0.66
Batch: 180; loss: 4.06; acc: 0.62
Batch: 200; loss: 4.65; acc: 0.62
Batch: 220; loss: 3.82; acc: 0.66
Batch: 240; loss: 6.1; acc: 0.52
Batch: 260; loss: 3.8; acc: 0.59
Batch: 280; loss: 4.75; acc: 0.61
Batch: 300; loss: 4.11; acc: 0.59
Batch: 320; loss: 5.55; acc: 0.59
Batch: 340; loss: 5.49; acc: 0.66
Batch: 360; loss: 4.62; acc: 0.66
Batch: 380; loss: 3.47; acc: 0.73
Batch: 400; loss: 3.79; acc: 0.62
Batch: 420; loss: 5.16; acc: 0.64
Batch: 440; loss: 3.92; acc: 0.58
Batch: 460; loss: 3.76; acc: 0.7
Batch: 480; loss: 4.65; acc: 0.61
Batch: 500; loss: 5.72; acc: 0.64
Batch: 520; loss: 5.8; acc: 0.52
Batch: 540; loss: 4.68; acc: 0.66
Batch: 560; loss: 1.96; acc: 0.73
Batch: 580; loss: 2.4; acc: 0.67
Batch: 600; loss: 3.94; acc: 0.64
Batch: 620; loss: 2.18; acc: 0.81
Train Epoch over. train_loss: 4.82; train_accuracy: 0.61 

Batch: 0; loss: 8.65; acc: 0.55
Batch: 20; loss: 13.58; acc: 0.36
Batch: 40; loss: 10.68; acc: 0.52
Batch: 60; loss: 6.64; acc: 0.52
Batch: 80; loss: 8.36; acc: 0.56
Batch: 100; loss: 10.65; acc: 0.41
Batch: 120; loss: 11.92; acc: 0.42
Batch: 140; loss: 16.81; acc: 0.38
Val Epoch over. val_loss: 9.527164978586185; val_accuracy: 0.48915207006369427 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 11.76; acc: 0.48
Batch: 20; loss: 6.68; acc: 0.5
Batch: 40; loss: 8.74; acc: 0.55
Batch: 60; loss: 7.91; acc: 0.58
Batch: 80; loss: 7.65; acc: 0.66
Batch: 100; loss: 6.72; acc: 0.58
Batch: 120; loss: 8.09; acc: 0.61
Batch: 140; loss: 7.02; acc: 0.53
Batch: 160; loss: 6.68; acc: 0.58
Batch: 180; loss: 6.09; acc: 0.55
Batch: 200; loss: 7.35; acc: 0.56
Batch: 220; loss: 4.81; acc: 0.64
Batch: 240; loss: 4.55; acc: 0.64
Batch: 260; loss: 8.28; acc: 0.52
Batch: 280; loss: 6.83; acc: 0.48
Batch: 300; loss: 5.97; acc: 0.62
Batch: 320; loss: 4.4; acc: 0.59
Batch: 340; loss: 5.14; acc: 0.56
Batch: 360; loss: 5.86; acc: 0.58
Batch: 380; loss: 7.72; acc: 0.64
Batch: 400; loss: 4.9; acc: 0.61
Batch: 420; loss: 5.68; acc: 0.58
Batch: 440; loss: 5.26; acc: 0.66
Batch: 460; loss: 4.86; acc: 0.62
Batch: 480; loss: 4.93; acc: 0.62
Batch: 500; loss: 5.2; acc: 0.66
Batch: 520; loss: 4.01; acc: 0.58
Batch: 540; loss: 3.6; acc: 0.7
Batch: 560; loss: 4.0; acc: 0.62
Batch: 580; loss: 3.44; acc: 0.62
Batch: 600; loss: 5.96; acc: 0.61
Batch: 620; loss: 4.75; acc: 0.61
Train Epoch over. train_loss: 6.05; train_accuracy: 0.58 

Batch: 0; loss: 10.36; acc: 0.59
Batch: 20; loss: 16.51; acc: 0.33
Batch: 40; loss: 12.66; acc: 0.52
Batch: 60; loss: 8.91; acc: 0.48
Batch: 80; loss: 10.07; acc: 0.55
Batch: 100; loss: 12.71; acc: 0.41
Batch: 120; loss: 14.95; acc: 0.41
Batch: 140; loss: 19.37; acc: 0.36
Val Epoch over. val_loss: 11.850774282103131; val_accuracy: 0.47054140127388533 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 11.15; acc: 0.47
Batch: 20; loss: 10.71; acc: 0.45
Batch: 40; loss: 11.04; acc: 0.48
Batch: 60; loss: 12.88; acc: 0.53
Batch: 80; loss: 9.61; acc: 0.55
Batch: 100; loss: 9.03; acc: 0.44
Batch: 120; loss: 5.91; acc: 0.67
Batch: 140; loss: 6.46; acc: 0.61
Batch: 160; loss: 7.22; acc: 0.61
Batch: 180; loss: 8.56; acc: 0.59
Batch: 200; loss: 9.12; acc: 0.47
Batch: 220; loss: 7.19; acc: 0.5
Batch: 240; loss: 4.61; acc: 0.67
Batch: 260; loss: 10.15; acc: 0.47
Batch: 280; loss: 7.66; acc: 0.5
Batch: 300; loss: 8.51; acc: 0.45
Batch: 320; loss: 8.24; acc: 0.47
Batch: 340; loss: 5.56; acc: 0.58
Batch: 360; loss: 7.69; acc: 0.64
Batch: 380; loss: 8.1; acc: 0.56
Batch: 400; loss: 3.96; acc: 0.69
Batch: 420; loss: 7.11; acc: 0.53
Batch: 440; loss: 4.49; acc: 0.69
Batch: 460; loss: 5.28; acc: 0.72
Batch: 480; loss: 6.61; acc: 0.69
Batch: 500; loss: 5.95; acc: 0.56
Batch: 520; loss: 5.2; acc: 0.67
Batch: 540; loss: 8.52; acc: 0.53
Batch: 560; loss: 6.79; acc: 0.61
Batch: 580; loss: 6.93; acc: 0.55
Batch: 600; loss: 5.86; acc: 0.58
Batch: 620; loss: 6.62; acc: 0.58
Train Epoch over. train_loss: 7.42; train_accuracy: 0.56 

Batch: 0; loss: 12.43; acc: 0.52
Batch: 20; loss: 20.72; acc: 0.31
Batch: 40; loss: 14.04; acc: 0.48
Batch: 60; loss: 11.62; acc: 0.53
Batch: 80; loss: 11.45; acc: 0.48
Batch: 100; loss: 15.13; acc: 0.39
Batch: 120; loss: 17.09; acc: 0.38
Batch: 140; loss: 21.93; acc: 0.38
Val Epoch over. val_loss: 14.272632210117996; val_accuracy: 0.462281050955414 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 14.44; acc: 0.44
Batch: 20; loss: 14.26; acc: 0.48
Batch: 40; loss: 14.14; acc: 0.47
Batch: 60; loss: 9.59; acc: 0.58
Batch: 80; loss: 10.51; acc: 0.5
Batch: 100; loss: 10.0; acc: 0.5
Batch: 120; loss: 9.77; acc: 0.5
Batch: 140; loss: 7.55; acc: 0.59
Batch: 160; loss: 7.24; acc: 0.55
Batch: 180; loss: 9.27; acc: 0.59
Batch: 200; loss: 11.25; acc: 0.42
Batch: 220; loss: 9.09; acc: 0.58
Batch: 240; loss: 5.53; acc: 0.69
Batch: 260; loss: 8.25; acc: 0.58
Batch: 280; loss: 8.32; acc: 0.58
Batch: 300; loss: 13.39; acc: 0.41
Batch: 320; loss: 12.28; acc: 0.47
Batch: 340; loss: 9.21; acc: 0.56
Batch: 360; loss: 10.73; acc: 0.52
Batch: 380; loss: 11.22; acc: 0.45
Batch: 400; loss: 10.1; acc: 0.47
Batch: 420; loss: 7.46; acc: 0.58
Batch: 440; loss: 7.62; acc: 0.58
Batch: 460; loss: 6.72; acc: 0.55
Batch: 480; loss: 6.96; acc: 0.62
Batch: 500; loss: 7.96; acc: 0.59
Batch: 520; loss: 6.73; acc: 0.55
Batch: 540; loss: 5.28; acc: 0.61
Batch: 560; loss: 8.5; acc: 0.53
Batch: 580; loss: 4.98; acc: 0.64
Batch: 600; loss: 7.51; acc: 0.56
Batch: 620; loss: 5.16; acc: 0.58
Train Epoch over. train_loss: 8.88; train_accuracy: 0.54 

Batch: 0; loss: 15.66; acc: 0.52
Batch: 20; loss: 24.72; acc: 0.31
Batch: 40; loss: 17.75; acc: 0.5
Batch: 60; loss: 16.22; acc: 0.55
Batch: 80; loss: 14.58; acc: 0.48
Batch: 100; loss: 18.32; acc: 0.41
Batch: 120; loss: 19.5; acc: 0.41
Batch: 140; loss: 24.59; acc: 0.39
Val Epoch over. val_loss: 17.21018042837738; val_accuracy: 0.44516321656050956 

plots/subspace_training/lenet/2020-01-10 14:00:56/d_dim_400_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 105915
elements in E: 22213000
fraction nonzero: 0.004768153783820285
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 1.8; acc: 0.45
Batch: 40; loss: 1.34; acc: 0.64
Batch: 60; loss: 0.83; acc: 0.69
Batch: 80; loss: 0.99; acc: 0.75
Batch: 100; loss: 0.59; acc: 0.77
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 1.07; acc: 0.66
Batch: 160; loss: 1.04; acc: 0.72
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.92; acc: 0.77
Batch: 240; loss: 0.66; acc: 0.75
Batch: 260; loss: 1.05; acc: 0.69
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.75; acc: 0.75
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.82; acc: 0.78
Batch: 400; loss: 0.57; acc: 0.8
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.81
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.82; train_accuracy: 0.77 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 1.14; acc: 0.66
Val Epoch over. val_loss: 0.6128626779955664; val_accuracy: 0.8275278662420382 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.8
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.65; acc: 0.77
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.81
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 1.05; acc: 0.8
Val Epoch over. val_loss: 0.49316530722151897; val_accuracy: 0.8608678343949044 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.76; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.98; acc: 0.8
Val Epoch over. val_loss: 0.43791910408029133; val_accuracy: 0.872312898089172 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.77; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.74; acc: 0.84
Batch: 220; loss: 0.35; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.7; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.8
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.88; acc: 0.83
Val Epoch over. val_loss: 0.4528348704052579; val_accuracy: 0.8602707006369427 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.72; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.83
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.41424949894285507; val_accuracy: 0.8827627388535032 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.63; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.61; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 0.72; acc: 0.86
Batch: 140; loss: 1.12; acc: 0.75
Val Epoch over. val_loss: 0.446987550919223; val_accuracy: 0.8794785031847133 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.84
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.77
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 1.11; acc: 0.72
Val Epoch over. val_loss: 0.4371332574137457; val_accuracy: 0.8843550955414012 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.89
Batch: 240; loss: 0.83; acc: 0.84
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.84
Batch: 140; loss: 1.37; acc: 0.75
Val Epoch over. val_loss: 0.4470582916193707; val_accuracy: 0.8744028662420382 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.57; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.82; acc: 0.84
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.6; acc: 0.86
Batch: 380; loss: 0.76; acc: 0.83
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.78; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.83
Batch: 140; loss: 1.04; acc: 0.75
Val Epoch over. val_loss: 0.45038514456171896; val_accuracy: 0.8746019108280255 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.83
Batch: 140; loss: 1.1; acc: 0.78
Val Epoch over. val_loss: 0.4152254170388173; val_accuracy: 0.8859474522292994 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.76; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.83
Batch: 140; loss: 1.02; acc: 0.78
Val Epoch over. val_loss: 0.39648689068616577; val_accuracy: 0.8857484076433121 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.81
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.61; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 1.01; acc: 0.81
Batch: 140; loss: 1.2; acc: 0.77
Val Epoch over. val_loss: 0.4584844960433662; val_accuracy: 0.8718152866242038 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.74; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.81
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 1.08; acc: 0.78
Batch: 140; loss: 1.38; acc: 0.77
Val Epoch over. val_loss: 0.4953098228308046; val_accuracy: 0.8657444267515924 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.88
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.7; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.89
Batch: 620; loss: 0.91; acc: 0.83
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.81
Batch: 140; loss: 1.48; acc: 0.77
Val Epoch over. val_loss: 0.5628066061029009; val_accuracy: 0.8501194267515924 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.81
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.67; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.87; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.83; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.77
Batch: 140; loss: 1.63; acc: 0.75
Val Epoch over. val_loss: 0.6216993069952461; val_accuracy: 0.8416600318471338 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.67; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.98; acc: 0.89
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.65; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.74; acc: 0.81
Batch: 400; loss: 0.65; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 1.06; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.8; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.77
Batch: 140; loss: 1.75; acc: 0.7
Val Epoch over. val_loss: 0.6767826605184822; val_accuracy: 0.8342953821656051 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.83; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.78
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.86
Batch: 340; loss: 1.04; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.79; acc: 0.84
Batch: 540; loss: 0.82; acc: 0.77
Batch: 560; loss: 0.85; acc: 0.81
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.55; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.88 

Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.88; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.7
Batch: 140; loss: 2.14; acc: 0.69
Val Epoch over. val_loss: 0.7564495083442919; val_accuracy: 0.8215565286624203 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.52; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.89
Batch: 140; loss: 0.84; acc: 0.83
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.72; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.71; acc: 0.84
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.84
Batch: 400; loss: 0.71; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.88 

Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 1.34; acc: 0.69
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 1.1; acc: 0.8
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 1.35; acc: 0.7
Batch: 120; loss: 1.39; acc: 0.73
Batch: 140; loss: 2.59; acc: 0.61
Val Epoch over. val_loss: 0.9374588046483933; val_accuracy: 0.7887141719745223 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.72; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 1.01; acc: 0.8
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.67; acc: 0.89
Batch: 160; loss: 0.68; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.85; acc: 0.89
Batch: 260; loss: 0.96; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.68; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.71; acc: 0.84
Batch: 500; loss: 0.17; acc: 0.91
Batch: 520; loss: 1.13; acc: 0.8
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.61; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.78; acc: 0.83
Batch: 20; loss: 1.46; acc: 0.67
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 1.32; acc: 0.77
Batch: 80; loss: 1.03; acc: 0.8
Batch: 100; loss: 1.7; acc: 0.62
Batch: 120; loss: 1.53; acc: 0.7
Batch: 140; loss: 2.98; acc: 0.62
Val Epoch over. val_loss: 1.0771773047507949; val_accuracy: 0.7709992038216561 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.35; acc: 0.78
Batch: 20; loss: 1.03; acc: 0.84
Batch: 40; loss: 1.06; acc: 0.78
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.89
Batch: 160; loss: 0.72; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.68; acc: 0.78
Batch: 240; loss: 0.92; acc: 0.84
Batch: 260; loss: 0.69; acc: 0.88
Batch: 280; loss: 0.87; acc: 0.89
Batch: 300; loss: 0.83; acc: 0.83
Batch: 320; loss: 0.79; acc: 0.86
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.72; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.74; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.61; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.56; acc: 0.78
Batch: 620; loss: 0.44; acc: 0.92
Train Epoch over. train_loss: 0.55; train_accuracy: 0.87 

Batch: 0; loss: 0.83; acc: 0.84
Batch: 20; loss: 1.54; acc: 0.66
Batch: 40; loss: 0.69; acc: 0.81
Batch: 60; loss: 1.4; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.81
Batch: 100; loss: 1.89; acc: 0.55
Batch: 120; loss: 1.56; acc: 0.67
Batch: 140; loss: 2.99; acc: 0.61
Val Epoch over. val_loss: 1.115945024854818; val_accuracy: 0.7698049363057324 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.14; acc: 0.7
Batch: 20; loss: 0.96; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.8
Batch: 60; loss: 1.1; acc: 0.77
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.92; acc: 0.78
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.88
Batch: 260; loss: 0.67; acc: 0.88
Batch: 280; loss: 0.96; acc: 0.8
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.72; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.84; acc: 0.83
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 1.08; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.83; acc: 0.77
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.84 

Batch: 0; loss: 1.47; acc: 0.75
Batch: 20; loss: 2.38; acc: 0.55
Batch: 40; loss: 1.1; acc: 0.78
Batch: 60; loss: 2.12; acc: 0.66
Batch: 80; loss: 1.62; acc: 0.75
Batch: 100; loss: 3.04; acc: 0.5
Batch: 120; loss: 2.42; acc: 0.62
Batch: 140; loss: 4.33; acc: 0.56
Val Epoch over. val_loss: 1.8212797742360716; val_accuracy: 0.6984474522292994 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.97; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.75
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.3; acc: 0.8
Batch: 80; loss: 1.81; acc: 0.73
Batch: 100; loss: 1.59; acc: 0.73
Batch: 120; loss: 1.36; acc: 0.8
Batch: 140; loss: 1.68; acc: 0.7
Batch: 160; loss: 0.73; acc: 0.81
Batch: 180; loss: 1.17; acc: 0.84
Batch: 200; loss: 2.12; acc: 0.72
Batch: 220; loss: 2.1; acc: 0.7
Batch: 240; loss: 1.49; acc: 0.77
Batch: 260; loss: 1.08; acc: 0.7
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 1.36; acc: 0.77
Batch: 320; loss: 0.78; acc: 0.86
Batch: 340; loss: 1.0; acc: 0.81
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 1.29; acc: 0.77
Batch: 420; loss: 1.34; acc: 0.77
Batch: 440; loss: 1.0; acc: 0.81
Batch: 460; loss: 0.85; acc: 0.78
Batch: 480; loss: 1.36; acc: 0.75
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.74; acc: 0.86
Batch: 540; loss: 1.1; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.86
Batch: 600; loss: 2.04; acc: 0.7
Batch: 620; loss: 1.18; acc: 0.81
Train Epoch over. train_loss: 1.13; train_accuracy: 0.79 

Batch: 0; loss: 2.56; acc: 0.64
Batch: 20; loss: 3.96; acc: 0.48
Batch: 40; loss: 2.3; acc: 0.69
Batch: 60; loss: 3.04; acc: 0.66
Batch: 80; loss: 2.39; acc: 0.67
Batch: 100; loss: 4.8; acc: 0.44
Batch: 120; loss: 3.42; acc: 0.55
Batch: 140; loss: 5.97; acc: 0.5
Val Epoch over. val_loss: 2.9863502592038196; val_accuracy: 0.6168391719745223 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.09; acc: 0.62
Batch: 20; loss: 2.25; acc: 0.66
Batch: 40; loss: 1.44; acc: 0.77
Batch: 60; loss: 2.39; acc: 0.67
Batch: 80; loss: 1.2; acc: 0.86
Batch: 100; loss: 1.49; acc: 0.8
Batch: 120; loss: 2.28; acc: 0.73
Batch: 140; loss: 1.23; acc: 0.83
Batch: 160; loss: 1.82; acc: 0.75
Batch: 180; loss: 2.08; acc: 0.72
Batch: 200; loss: 1.48; acc: 0.8
Batch: 220; loss: 1.62; acc: 0.77
Batch: 240; loss: 1.66; acc: 0.72
Batch: 260; loss: 1.15; acc: 0.81
Batch: 280; loss: 1.8; acc: 0.72
Batch: 300; loss: 2.07; acc: 0.64
Batch: 320; loss: 1.28; acc: 0.75
Batch: 340; loss: 1.05; acc: 0.75
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 1.4; acc: 0.66
Batch: 400; loss: 2.01; acc: 0.7
Batch: 420; loss: 1.19; acc: 0.77
Batch: 440; loss: 1.59; acc: 0.73
Batch: 460; loss: 1.47; acc: 0.81
Batch: 480; loss: 2.0; acc: 0.72
Batch: 500; loss: 1.77; acc: 0.75
Batch: 520; loss: 1.45; acc: 0.77
Batch: 540; loss: 1.48; acc: 0.75
Batch: 560; loss: 0.73; acc: 0.84
Batch: 580; loss: 1.23; acc: 0.75
Batch: 600; loss: 1.43; acc: 0.73
Batch: 620; loss: 3.4; acc: 0.69
Train Epoch over. train_loss: 1.74; train_accuracy: 0.74 

Batch: 0; loss: 3.78; acc: 0.59
Batch: 20; loss: 5.97; acc: 0.42
Batch: 40; loss: 4.06; acc: 0.61
Batch: 60; loss: 4.24; acc: 0.62
Batch: 80; loss: 3.31; acc: 0.64
Batch: 100; loss: 6.24; acc: 0.47
Batch: 120; loss: 4.68; acc: 0.5
Batch: 140; loss: 7.33; acc: 0.47
Val Epoch over. val_loss: 4.373719736269325; val_accuracy: 0.566281847133758 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 5.05; acc: 0.55
Batch: 20; loss: 2.57; acc: 0.67
Batch: 40; loss: 2.39; acc: 0.7
Batch: 60; loss: 2.57; acc: 0.66
Batch: 80; loss: 3.76; acc: 0.61
Batch: 100; loss: 3.49; acc: 0.59
Batch: 120; loss: 3.03; acc: 0.66
Batch: 140; loss: 1.57; acc: 0.69
Batch: 160; loss: 2.2; acc: 0.69
Batch: 180; loss: 2.78; acc: 0.69
Batch: 200; loss: 3.87; acc: 0.58
Batch: 220; loss: 1.81; acc: 0.66
Batch: 240; loss: 2.77; acc: 0.7
Batch: 260; loss: 1.95; acc: 0.67
Batch: 280; loss: 2.56; acc: 0.69
Batch: 300; loss: 1.89; acc: 0.75
Batch: 320; loss: 2.11; acc: 0.67
Batch: 340; loss: 1.86; acc: 0.66
Batch: 360; loss: 2.75; acc: 0.69
Batch: 380; loss: 1.01; acc: 0.72
Batch: 400; loss: 1.47; acc: 0.77
Batch: 420; loss: 2.57; acc: 0.66
Batch: 440; loss: 2.6; acc: 0.67
Batch: 460; loss: 3.35; acc: 0.7
Batch: 480; loss: 0.89; acc: 0.81
Batch: 500; loss: 2.92; acc: 0.62
Batch: 520; loss: 1.49; acc: 0.73
Batch: 540; loss: 2.09; acc: 0.67
Batch: 560; loss: 2.09; acc: 0.73
Batch: 580; loss: 2.91; acc: 0.72
Batch: 600; loss: 2.74; acc: 0.7
Batch: 620; loss: 2.56; acc: 0.66
Train Epoch over. train_loss: 2.53; train_accuracy: 0.7 

Batch: 0; loss: 5.16; acc: 0.48
Batch: 20; loss: 8.95; acc: 0.34
Batch: 40; loss: 5.48; acc: 0.59
Batch: 60; loss: 6.23; acc: 0.58
Batch: 80; loss: 4.32; acc: 0.61
Batch: 100; loss: 7.88; acc: 0.5
Batch: 120; loss: 6.05; acc: 0.48
Batch: 140; loss: 9.77; acc: 0.41
Val Epoch over. val_loss: 6.058656508755532; val_accuracy: 0.5240843949044586 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 7.18; acc: 0.55
Batch: 20; loss: 5.58; acc: 0.56
Batch: 40; loss: 4.95; acc: 0.61
Batch: 60; loss: 5.97; acc: 0.53
Batch: 80; loss: 3.57; acc: 0.67
Batch: 100; loss: 4.21; acc: 0.66
Batch: 120; loss: 4.07; acc: 0.59
Batch: 140; loss: 1.87; acc: 0.8
Batch: 160; loss: 2.78; acc: 0.66
Batch: 180; loss: 1.45; acc: 0.8
Batch: 200; loss: 4.1; acc: 0.7
Batch: 220; loss: 4.39; acc: 0.7
Batch: 240; loss: 3.67; acc: 0.66
Batch: 260; loss: 3.5; acc: 0.66
Batch: 280; loss: 2.7; acc: 0.72
Batch: 300; loss: 3.2; acc: 0.66
Batch: 320; loss: 3.1; acc: 0.64
Batch: 340; loss: 2.27; acc: 0.77
Batch: 360; loss: 2.57; acc: 0.73
Batch: 380; loss: 2.05; acc: 0.81
Batch: 400; loss: 1.72; acc: 0.77
Batch: 420; loss: 2.82; acc: 0.67
Batch: 440; loss: 4.82; acc: 0.58
Batch: 460; loss: 2.36; acc: 0.73
Batch: 480; loss: 2.84; acc: 0.72
Batch: 500; loss: 3.79; acc: 0.64
Batch: 520; loss: 4.22; acc: 0.5
Batch: 540; loss: 2.15; acc: 0.73
Batch: 560; loss: 3.07; acc: 0.73
Batch: 580; loss: 3.32; acc: 0.69
Batch: 600; loss: 3.84; acc: 0.52
Batch: 620; loss: 2.33; acc: 0.77
Train Epoch over. train_loss: 3.48; train_accuracy: 0.67 

Batch: 0; loss: 7.09; acc: 0.47
Batch: 20; loss: 12.07; acc: 0.36
Batch: 40; loss: 7.02; acc: 0.56
Batch: 60; loss: 8.43; acc: 0.53
Batch: 80; loss: 6.8; acc: 0.44
Batch: 100; loss: 10.4; acc: 0.44
Batch: 120; loss: 8.37; acc: 0.45
Batch: 140; loss: 13.13; acc: 0.34
Val Epoch over. val_loss: 8.240143062202794; val_accuracy: 0.49303343949044587 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 8.77; acc: 0.45
Batch: 20; loss: 6.98; acc: 0.61
Batch: 40; loss: 8.2; acc: 0.52
Batch: 60; loss: 7.16; acc: 0.55
Batch: 80; loss: 5.2; acc: 0.64
Batch: 100; loss: 3.98; acc: 0.72
Batch: 120; loss: 5.32; acc: 0.67
Batch: 140; loss: 5.28; acc: 0.61
Batch: 160; loss: 5.19; acc: 0.58
Batch: 180; loss: 7.11; acc: 0.58
Batch: 200; loss: 4.4; acc: 0.59
Batch: 220; loss: 4.76; acc: 0.59
Batch: 240; loss: 5.09; acc: 0.55
Batch: 260; loss: 3.88; acc: 0.69
Batch: 280; loss: 2.34; acc: 0.78
Batch: 300; loss: 4.16; acc: 0.69
Batch: 320; loss: 4.79; acc: 0.64
Batch: 340; loss: 4.19; acc: 0.67
Batch: 360; loss: 3.85; acc: 0.7
Batch: 380; loss: 3.4; acc: 0.72
Batch: 400; loss: 5.03; acc: 0.58
Batch: 420; loss: 2.25; acc: 0.73
Batch: 440; loss: 3.72; acc: 0.66
Batch: 460; loss: 5.83; acc: 0.62
Batch: 480; loss: 3.66; acc: 0.66
Batch: 500; loss: 3.58; acc: 0.72
Batch: 520; loss: 3.02; acc: 0.67
Batch: 540; loss: 4.64; acc: 0.66
Batch: 560; loss: 4.17; acc: 0.66
Batch: 580; loss: 4.82; acc: 0.59
Batch: 600; loss: 1.92; acc: 0.78
Batch: 620; loss: 2.93; acc: 0.72
Train Epoch over. train_loss: 4.62; train_accuracy: 0.65 

Batch: 0; loss: 9.32; acc: 0.61
Batch: 20; loss: 15.44; acc: 0.34
Batch: 40; loss: 8.9; acc: 0.5
Batch: 60; loss: 9.94; acc: 0.55
Batch: 80; loss: 10.65; acc: 0.44
Batch: 100; loss: 12.88; acc: 0.48
Batch: 120; loss: 11.46; acc: 0.48
Batch: 140; loss: 16.77; acc: 0.31
Val Epoch over. val_loss: 10.87411178297298; val_accuracy: 0.47870222929936307 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 8.56; acc: 0.5
Batch: 20; loss: 7.38; acc: 0.59
Batch: 40; loss: 9.25; acc: 0.56
Batch: 60; loss: 8.52; acc: 0.53
Batch: 80; loss: 6.47; acc: 0.58
Batch: 100; loss: 7.66; acc: 0.62
Batch: 120; loss: 5.2; acc: 0.69
Batch: 140; loss: 6.41; acc: 0.53
Batch: 160; loss: 4.28; acc: 0.78
Batch: 180; loss: 6.11; acc: 0.64
Batch: 200; loss: 7.05; acc: 0.61
Batch: 220; loss: 5.42; acc: 0.66
Batch: 240; loss: 4.32; acc: 0.7
Batch: 260; loss: 3.2; acc: 0.73
Batch: 280; loss: 5.0; acc: 0.64
Batch: 300; loss: 6.69; acc: 0.58
Batch: 320; loss: 4.3; acc: 0.67
Batch: 340; loss: 4.48; acc: 0.67
Batch: 360; loss: 5.25; acc: 0.66
Batch: 380; loss: 5.78; acc: 0.69
Batch: 400; loss: 5.29; acc: 0.64
Batch: 420; loss: 5.48; acc: 0.64
Batch: 440; loss: 4.2; acc: 0.62
Batch: 460; loss: 3.89; acc: 0.72
Batch: 480; loss: 5.38; acc: 0.72
Batch: 500; loss: 6.46; acc: 0.62
Batch: 520; loss: 7.1; acc: 0.64
Batch: 540; loss: 5.95; acc: 0.59
Batch: 560; loss: 2.94; acc: 0.69
Batch: 580; loss: 3.26; acc: 0.78
Batch: 600; loss: 5.28; acc: 0.59
Batch: 620; loss: 3.37; acc: 0.73
Train Epoch over. train_loss: 6.01; train_accuracy: 0.63 

Batch: 0; loss: 13.29; acc: 0.52
Batch: 20; loss: 20.29; acc: 0.38
Batch: 40; loss: 10.68; acc: 0.48
Batch: 60; loss: 13.0; acc: 0.58
Batch: 80; loss: 15.64; acc: 0.38
Batch: 100; loss: 16.5; acc: 0.47
Batch: 120; loss: 15.62; acc: 0.48
Batch: 140; loss: 21.21; acc: 0.31
Val Epoch over. val_loss: 14.329948613598088; val_accuracy: 0.4625796178343949 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 10.74; acc: 0.5
Batch: 20; loss: 10.91; acc: 0.48
Batch: 40; loss: 5.67; acc: 0.69
Batch: 60; loss: 10.53; acc: 0.48
Batch: 80; loss: 9.94; acc: 0.48
Batch: 100; loss: 9.56; acc: 0.59
Batch: 120; loss: 8.81; acc: 0.7
Batch: 140; loss: 8.21; acc: 0.59
Batch: 160; loss: 8.39; acc: 0.66
Batch: 180; loss: 7.53; acc: 0.61
Batch: 200; loss: 7.28; acc: 0.62
Batch: 220; loss: 6.37; acc: 0.62
Batch: 240; loss: 8.66; acc: 0.59
Batch: 260; loss: 6.04; acc: 0.67
Batch: 280; loss: 7.6; acc: 0.7
Batch: 300; loss: 6.13; acc: 0.66
Batch: 320; loss: 8.1; acc: 0.59
Batch: 340; loss: 7.75; acc: 0.61
Batch: 360; loss: 3.14; acc: 0.73
Batch: 380; loss: 10.03; acc: 0.55
Batch: 400; loss: 7.9; acc: 0.56
Batch: 420; loss: 6.13; acc: 0.66
Batch: 440; loss: 5.61; acc: 0.69
Batch: 460; loss: 7.93; acc: 0.58
Batch: 480; loss: 6.79; acc: 0.66
Batch: 500; loss: 5.77; acc: 0.67
Batch: 520; loss: 6.1; acc: 0.64
Batch: 540; loss: 5.7; acc: 0.73
Batch: 560; loss: 9.01; acc: 0.56
Batch: 580; loss: 5.82; acc: 0.69
Batch: 600; loss: 4.98; acc: 0.72
Batch: 620; loss: 5.1; acc: 0.72
Train Epoch over. train_loss: 7.78; train_accuracy: 0.62 

Batch: 0; loss: 17.63; acc: 0.52
Batch: 20; loss: 26.93; acc: 0.38
Batch: 40; loss: 15.59; acc: 0.5
Batch: 60; loss: 18.38; acc: 0.48
Batch: 80; loss: 21.89; acc: 0.41
Batch: 100; loss: 24.0; acc: 0.5
Batch: 120; loss: 20.28; acc: 0.48
Batch: 140; loss: 26.02; acc: 0.33
Val Epoch over. val_loss: 20.00865288752659; val_accuracy: 0.43988853503184716 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 13.8; acc: 0.52
Batch: 20; loss: 13.63; acc: 0.5
Batch: 40; loss: 9.47; acc: 0.61
Batch: 60; loss: 11.46; acc: 0.61
Batch: 80; loss: 12.27; acc: 0.5
Batch: 100; loss: 12.77; acc: 0.62
Batch: 120; loss: 8.82; acc: 0.7
Batch: 140; loss: 10.49; acc: 0.66
Batch: 160; loss: 14.0; acc: 0.5
Batch: 180; loss: 11.35; acc: 0.61
Batch: 200; loss: 12.07; acc: 0.58
Batch: 220; loss: 11.49; acc: 0.61
Batch: 240; loss: 7.42; acc: 0.62
Batch: 260; loss: 9.2; acc: 0.61
Batch: 280; loss: 6.99; acc: 0.72
Batch: 300; loss: 6.31; acc: 0.66
Batch: 320; loss: 9.77; acc: 0.58
Batch: 340; loss: 6.86; acc: 0.67
Batch: 360; loss: 10.25; acc: 0.59
Batch: 380; loss: 9.99; acc: 0.56
Batch: 400; loss: 9.43; acc: 0.62
Batch: 420; loss: 8.14; acc: 0.69
Batch: 440; loss: 13.72; acc: 0.55
Batch: 460; loss: 6.42; acc: 0.72
Batch: 480; loss: 8.72; acc: 0.77
Batch: 500; loss: 9.65; acc: 0.53
Batch: 520; loss: 9.17; acc: 0.64
Batch: 540; loss: 14.27; acc: 0.59
Batch: 560; loss: 7.28; acc: 0.72
Batch: 580; loss: 5.49; acc: 0.77
Batch: 600; loss: 9.63; acc: 0.62
Batch: 620; loss: 4.99; acc: 0.64
Train Epoch over. train_loss: 9.91; train_accuracy: 0.61 

Batch: 0; loss: 21.58; acc: 0.5
Batch: 20; loss: 33.83; acc: 0.31
Batch: 40; loss: 19.6; acc: 0.48
Batch: 60; loss: 24.38; acc: 0.5
Batch: 80; loss: 29.38; acc: 0.41
Batch: 100; loss: 29.45; acc: 0.47
Batch: 120; loss: 25.07; acc: 0.45
Batch: 140; loss: 35.23; acc: 0.38
Val Epoch over. val_loss: 25.991878284770213; val_accuracy: 0.43341958598726116 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 25.7; acc: 0.42
Batch: 20; loss: 17.71; acc: 0.48
Batch: 40; loss: 19.19; acc: 0.5
Batch: 60; loss: 10.05; acc: 0.66
Batch: 80; loss: 15.96; acc: 0.55
Batch: 100; loss: 12.66; acc: 0.56
Batch: 120; loss: 14.57; acc: 0.56
Batch: 140; loss: 14.24; acc: 0.59
Batch: 160; loss: 10.55; acc: 0.66
Batch: 180; loss: 7.92; acc: 0.75
Batch: 200; loss: 10.12; acc: 0.64
Batch: 220; loss: 14.96; acc: 0.53
Batch: 240; loss: 12.07; acc: 0.61
Batch: 260; loss: 12.9; acc: 0.61
Batch: 280; loss: 14.19; acc: 0.58
Batch: 300; loss: 6.1; acc: 0.69
Batch: 320; loss: 17.05; acc: 0.56
Batch: 340; loss: 12.64; acc: 0.53
Batch: 360; loss: 19.33; acc: 0.44
Batch: 380; loss: 8.67; acc: 0.72
Batch: 400; loss: 11.67; acc: 0.56
Batch: 420; loss: 8.26; acc: 0.66
Batch: 440; loss: 10.34; acc: 0.62
Batch: 460; loss: 9.74; acc: 0.61
Batch: 480; loss: 11.45; acc: 0.56
Batch: 500; loss: 9.14; acc: 0.58
Batch: 520; loss: 11.16; acc: 0.58
Batch: 540; loss: 9.6; acc: 0.7
Batch: 560; loss: 11.91; acc: 0.59
Batch: 580; loss: 10.14; acc: 0.61
Batch: 600; loss: 12.37; acc: 0.66
Batch: 620; loss: 10.76; acc: 0.58
Train Epoch over. train_loss: 12.53; train_accuracy: 0.6 

Batch: 0; loss: 27.24; acc: 0.47
Batch: 20; loss: 45.32; acc: 0.31
Batch: 40; loss: 25.33; acc: 0.53
Batch: 60; loss: 28.1; acc: 0.55
Batch: 80; loss: 40.04; acc: 0.31
Batch: 100; loss: 38.76; acc: 0.47
Batch: 120; loss: 31.72; acc: 0.44
Batch: 140; loss: 48.35; acc: 0.31
Val Epoch over. val_loss: 34.600144902611994; val_accuracy: 0.42167595541401276 

plots/subspace_training/lenet/2020-01-10 14:00:56/d_dim_500_lr_0.1_seed_1_epochs_30_batchsize_64
plots/subspace_training/lenet/2020-01-10 14:00:56/d_dim_XXXXX_lr_0.1_seed_1_epochs_30_batchsize_64
