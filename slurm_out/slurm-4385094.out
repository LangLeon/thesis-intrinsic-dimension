Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.8
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09630938619375229; val_accuracy: 0.9700437898089171 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.01; acc: 0.98
Val Epoch over. val_loss: 0.09351374800939849; val_accuracy: 0.9722332802547771 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05069396557037238; val_accuracy: 0.9856687898089171 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.05080676884360754; val_accuracy: 0.9856687898089171 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07336135493342284; val_accuracy: 0.9793988853503185 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05434372287931716; val_accuracy: 0.9861664012738853 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.95
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05118895034976066; val_accuracy: 0.988156847133758 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.97
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.6; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.95
Batch: 80; loss: 0.46; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.68; acc: 0.89
Batch: 140; loss: 0.96; acc: 0.92
Val Epoch over. val_loss: 0.510572793923176; val_accuracy: 0.9213773885350318 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.71; acc: 0.89
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04817530882965987; val_accuracy: 0.9873606687898089 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056230637227084226; val_accuracy: 0.9867635350318471 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04676991753327619; val_accuracy: 0.9892515923566879 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.63; acc: 0.94
Batch: 20; loss: 1.35; acc: 0.88
Batch: 40; loss: 1.23; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.92
Batch: 80; loss: 0.68; acc: 0.92
Batch: 100; loss: 1.17; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.82; acc: 0.88
Val Epoch over. val_loss: 0.7953890411147646; val_accuracy: 0.9063495222929936 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.07552684027298241; val_accuracy: 0.981687898089172 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05016215456424245; val_accuracy: 0.9903463375796179 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07002805467623814; val_accuracy: 0.98546974522293 

Epoch 16 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09470704292795461; val_accuracy: 0.9792993630573248 

Epoch 17 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05816338182824433; val_accuracy: 0.9894506369426752 

Epoch 18 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055582096764616146; val_accuracy: 0.9897492038216561 

Epoch 19 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055835222837271964; val_accuracy: 0.9895501592356688 

Epoch 20 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05227916464684116; val_accuracy: 0.9905453821656051 

Epoch 21 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0555506324644681; val_accuracy: 0.9906449044585988 

Epoch 22 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05724930241229428; val_accuracy: 0.9899482484076433 

Epoch 23 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05908736543860405; val_accuracy: 0.9897492038216561 

Epoch 24 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06031155515058785; val_accuracy: 0.9901472929936306 

Epoch 25 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06158643158947586; val_accuracy: 0.990047770700637 

Epoch 26 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061595250229546976; val_accuracy: 0.9901472929936306 

Epoch 27 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06262183485991636; val_accuracy: 0.9901472929936306 

Epoch 28 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06284316059700243; val_accuracy: 0.9897492038216561 

Epoch 29 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06402577146603043; val_accuracy: 0.9896496815286624 

Epoch 30 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06428283116050587; val_accuracy: 0.9901472929936306 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06437021285105662; val_accuracy: 0.990047770700637 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06556842627988499; val_accuracy: 0.9896496815286624 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06524783221020061; val_accuracy: 0.9902468152866242 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06619683487020481; val_accuracy: 0.9898487261146497 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0665940029226291; val_accuracy: 0.9898487261146497 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06672481827105686; val_accuracy: 0.9896496815286624 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06711245987825333; val_accuracy: 0.9899482484076433 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06745263753802913; val_accuracy: 0.9899482484076433 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06765736402220028; val_accuracy: 0.9901472929936306 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06811794448810018; val_accuracy: 0.9901472929936306 

Epoch 41 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06839675418320734; val_accuracy: 0.9899482484076433 

Epoch 42 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06858289711604452; val_accuracy: 0.990047770700637 

Epoch 43 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06914011588332; val_accuracy: 0.990047770700637 

Epoch 44 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06923324396465994; val_accuracy: 0.9898487261146497 

Epoch 45 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06943079392621472; val_accuracy: 0.990047770700637 

Epoch 46 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06962818006990822; val_accuracy: 0.990047770700637 

Epoch 47 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963075555054246; val_accuracy: 0.990047770700637 

Epoch 48 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06997010690771091; val_accuracy: 0.990047770700637 

Epoch 49 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07020801438647471; val_accuracy: 0.990047770700637 

Epoch 50 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.27; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07032824355136057; val_accuracy: 0.990047770700637 

plots/no_subspace_training/lenet/2020-01-18 17:41:45/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.8
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09188541637104787; val_accuracy: 0.971437101910828 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08584611402575378; val_accuracy: 0.9749203821656051 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05002452532766731; val_accuracy: 0.9851711783439491 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.04992515903417092; val_accuracy: 0.9856687898089171 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.13867099438408378; val_accuracy: 0.9625796178343949 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05686306903601452; val_accuracy: 0.9846735668789809 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058931451765405145; val_accuracy: 0.9868630573248408 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.91
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.07404106914712365; val_accuracy: 0.9803941082802548 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0505271738952702; val_accuracy: 0.9864649681528662 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05631911558852454; val_accuracy: 0.9872611464968153 

Epoch 11 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04700005548015521; val_accuracy: 0.9895501592356688 

Epoch 12 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049163402503083466; val_accuracy: 0.9885549363057324 

Epoch 13 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0701586435649805; val_accuracy: 0.9829816878980892 

Epoch 14 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053149726265554975; val_accuracy: 0.9887539808917197 

Epoch 15 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054789403678884935; val_accuracy: 0.9886544585987261 

Epoch 16 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052240688045313406; val_accuracy: 0.9892515923566879 

Epoch 17 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05446801813924389; val_accuracy: 0.9895501592356688 

Epoch 18 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06268174405310564; val_accuracy: 0.9875597133757962 

Epoch 19 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056938752437093454; val_accuracy: 0.9892515923566879 

Epoch 20 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05539572243667712; val_accuracy: 0.9904458598726115 

Epoch 21 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058320200035146846; val_accuracy: 0.9898487261146497 

Epoch 22 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060290450313288696; val_accuracy: 0.9894506369426752 

Epoch 23 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060990748701581526; val_accuracy: 0.990047770700637 

Epoch 24 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06131522964899707; val_accuracy: 0.9896496815286624 

Epoch 25 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06404556452658526; val_accuracy: 0.9894506369426752 

Epoch 26 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.3; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0636984102760151; val_accuracy: 0.9901472929936306 

Epoch 27 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06474930835757287; val_accuracy: 0.9897492038216561 

Epoch 28 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06443456156997923; val_accuracy: 0.9901472929936306 

Epoch 29 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06538750677351739; val_accuracy: 0.9899482484076433 

Epoch 30 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06623930412872582; val_accuracy: 0.9897492038216561 

Epoch 31 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06606903643744766; val_accuracy: 0.990047770700637 

Epoch 32 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06746603339720683; val_accuracy: 0.9898487261146497 

Epoch 33 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0670302117326457; val_accuracy: 0.9898487261146497 

Epoch 34 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06800607829147083; val_accuracy: 0.9897492038216561 

Epoch 35 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06847192546364608; val_accuracy: 0.9897492038216561 

Epoch 36 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06845791244013294; val_accuracy: 0.9899482484076433 

Epoch 37 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06898516407058497; val_accuracy: 0.9901472929936306 

Epoch 38 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06933096682379959; val_accuracy: 0.990047770700637 

Epoch 39 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06965566383805244; val_accuracy: 0.990047770700637 

Epoch 40 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07007440877188544; val_accuracy: 0.990047770700637 

Epoch 41 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07018790408304543; val_accuracy: 0.9898487261146497 

Epoch 42 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07028526585003372; val_accuracy: 0.9898487261146497 

Epoch 43 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0709212204548204; val_accuracy: 0.9898487261146497 

Epoch 44 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0709154355298182; val_accuracy: 0.9899482484076433 

Epoch 45 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0711762118776133; val_accuracy: 0.9899482484076433 

Epoch 46 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07140787652913172; val_accuracy: 0.9899482484076433 

Epoch 47 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07132463940199774; val_accuracy: 0.9899482484076433 

Epoch 48 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07170349867290753; val_accuracy: 0.9899482484076433 

Epoch 49 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.36; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07210366176381992; val_accuracy: 0.990047770700637 

Epoch 50 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.36; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0721795710788411; val_accuracy: 0.9899482484076433 

plots/no_subspace_training/lenet/2020-01-18 17:46:54/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.45
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.84; acc: 0.66
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.78
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.92
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09094215037336775; val_accuracy: 0.9718351910828026 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08539216697310946; val_accuracy: 0.9753184713375797 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05146535445645357; val_accuracy: 0.9849721337579618 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04686194856407916; val_accuracy: 0.9865644904458599 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10534878496294188; val_accuracy: 0.9709394904458599 

Epoch 6 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.95
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05094786351368685; val_accuracy: 0.9861664012738853 

Epoch 7 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05124287328617588; val_accuracy: 0.9877587579617835 

Epoch 8 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.05697568199911695; val_accuracy: 0.9848726114649682 

Epoch 9 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04740647933665355; val_accuracy: 0.9880573248407644 

Epoch 10 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047067827740862114; val_accuracy: 0.9882563694267515 

Epoch 11 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04749511614157136; val_accuracy: 0.9895501592356688 

Epoch 12 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0500884349843499; val_accuracy: 0.9893511146496815 

Epoch 13 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.32; acc: 0.95
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.06948257797652749; val_accuracy: 0.9848726114649682 

Epoch 14 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054741681618675306; val_accuracy: 0.9892515923566879 

Epoch 15 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05878218785402881; val_accuracy: 0.9878582802547771 

Epoch 16 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05640612637541097; val_accuracy: 0.9890525477707006 

Epoch 17 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05904663638893966; val_accuracy: 0.9888535031847133 

Epoch 18 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06236574234096867; val_accuracy: 0.9888535031847133 

Epoch 19 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05839329730173585; val_accuracy: 0.990047770700637 

Epoch 20 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06011328993329577; val_accuracy: 0.9893511146496815 

Epoch 21 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06094470985565975; val_accuracy: 0.9899482484076433 

Epoch 22 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06185547148535965; val_accuracy: 0.9892515923566879 

Epoch 23 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0629803750450444; val_accuracy: 0.9893511146496815 

Epoch 24 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06404344433811819; val_accuracy: 0.9894506369426752 

Epoch 25 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06540886141881821; val_accuracy: 0.988953025477707 

Epoch 26 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06553672695425665; val_accuracy: 0.9895501592356688 

Epoch 27 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06641282354760322; val_accuracy: 0.9895501592356688 

Epoch 28 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06613620361135264; val_accuracy: 0.9896496815286624 

Epoch 29 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06728184389270794; val_accuracy: 0.9896496815286624 

Epoch 30 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06700232904997601; val_accuracy: 0.9897492038216561 

Epoch 31 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06738903245348839; val_accuracy: 0.9895501592356688 

Epoch 32 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06868724904622242; val_accuracy: 0.9895501592356688 

Epoch 33 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06823636572452108; val_accuracy: 0.9895501592356688 

Epoch 34 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06904232146064188; val_accuracy: 0.9894506369426752 

Epoch 35 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06970225227106909; val_accuracy: 0.9891520700636943 

Epoch 36 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06918306283320591; val_accuracy: 0.9894506369426752 

Epoch 37 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06950260143560968; val_accuracy: 0.9893511146496815 

Epoch 38 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0700485065199767; val_accuracy: 0.9893511146496815 

Epoch 39 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07029446176472742; val_accuracy: 0.9894506369426752 

Epoch 40 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07057485311843786; val_accuracy: 0.9895501592356688 

Epoch 41 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07049583975866341; val_accuracy: 0.9894506369426752 

Epoch 42 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058076451348651; val_accuracy: 0.9895501592356688 

Epoch 43 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07126853681483845; val_accuracy: 0.9894506369426752 

Epoch 44 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07124451557352285; val_accuracy: 0.9894506369426752 

Epoch 45 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07136905406880531; val_accuracy: 0.9894506369426752 

Epoch 46 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07149533234584103; val_accuracy: 0.9894506369426752 

Epoch 47 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0712712631578658; val_accuracy: 0.9895501592356688 

Epoch 48 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07166448434826675; val_accuracy: 0.9895501592356688 

Epoch 49 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07204380973129515; val_accuracy: 0.9893511146496815 

Epoch 50 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07199589015951582; val_accuracy: 0.9895501592356688 

plots/no_subspace_training/lenet/2020-01-18 17:51:58/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.45
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.84; acc: 0.66
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.78
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.098361180110532; val_accuracy: 0.9693471337579618 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09133227492214009; val_accuracy: 0.9745222929936306 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05110419035005342; val_accuracy: 0.9846735668789809 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.04757119117269091; val_accuracy: 0.9871616242038217 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1385721588258151; val_accuracy: 0.9621815286624203 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.94
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055780729694161445; val_accuracy: 0.9852707006369427 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05283845398741163; val_accuracy: 0.9872611464968153 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.39; acc: 0.94
Val Epoch over. val_loss: 0.21435586792553307; val_accuracy: 0.9537221337579618 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048420074093303865; val_accuracy: 0.9868630573248408 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05107510099365453; val_accuracy: 0.9879578025477707 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048768341161642866; val_accuracy: 0.9882563694267515 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08573627130241152; val_accuracy: 0.9812898089171974 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06145813318479593; val_accuracy: 0.9851711783439491 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05127480545431186; val_accuracy: 0.9882563694267515 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05651213883594343; val_accuracy: 0.9885549363057324 

Epoch 16 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05505897242362332; val_accuracy: 0.9887539808917197 

Epoch 17 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05567151056543277; val_accuracy: 0.9894506369426752 

Epoch 18 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0553402068793394; val_accuracy: 0.9895501592356688 

Epoch 19 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056712480392425685; val_accuracy: 0.9897492038216561 

Epoch 20 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05699058826182299; val_accuracy: 0.9902468152866242 

Epoch 21 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05841160921534155; val_accuracy: 0.990047770700637 

Epoch 22 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05915524217353505; val_accuracy: 0.990047770700637 

Epoch 23 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06011441449640663; val_accuracy: 0.9902468152866242 

Epoch 24 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06120245917967171; val_accuracy: 0.9899482484076433 

Epoch 25 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06223739954126868; val_accuracy: 0.9901472929936306 

Epoch 26 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06220646753052997; val_accuracy: 0.9901472929936306 

Epoch 27 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0633910740162157; val_accuracy: 0.990047770700637 

Epoch 28 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06394497708530184; val_accuracy: 0.9897492038216561 

Epoch 29 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0652174942527607; val_accuracy: 0.9895501592356688 

Epoch 30 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06504225142442496; val_accuracy: 0.9901472929936306 

Epoch 31 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06498265366075905; val_accuracy: 0.9901472929936306 

Epoch 32 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06567387145226168; val_accuracy: 0.9901472929936306 

Epoch 33 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06542715529917152; val_accuracy: 0.9901472929936306 

Epoch 34 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06601178218984301; val_accuracy: 0.9903463375796179 

Epoch 35 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0662703812122345; val_accuracy: 0.9902468152866242 

Epoch 36 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06635508766979169; val_accuracy: 0.9902468152866242 

Epoch 37 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06656397371345264; val_accuracy: 0.9903463375796179 

Epoch 38 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06671824315740804; val_accuracy: 0.9902468152866242 

Epoch 39 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06695084046026703; val_accuracy: 0.9901472929936306 

Epoch 40 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06718031264794101; val_accuracy: 0.9904458598726115 

Epoch 41 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06724840464295855; val_accuracy: 0.9901472929936306 

Epoch 42 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06732571860597392; val_accuracy: 0.9901472929936306 

Epoch 43 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06773380308773867; val_accuracy: 0.9902468152866242 

Epoch 44 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06790004423849143; val_accuracy: 0.990047770700637 

Epoch 45 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0680844757682199; val_accuracy: 0.9902468152866242 

Epoch 46 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06808345361500029; val_accuracy: 0.9902468152866242 

Epoch 47 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06806294249880845; val_accuracy: 0.9902468152866242 

Epoch 48 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06816545442031448; val_accuracy: 0.9902468152866242 

Epoch 49 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06827229361055763; val_accuracy: 0.9902468152866242 

Epoch 50 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06828253744703949; val_accuracy: 0.9902468152866242 

plots/no_subspace_training/lenet/2020-01-18 17:57:01/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.45; acc: 0.8
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11057562264762107; val_accuracy: 0.966859076433121 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1054704306991237; val_accuracy: 0.9682523885350318 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04968511068801971; val_accuracy: 0.98546974522293 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04894842117265531; val_accuracy: 0.9861664012738853 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.92; acc: 0.84
Batch: 80; loss: 0.98; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.5001622555884206; val_accuracy: 0.8903264331210191 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05247345749454893; val_accuracy: 0.9864649681528662 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053249449036113776; val_accuracy: 0.9871616242038217 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.07910101832288086; val_accuracy: 0.978702229299363 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05156747569703752; val_accuracy: 0.9864649681528662 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055229183760987725; val_accuracy: 0.9874601910828026 

Epoch 11 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04420263434101822; val_accuracy: 0.9898487261146497 

Epoch 12 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04477715390218291; val_accuracy: 0.9897492038216561 

Epoch 13 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05150613634829308; val_accuracy: 0.988953025477707 

Epoch 14 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04878559376403784; val_accuracy: 0.9895501592356688 

Epoch 15 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05254436735134975; val_accuracy: 0.9891520700636943 

Epoch 16 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04996462631377445; val_accuracy: 0.9893511146496815 

Epoch 17 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05364622460429076; val_accuracy: 0.9890525477707006 

Epoch 18 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05382016519452356; val_accuracy: 0.9894506369426752 

Epoch 19 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05419162727275472; val_accuracy: 0.9903463375796179 

Epoch 20 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054366814767479135; val_accuracy: 0.990047770700637 

Epoch 21 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05513445003207322; val_accuracy: 0.9899482484076433 

Epoch 22 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055900830514491746; val_accuracy: 0.9897492038216561 

Epoch 23 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056268678634030046; val_accuracy: 0.9899482484076433 

Epoch 24 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05652907362599282; val_accuracy: 0.9899482484076433 

Epoch 25 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05725753217176267; val_accuracy: 0.9897492038216561 

Epoch 26 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057387537732245815; val_accuracy: 0.9897492038216561 

Epoch 27 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05801233037072382; val_accuracy: 0.9896496815286624 

Epoch 28 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057968502781193726; val_accuracy: 0.9897492038216561 

Epoch 29 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05872774532266483; val_accuracy: 0.9897492038216561 

Epoch 30 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058640692596602595; val_accuracy: 0.9897492038216561 

Epoch 31 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05843199780032893; val_accuracy: 0.9899482484076433 

Epoch 32 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05901666991649919; val_accuracy: 0.9897492038216561 

Epoch 33 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05882796730585159; val_accuracy: 0.9898487261146497 

Epoch 34 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05915305711281527; val_accuracy: 0.9898487261146497 

Epoch 35 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05936056123987125; val_accuracy: 0.9897492038216561 

Epoch 36 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059310231998467904; val_accuracy: 0.9898487261146497 

Epoch 37 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05943325541581318; val_accuracy: 0.9899482484076433 

Epoch 38 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059553398732926435; val_accuracy: 0.9898487261146497 

Epoch 39 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05967959457901633; val_accuracy: 0.9898487261146497 

Epoch 40 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05987417209110442; val_accuracy: 0.9898487261146497 

Epoch 41 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059889291739387875; val_accuracy: 0.9898487261146497 

Epoch 42 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05989664427603886; val_accuracy: 0.9898487261146497 

Epoch 43 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05994037681134643; val_accuracy: 0.9898487261146497 

Epoch 44 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06002117370723919; val_accuracy: 0.9898487261146497 

Epoch 45 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06004601403786119; val_accuracy: 0.9898487261146497 

Epoch 46 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060108658472064196; val_accuracy: 0.9898487261146497 

Epoch 47 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06005283060726846; val_accuracy: 0.9898487261146497 

Epoch 48 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06013858688485091; val_accuracy: 0.9898487261146497 

Epoch 49 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06022922277070914; val_accuracy: 0.9898487261146497 

Epoch 50 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060271308964984435; val_accuracy: 0.9898487261146497 

plots/no_subspace_training/lenet/2020-01-18 18:02:06/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.12; acc: 0.59
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.7; acc: 0.7
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.45; acc: 0.8
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0960073244941842; val_accuracy: 0.9700437898089171 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07507959043808804; val_accuracy: 0.9775079617834395 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05135295392981001; val_accuracy: 0.9853702229299363 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04867999538019964; val_accuracy: 0.9864649681528662 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05545932832796862; val_accuracy: 0.9840764331210191 

Epoch 6 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04403304187640263; val_accuracy: 0.9885549363057324 

Epoch 7 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04586553730212959; val_accuracy: 0.9893511146496815 

Epoch 8 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04780323815288817; val_accuracy: 0.988156847133758 

Epoch 9 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047363469460207946; val_accuracy: 0.9888535031847133 

Epoch 10 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047450215217604; val_accuracy: 0.9890525477707006 

Epoch 11 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04807712601580817; val_accuracy: 0.9897492038216561 

Epoch 12 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04859491432927976; val_accuracy: 0.9896496815286624 

Epoch 13 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05065668195153877; val_accuracy: 0.9897492038216561 

Epoch 14 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05084564750361594; val_accuracy: 0.9894506369426752 

Epoch 15 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05278706334673675; val_accuracy: 0.988953025477707 

Epoch 16 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05082486994611989; val_accuracy: 0.9895501592356688 

Epoch 17 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05217896342562262; val_accuracy: 0.9896496815286624 

Epoch 18 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05224763056275192; val_accuracy: 0.9896496815286624 

Epoch 19 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052789741308446145; val_accuracy: 0.9894506369426752 

Epoch 20 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052473030723394105; val_accuracy: 0.9895501592356688 

Epoch 21 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05271925098577123; val_accuracy: 0.9894506369426752 

Epoch 22 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0530789292826774; val_accuracy: 0.9897492038216561 

Epoch 23 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053204916417598724; val_accuracy: 0.9897492038216561 

Epoch 24 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053286312705582116; val_accuracy: 0.9894506369426752 

Epoch 25 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053455466558788994; val_accuracy: 0.9895501592356688 

Epoch 26 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05356978881321135; val_accuracy: 0.9895501592356688 

Epoch 27 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053688040964163034; val_accuracy: 0.9896496815286624 

Epoch 28 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05372813253835508; val_accuracy: 0.9896496815286624 

Epoch 29 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053774392196706905; val_accuracy: 0.9896496815286624 

Epoch 30 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05380101902944267; val_accuracy: 0.9895501592356688 

Epoch 31 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05382640992000604; val_accuracy: 0.9895501592356688 

Epoch 32 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05386556058552614; val_accuracy: 0.9896496815286624 

Epoch 33 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05388326976139834; val_accuracy: 0.9896496815286624 

Epoch 34 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05390624870445318; val_accuracy: 0.9896496815286624 

Epoch 35 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0539345412402396; val_accuracy: 0.9896496815286624 

Epoch 36 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053942178963286104; val_accuracy: 0.9896496815286624 

Epoch 37 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0539499803618261; val_accuracy: 0.9896496815286624 

Epoch 38 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053959193408109576; val_accuracy: 0.9896496815286624 

Epoch 39 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053967268674806425; val_accuracy: 0.9896496815286624 

Epoch 40 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053974468901658516; val_accuracy: 0.9896496815286624 

Epoch 41 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05397738070245002; val_accuracy: 0.9896496815286624 

Epoch 42 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053980440779286584; val_accuracy: 0.9896496815286624 

Epoch 43 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05398403874533192; val_accuracy: 0.9896496815286624 

Epoch 44 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053986592704703094; val_accuracy: 0.9896496815286624 

Epoch 45 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053989451236216124; val_accuracy: 0.9896496815286624 

Epoch 46 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053990460172959956; val_accuracy: 0.9896496815286624 

Epoch 47 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053991614301113565; val_accuracy: 0.9896496815286624 

Epoch 48 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05399269423192474; val_accuracy: 0.9896496815286624 

Epoch 49 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053993690616575774; val_accuracy: 0.9896496815286624 

Epoch 50 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05399478428587792; val_accuracy: 0.9896496815286624 

plots/no_subspace_training/lenet/2020-01-18 18:07:12/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.29; acc: 0.45
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.44; acc: 0.83
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10206926758786675; val_accuracy: 0.9697452229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07185592718280046; val_accuracy: 0.9783041401273885 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052383163599831285; val_accuracy: 0.9844745222929936 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 0.98
Val Epoch over. val_loss: 0.05355025120791356; val_accuracy: 0.9856687898089171 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08825133649549287; val_accuracy: 0.9735270700636943 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054249805321169506; val_accuracy: 0.98546974522293 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.95
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04766050673973788; val_accuracy: 0.9892515923566879 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.1299762045905279; val_accuracy: 0.9658638535031847 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048858415074409196; val_accuracy: 0.9869625796178344 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04666982652844897; val_accuracy: 0.9890525477707006 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047908033772259; val_accuracy: 0.9892515923566879 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 1.11; acc: 0.88
Batch: 20; loss: 2.08; acc: 0.83
Batch: 40; loss: 1.91; acc: 0.81
Batch: 60; loss: 1.22; acc: 0.86
Batch: 80; loss: 1.25; acc: 0.92
Batch: 100; loss: 1.86; acc: 0.81
Batch: 120; loss: 1.31; acc: 0.78
Batch: 140; loss: 1.59; acc: 0.83
Val Epoch over. val_loss: 1.3144027530957179; val_accuracy: 0.859375 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058223578628081425; val_accuracy: 0.9860668789808917 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05248887091875076; val_accuracy: 0.9886544585987261 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05615662897278549; val_accuracy: 0.9893511146496815 

Epoch 16 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04904167014797022; val_accuracy: 0.9903463375796179 

Epoch 17 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05149700029925176; val_accuracy: 0.990843949044586 

Epoch 18 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051703249288212724; val_accuracy: 0.9909434713375797 

Epoch 19 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05323161920354624; val_accuracy: 0.9904458598726115 

Epoch 20 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05333384007784971; val_accuracy: 0.9911425159235668 

Epoch 21 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05401320632096309; val_accuracy: 0.9910429936305732 

Epoch 22 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05497155684954042; val_accuracy: 0.9910429936305732 

Epoch 23 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055564689218618304; val_accuracy: 0.9910429936305732 

Epoch 24 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05619727203230949; val_accuracy: 0.9909434713375797 

Epoch 25 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05703064115943423; val_accuracy: 0.9909434713375797 

Epoch 26 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057399713546986794; val_accuracy: 0.9909434713375797 

Epoch 27 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05813633897312128; val_accuracy: 0.9909434713375797 

Epoch 28 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05802395911353409; val_accuracy: 0.990843949044586 

Epoch 29 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05928572305258672; val_accuracy: 0.9909434713375797 

Epoch 30 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05922054475659778; val_accuracy: 0.9906449044585988 

Epoch 31 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05916395944774531; val_accuracy: 0.9907444267515924 

Epoch 32 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05937018620360429; val_accuracy: 0.990843949044586 

Epoch 33 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05935228141439948; val_accuracy: 0.990843949044586 

Epoch 34 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05951983135217314; val_accuracy: 0.990843949044586 

Epoch 35 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05963867584801024; val_accuracy: 0.990843949044586 

Epoch 36 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05970822293667277; val_accuracy: 0.990843949044586 

Epoch 37 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05979806616617616; val_accuracy: 0.990843949044586 

Epoch 38 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0598629800377378; val_accuracy: 0.990843949044586 

Epoch 39 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05996884956101703; val_accuracy: 0.990843949044586 

Epoch 40 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060109432857887; val_accuracy: 0.990843949044586 

Epoch 41 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06017052524598541; val_accuracy: 0.990843949044586 

Epoch 42 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06019341955139379; val_accuracy: 0.990843949044586 

Epoch 43 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060320412989254966; val_accuracy: 0.990843949044586 

Epoch 44 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06045565175212872; val_accuracy: 0.990843949044586 

Epoch 45 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060555530818784314; val_accuracy: 0.990843949044586 

Epoch 46 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0605594349230171; val_accuracy: 0.990843949044586 

Epoch 47 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060558932317290336; val_accuracy: 0.990843949044586 

Epoch 48 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06056722519314213; val_accuracy: 0.990843949044586 

Epoch 49 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06058193638825872; val_accuracy: 0.990843949044586 

Epoch 50 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06059136070833085; val_accuracy: 0.990843949044586 

plots/no_subspace_training/lenet/2020-01-18 18:12:14/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.45
Batch: 80; loss: 0.71; acc: 0.75
Batch: 100; loss: 0.68; acc: 0.67
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10057770472708022; val_accuracy: 0.9688495222929936 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.06669976336845926; val_accuracy: 0.9807921974522293 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04994974363666431; val_accuracy: 0.9850716560509554 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.05217982864209041; val_accuracy: 0.9846735668789809 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10325411134132534; val_accuracy: 0.9708399681528662 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057294780329154554; val_accuracy: 0.9836783439490446 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04955756996467615; val_accuracy: 0.9871616242038217 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.15989952807308763; val_accuracy: 0.9626791401273885 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04646250368303554; val_accuracy: 0.9886544585987261 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04945956229878839; val_accuracy: 0.9879578025477707 

Epoch 11 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04075070041095375; val_accuracy: 0.9904458598726115 

Epoch 12 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04205894913927764; val_accuracy: 0.9906449044585988 

Epoch 13 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04428406422779818; val_accuracy: 0.9901472929936306 

Epoch 14 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04484255611896515; val_accuracy: 0.9904458598726115 

Epoch 15 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04644542056948516; val_accuracy: 0.990047770700637 

Epoch 16 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624090970132002; val_accuracy: 0.9901472929936306 

Epoch 17 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048166209582690224; val_accuracy: 0.990047770700637 

Epoch 18 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04824380870837315; val_accuracy: 0.9905453821656051 

Epoch 19 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04932896892546089; val_accuracy: 0.9899482484076433 

Epoch 20 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048905788618288225; val_accuracy: 0.9904458598726115 

Epoch 21 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0492410909408217; val_accuracy: 0.9903463375796179 

Epoch 22 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049516801384224256; val_accuracy: 0.9905453821656051 

Epoch 23 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049661414068975264; val_accuracy: 0.9902468152866242 

Epoch 24 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049770722296207576; val_accuracy: 0.9904458598726115 

Epoch 25 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04994837422469619; val_accuracy: 0.9902468152866242 

Epoch 26 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05010587271231755; val_accuracy: 0.9905453821656051 

Epoch 27 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05032021661472928; val_accuracy: 0.9902468152866242 

Epoch 28 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0504114267647646; val_accuracy: 0.9903463375796179 

Epoch 29 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05062058957139398; val_accuracy: 0.9902468152866242 

Epoch 30 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05065797791359531; val_accuracy: 0.9904458598726115 

Epoch 31 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05068661130158005; val_accuracy: 0.9903463375796179 

Epoch 32 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05072786692221453; val_accuracy: 0.9902468152866242 

Epoch 33 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050755843093061144; val_accuracy: 0.9903463375796179 

Epoch 34 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05078801797453765; val_accuracy: 0.9903463375796179 

Epoch 35 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050816005961910174; val_accuracy: 0.9903463375796179 

Epoch 36 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05083807704934649; val_accuracy: 0.9903463375796179 

Epoch 37 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05086134839209781; val_accuracy: 0.9903463375796179 

Epoch 38 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050896844476651236; val_accuracy: 0.9904458598726115 

Epoch 39 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05091750859075291; val_accuracy: 0.9904458598726115 

Epoch 40 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05094080665119135; val_accuracy: 0.9904458598726115 

Epoch 41 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05094589008267518; val_accuracy: 0.9904458598726115 

Epoch 42 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05095106239911098; val_accuracy: 0.9904458598726115 

Epoch 43 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05095657483217823; val_accuracy: 0.9904458598726115 

Epoch 44 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0509613876699642; val_accuracy: 0.9904458598726115 

Epoch 45 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050966620113059975; val_accuracy: 0.9904458598726115 

Epoch 46 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050971597005986864; val_accuracy: 0.9904458598726115 

Epoch 47 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0509766393406376; val_accuracy: 0.9904458598726115 

Epoch 48 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05098134867704598; val_accuracy: 0.9904458598726115 

Epoch 49 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05098679723443499; val_accuracy: 0.9904458598726115 

Epoch 50 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05099102959131739; val_accuracy: 0.9904458598726115 

plots/no_subspace_training/lenet/2020-01-18 18:17:21/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.8
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10037637964650324; val_accuracy: 0.9689490445859873 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09103648760800909; val_accuracy: 0.9730294585987261 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05022262893379874; val_accuracy: 0.9846735668789809 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.05087989300584338; val_accuracy: 0.98546974522293 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11332606540838625; val_accuracy: 0.9695461783439491 

Epoch 6 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.01; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.03995600212246749; val_accuracy: 0.9890525477707006 

Epoch 7 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04040217603657656; val_accuracy: 0.9898487261146497 

Epoch 8 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.043174526019460834; val_accuracy: 0.988953025477707 

Epoch 9 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0413184970807118; val_accuracy: 0.9895501592356688 

Epoch 10 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0428593197161225; val_accuracy: 0.9894506369426752 

Epoch 11 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04216838060955333; val_accuracy: 0.9895501592356688 

Epoch 12 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0427132872212085; val_accuracy: 0.9896496815286624 

Epoch 13 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04268994450474241; val_accuracy: 0.9894506369426752 

Epoch 14 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04279525836656807; val_accuracy: 0.9896496815286624 

Epoch 15 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0431118709551301; val_accuracy: 0.9894506369426752 

Epoch 16 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0429881552269884; val_accuracy: 0.9897492038216561 

Epoch 17 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04302630874856263; val_accuracy: 0.9896496815286624 

Epoch 18 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04304517537450335; val_accuracy: 0.9896496815286624 

Epoch 19 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04309046242362375; val_accuracy: 0.9895501592356688 

Epoch 20 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04312968778477353; val_accuracy: 0.9895501592356688 

Epoch 21 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04313968285728412; val_accuracy: 0.9895501592356688 

Epoch 22 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.043152340112408254; val_accuracy: 0.9895501592356688 

Epoch 23 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04316268702316436; val_accuracy: 0.9895501592356688 

Epoch 24 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.043170989224105887; val_accuracy: 0.9895501592356688 

Epoch 25 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.043180729149822974; val_accuracy: 0.9895501592356688 

Epoch 26 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04318267508012474; val_accuracy: 0.9895501592356688 

Epoch 27 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04318461500725169; val_accuracy: 0.9895501592356688 

Epoch 28 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04318728376251117; val_accuracy: 0.9895501592356688 

Epoch 29 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04318915850892188; val_accuracy: 0.9895501592356688 

Epoch 30 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.043191060708586575; val_accuracy: 0.9895501592356688 

Epoch 31 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319130899799857; val_accuracy: 0.9895501592356688 

Epoch 32 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319157059879819; val_accuracy: 0.9895501592356688 

Epoch 33 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319194490742532; val_accuracy: 0.9895501592356688 

Epoch 34 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319220316258206; val_accuracy: 0.9895501592356688 

Epoch 35 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319240928740258; val_accuracy: 0.9895501592356688 

Epoch 36 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319239365067452; val_accuracy: 0.9895501592356688 

Epoch 37 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319238816951491; val_accuracy: 0.9895501592356688 

Epoch 38 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319238041047078; val_accuracy: 0.9895501592356688 

Epoch 39 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319238691193283; val_accuracy: 0.9895501592356688 

Epoch 40 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319238774241156; val_accuracy: 0.9895501592356688 

Epoch 41 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319238043419874; val_accuracy: 0.9895501592356688 

Epoch 42 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.043192371797219964; val_accuracy: 0.9895501592356688 

Epoch 43 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319236128573205; val_accuracy: 0.9895501592356688 

Epoch 44 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319236012306183; val_accuracy: 0.9895501592356688 

Epoch 45 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.043192352743665126; val_accuracy: 0.9895501592356688 

Epoch 46 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319235136744323; val_accuracy: 0.9895501592356688 

Epoch 47 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319235134371527; val_accuracy: 0.9895501592356688 

Epoch 48 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.043192347974344424; val_accuracy: 0.9895501592356688 

Epoch 49 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0431923478557046; val_accuracy: 0.9895501592356688 

Epoch 50 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04319234773706479; val_accuracy: 0.9895501592356688 

plots/no_subspace_training/lenet/2020-01-18 18:22:26/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.45
Batch: 80; loss: 0.71; acc: 0.75
Batch: 100; loss: 0.68; acc: 0.67
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10783821313529257; val_accuracy: 0.9677547770700637 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.06858237472119605; val_accuracy: 0.9799960191082803 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04925798555942857; val_accuracy: 0.9846735668789809 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.04792220499010602; val_accuracy: 0.9860668789808917 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07263781231157719; val_accuracy: 0.9790007961783439 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05336904397625832; val_accuracy: 0.9865644904458599 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048250102360916744; val_accuracy: 0.9882563694267515 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.95
Val Epoch over. val_loss: 0.1605607682874628; val_accuracy: 0.9634753184713376 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04537577661359386; val_accuracy: 0.9877587579617835 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.97
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05158384298537947; val_accuracy: 0.9858678343949044 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04676641851283942; val_accuracy: 0.9887539808917197 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056213624988961375; val_accuracy: 0.9878582802547771 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07190575590653783; val_accuracy: 0.9820859872611465 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04867704820101428; val_accuracy: 0.9891520700636943 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051857485845210445; val_accuracy: 0.9892515923566879 

Epoch 16 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04970217085662921; val_accuracy: 0.9899482484076433 

Epoch 17 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05098077574163486; val_accuracy: 0.9897492038216561 

Epoch 18 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05041645652359458; val_accuracy: 0.9899482484076433 

Epoch 19 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050833665546338266; val_accuracy: 0.9898487261146497 

Epoch 20 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05084346443604512; val_accuracy: 0.9899482484076433 

Epoch 21 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0509187533597278; val_accuracy: 0.9901472929936306 

Epoch 22 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051577740414127424; val_accuracy: 0.990047770700637 

Epoch 23 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0517825432548857; val_accuracy: 0.9901472929936306 

Epoch 24 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051981141755155696; val_accuracy: 0.9903463375796179 

Epoch 25 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05233995189332658; val_accuracy: 0.9902468152866242 

Epoch 26 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05268654654360121; val_accuracy: 0.990047770700637 

Epoch 27 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05307288192639685; val_accuracy: 0.990047770700637 

Epoch 28 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053168787555709765; val_accuracy: 0.9902468152866242 

Epoch 29 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053621285233148346; val_accuracy: 0.9902468152866242 

Epoch 30 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05364389897911412; val_accuracy: 0.9902468152866242 

Epoch 31 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053650963505741896; val_accuracy: 0.9902468152866242 

Epoch 32 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05369298710564899; val_accuracy: 0.9902468152866242 

Epoch 33 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053717617586160164; val_accuracy: 0.9902468152866242 

Epoch 34 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0537571671187498; val_accuracy: 0.9901472929936306 

Epoch 35 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053792256743285305; val_accuracy: 0.9901472929936306 

Epoch 36 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05381388392797701; val_accuracy: 0.9901472929936306 

Epoch 37 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05384675579465878; val_accuracy: 0.9901472929936306 

Epoch 38 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05388556942818271; val_accuracy: 0.9901472929936306 

Epoch 39 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05392229333994495; val_accuracy: 0.9901472929936306 

Epoch 40 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05395886235556025; val_accuracy: 0.9901472929936306 

Epoch 41 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0539862627436401; val_accuracy: 0.9901472929936306 

Epoch 42 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05400958752176564; val_accuracy: 0.9901472929936306 

Epoch 43 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05404839413181232; val_accuracy: 0.9901472929936306 

Epoch 44 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05408177472603549; val_accuracy: 0.9901472929936306 

Epoch 45 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05411856744889241; val_accuracy: 0.9901472929936306 

Epoch 46 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05412122256057278; val_accuracy: 0.9901472929936306 

Epoch 47 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05412387700787016; val_accuracy: 0.9901472929936306 

Epoch 48 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054126695083205105; val_accuracy: 0.9901472929936306 

Epoch 49 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054130027058777536; val_accuracy: 0.9902468152866242 

Epoch 50 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05413354563105638; val_accuracy: 0.9902468152866242 

plots/no_subspace_training/lenet/2020-01-18 18:27:31/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 0.97; acc: 0.66
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 0.6; acc: 0.75
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.8
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09360927219983119; val_accuracy: 0.9702428343949044 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.0862618404304146; val_accuracy: 0.9748208598726115 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.95
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05483054687642747; val_accuracy: 0.9841759554140127 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051437971935530374; val_accuracy: 0.986265923566879 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10465798121254155; val_accuracy: 0.9713375796178344 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05770246202873576; val_accuracy: 0.9841759554140127 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052384925258767076; val_accuracy: 0.9871616242038217 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.1214020153044895; val_accuracy: 0.9688495222929936 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04788123450841114; val_accuracy: 0.9887539808917197 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053973958679255404; val_accuracy: 0.9864649681528662 

Epoch 11 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04249742259360423; val_accuracy: 0.9902468152866242 

Epoch 12 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04349307231842332; val_accuracy: 0.9903463375796179 

Epoch 13 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0450844653425323; val_accuracy: 0.9906449044585988 

Epoch 14 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04559567504248042; val_accuracy: 0.9899482484076433 

Epoch 15 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04709056292654602; val_accuracy: 0.9894506369426752 

Epoch 16 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04678167922386698; val_accuracy: 0.9898487261146497 

Epoch 17 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04835150217649284; val_accuracy: 0.9896496815286624 

Epoch 18 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0486106795917271; val_accuracy: 0.990047770700637 

Epoch 19 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049622817451407196; val_accuracy: 0.9898487261146497 

Epoch 20 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04930367147561851; val_accuracy: 0.9897492038216561 

Epoch 21 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049594151484928316; val_accuracy: 0.9897492038216561 

Epoch 22 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04981273770996719; val_accuracy: 0.9897492038216561 

Epoch 23 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04997047240946703; val_accuracy: 0.9897492038216561 

Epoch 24 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05005453970686645; val_accuracy: 0.9898487261146497 

Epoch 25 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05016534168059659; val_accuracy: 0.9898487261146497 

Epoch 26 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05025542992505298; val_accuracy: 0.9898487261146497 

Epoch 27 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05039103611544439; val_accuracy: 0.9897492038216561 

Epoch 28 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05044557073503543; val_accuracy: 0.9897492038216561 

Epoch 29 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05055670848317966; val_accuracy: 0.9897492038216561 

Epoch 30 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050610703505148555; val_accuracy: 0.9898487261146497 

Epoch 31 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05061805321817185; val_accuracy: 0.9898487261146497 

Epoch 32 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05062758889357755; val_accuracy: 0.9898487261146497 

Epoch 33 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05063624535301688; val_accuracy: 0.9897492038216561 

Epoch 34 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05064400515643654; val_accuracy: 0.9897492038216561 

Epoch 35 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05065157632254491; val_accuracy: 0.9897492038216561 

Epoch 36 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05066003396537653; val_accuracy: 0.9897492038216561 

Epoch 37 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050667051249628614; val_accuracy: 0.9897492038216561 

Epoch 38 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05067564653837757; val_accuracy: 0.9897492038216561 

Epoch 39 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05068342228698882; val_accuracy: 0.9897492038216561 

Epoch 40 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05069259732107448; val_accuracy: 0.9897492038216561 

Epoch 41 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05069355619181493; val_accuracy: 0.9897492038216561 

Epoch 42 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050694501869807576; val_accuracy: 0.9897492038216561 

Epoch 43 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05069553840217317; val_accuracy: 0.9897492038216561 

Epoch 44 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05069645442021121; val_accuracy: 0.9897492038216561 

Epoch 45 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0506974564521176; val_accuracy: 0.9897492038216561 

Epoch 46 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05069834079332412; val_accuracy: 0.9897492038216561 

Epoch 47 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05069921766022208; val_accuracy: 0.9897492038216561 

Epoch 48 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05070030470942236; val_accuracy: 0.9897492038216561 

Epoch 49 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05070125510927978; val_accuracy: 0.9897492038216561 

Epoch 50 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050702267059475; val_accuracy: 0.9897492038216561 

plots/no_subspace_training/lenet/2020-01-18 18:32:32/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.45
Batch: 80; loss: 0.7; acc: 0.75
Batch: 100; loss: 0.52; acc: 0.8
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.81
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0997770981281806; val_accuracy: 0.9697452229299363 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.0694515258548366; val_accuracy: 0.9799960191082803 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05137571960592725; val_accuracy: 0.9848726114649682 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04915801414353832; val_accuracy: 0.9855692675159236 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.18966498558118844; val_accuracy: 0.9523288216560509 

Epoch 6 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.040127446863112176; val_accuracy: 0.9892515923566879 

Epoch 7 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.03983567626612961; val_accuracy: 0.9898487261146497 

Epoch 8 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04168977756883688; val_accuracy: 0.9893511146496815 

Epoch 9 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04045883203103284; val_accuracy: 0.9896496815286624 

Epoch 10 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04199401184821584; val_accuracy: 0.9896496815286624 

Epoch 11 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041380879558195735; val_accuracy: 0.9896496815286624 

Epoch 12 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04139538152012855; val_accuracy: 0.9896496815286624 

Epoch 13 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04135800689269023; val_accuracy: 0.9897492038216561 

Epoch 14 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04140274568348174; val_accuracy: 0.9896496815286624 

Epoch 15 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04159126072457642; val_accuracy: 0.9895501592356688 

Epoch 16 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04158905241045223; val_accuracy: 0.9896496815286624 

Epoch 17 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04159214115066893; val_accuracy: 0.9896496815286624 

Epoch 18 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04159833127810697; val_accuracy: 0.9896496815286624 

Epoch 19 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041605653442964435; val_accuracy: 0.9896496815286624 

Epoch 20 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161182304571388; val_accuracy: 0.9896496815286624 

Epoch 21 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041612605048213035; val_accuracy: 0.9896496815286624 

Epoch 22 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161352386615079; val_accuracy: 0.9896496815286624 

Epoch 23 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161435783289041; val_accuracy: 0.9896496815286624 

Epoch 24 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161521168366359; val_accuracy: 0.9896496815286624 

Epoch 25 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161596381265646; val_accuracy: 0.9896496815286624 

Epoch 26 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041615984598352654; val_accuracy: 0.9896496815286624 

Epoch 27 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041616022159719164; val_accuracy: 0.9896496815286624 

Epoch 28 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041616111614142255; val_accuracy: 0.9896496815286624 

Epoch 29 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161611526824866; val_accuracy: 0.9896496815286624 

Epoch 30 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161611961046602; val_accuracy: 0.9896496815286624 

Epoch 31 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041616082310107105; val_accuracy: 0.9896496815286624 

Epoch 32 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041616044962292265; val_accuracy: 0.9896496815286624 

Epoch 33 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161601383120391; val_accuracy: 0.9896496815286624 

Epoch 34 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161598111034199; val_accuracy: 0.9896496815286624 

Epoch 35 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041615930332499704; val_accuracy: 0.9896496815286624 

Epoch 36 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041615929430837084; val_accuracy: 0.9896496815286624 

Epoch 37 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592629874588; val_accuracy: 0.9896496815286624 

Epoch 38 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.97
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592553945104; val_accuracy: 0.9896496815286624 

Epoch 39 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592181416074; val_accuracy: 0.9896496815286624 

Epoch 40 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592039048292; val_accuracy: 0.9896496815286624 

Epoch 41 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041615920271843104; val_accuracy: 0.9896496815286624 

Epoch 42 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592069894645; val_accuracy: 0.9896496815286624 

Epoch 43 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592074640238; val_accuracy: 0.9896496815286624 

Epoch 44 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592195652852; val_accuracy: 0.9896496815286624 

Epoch 45 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592107859387; val_accuracy: 0.9896496815286624 

Epoch 46 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592103113794; val_accuracy: 0.9896496815286624 

Epoch 47 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592155315314; val_accuracy: 0.9896496815286624 

Epoch 48 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041615921648065; val_accuracy: 0.9896496815286624 

Epoch 49 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041615921648065; val_accuracy: 0.9896496815286624 

Epoch 50 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04161592150569721; val_accuracy: 0.9896496815286624 

plots/no_subspace_training/lenet/2020-01-18 18:37:36/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 0.75; acc: 0.78
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09503182230196941; val_accuracy: 0.9705414012738853 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.09186367669208034; val_accuracy: 0.9736265923566879 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05136103292179715; val_accuracy: 0.9848726114649682 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.05069636689961716; val_accuracy: 0.9861664012738853 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.72; acc: 0.88
Batch: 80; loss: 0.76; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.3650816681136371; val_accuracy: 0.9132165605095541 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060093784621756546; val_accuracy: 0.9834792993630573 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04905920644189901; val_accuracy: 0.9884554140127388 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.06125696940691608; val_accuracy: 0.9833797770700637 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05005508454836857; val_accuracy: 0.9871616242038217 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.92
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05108839878515833; val_accuracy: 0.9872611464968153 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05165619165844219; val_accuracy: 0.9872611464968153 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 1.0; acc: 0.92
Batch: 20; loss: 2.2; acc: 0.84
Batch: 40; loss: 1.89; acc: 0.86
Batch: 60; loss: 1.11; acc: 0.94
Batch: 80; loss: 1.34; acc: 0.92
Batch: 100; loss: 1.82; acc: 0.81
Batch: 120; loss: 0.98; acc: 0.83
Batch: 140; loss: 1.57; acc: 0.83
Val Epoch over. val_loss: 1.2769759968872283; val_accuracy: 0.8835589171974523 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.32; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07337854418215478; val_accuracy: 0.9825835987261147 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04904052180943975; val_accuracy: 0.9879578025477707 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 0.98
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05707856130068469; val_accuracy: 0.9878582802547771 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04712675836910108; val_accuracy: 0.9907444267515924 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04766027774116036; val_accuracy: 0.9907444267515924 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04708584438368773; val_accuracy: 0.9906449044585988 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047738087927080265; val_accuracy: 0.9903463375796179 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04819195776893075; val_accuracy: 0.9904458598726115 

Epoch 21 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04843927101249908; val_accuracy: 0.9904458598726115 

Epoch 22 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04903280196391094; val_accuracy: 0.9906449044585988 

Epoch 23 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049318745613667614; val_accuracy: 0.9906449044585988 

Epoch 24 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04957088139975906; val_accuracy: 0.9904458598726115 

Epoch 25 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05015231735387426; val_accuracy: 0.9905453821656051 

Epoch 26 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05056065377915741; val_accuracy: 0.9907444267515924 

Epoch 27 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050999201122362905; val_accuracy: 0.9907444267515924 

Epoch 28 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05110907647165523; val_accuracy: 0.9906449044585988 

Epoch 29 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05142242798380032; val_accuracy: 0.9906449044585988 

Epoch 30 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0517146209148085; val_accuracy: 0.9907444267515924 

Epoch 31 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05175171771152004; val_accuracy: 0.9907444267515924 

Epoch 32 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05179353186469169; val_accuracy: 0.9907444267515924 

Epoch 33 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05182750561055104; val_accuracy: 0.9907444267515924 

Epoch 34 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05186145653010933; val_accuracy: 0.9907444267515924 

Epoch 35 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05189405376933942; val_accuracy: 0.9907444267515924 

Epoch 36 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051921596335377664; val_accuracy: 0.9907444267515924 

Epoch 37 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05195156605854915; val_accuracy: 0.9907444267515924 

Epoch 38 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05197720303182389; val_accuracy: 0.9907444267515924 

Epoch 39 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05200545200306898; val_accuracy: 0.9907444267515924 

Epoch 40 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052033664978993165; val_accuracy: 0.9907444267515924 

Epoch 41 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05205510219760761; val_accuracy: 0.9907444267515924 

Epoch 42 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05207793049182102; val_accuracy: 0.9907444267515924 

Epoch 43 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052107835200372016; val_accuracy: 0.9907444267515924 

Epoch 44 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05213285451102408; val_accuracy: 0.9907444267515924 

Epoch 45 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05216067877544719; val_accuracy: 0.9907444267515924 

Epoch 46 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05216331276923988; val_accuracy: 0.9907444267515924 

Epoch 47 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052166002476291294; val_accuracy: 0.9907444267515924 

Epoch 48 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052168666391046184; val_accuracy: 0.9907444267515924 

Epoch 49 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05217140094394897; val_accuracy: 0.9907444267515924 

Epoch 50 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05217404003925384; val_accuracy: 0.9907444267515924 

plots/no_subspace_training/lenet/2020-01-18 18:42:42/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.45
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.84; acc: 0.66
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.51; acc: 0.78
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09519038888014805; val_accuracy: 0.9705414012738853 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.1479114733019452; val_accuracy: 0.9579020700636943 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049202444423346; val_accuracy: 0.9852707006369427 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047292595112304776; val_accuracy: 0.9878582802547771 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08968611379527743; val_accuracy: 0.974422770700637 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05308658287973161; val_accuracy: 0.984375 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0518630146743006; val_accuracy: 0.9873606687898089 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.92
Val Epoch over. val_loss: 0.2100599545534629; val_accuracy: 0.955812101910828 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048304825048347946; val_accuracy: 0.9864649681528662 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05150122959522685; val_accuracy: 0.9887539808917197 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04269385079194786; val_accuracy: 0.9901472929936306 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0425573543995429; val_accuracy: 0.9902468152866242 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04321496209995762; val_accuracy: 0.9905453821656051 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04357328327598086; val_accuracy: 0.9909434713375797 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.044528836894566846; val_accuracy: 0.9905453821656051 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04419442859424907; val_accuracy: 0.9910429936305732 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.045482410936598563; val_accuracy: 0.9909434713375797 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04538510549979605; val_accuracy: 0.990843949044586 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.045957700224818696; val_accuracy: 0.9909434713375797 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.045809453839709045; val_accuracy: 0.9912420382165605 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.045919620924314876; val_accuracy: 0.9911425159235668 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04603082710390637; val_accuracy: 0.9912420382165605 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046112494341507076; val_accuracy: 0.9911425159235668 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046180842978179835; val_accuracy: 0.9910429936305732 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046244254252713196; val_accuracy: 0.9910429936305732 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04631026644425787; val_accuracy: 0.9909434713375797 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046380964006967604; val_accuracy: 0.9909434713375797 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04642237153402559; val_accuracy: 0.9909434713375797 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04647734092109522; val_accuracy: 0.9909434713375797 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04652025882795358; val_accuracy: 0.9909434713375797 

Epoch 31 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046524941826322275; val_accuracy: 0.9909434713375797 

Epoch 32 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04652991354655309; val_accuracy: 0.9909434713375797 

Epoch 33 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04653479600218451; val_accuracy: 0.9909434713375797 

Epoch 34 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04653957358021645; val_accuracy: 0.9910429936305732 

Epoch 35 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04654393003434892; val_accuracy: 0.9910429936305732 

Epoch 36 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04654847987138542; val_accuracy: 0.9910429936305732 

Epoch 37 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04655288240522336; val_accuracy: 0.9910429936305732 

Epoch 38 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046557580779312525; val_accuracy: 0.9910429936305732 

Epoch 39 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656227364851411; val_accuracy: 0.9910429936305732 

Epoch 40 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046566950714891885; val_accuracy: 0.9910429936305732 

Epoch 41 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046567221878060866; val_accuracy: 0.9910429936305732 

Epoch 42 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656752735186534; val_accuracy: 0.9910429936305732 

Epoch 43 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656780587070307; val_accuracy: 0.9910429936305732 

Epoch 44 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656810199568985; val_accuracy: 0.9910429936305732 

Epoch 45 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656838317205952; val_accuracy: 0.9910429936305732 

Epoch 46 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656866012485164; val_accuracy: 0.9910429936305732 

Epoch 47 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656891254292932; val_accuracy: 0.9910429936305732 

Epoch 48 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656920985431428; val_accuracy: 0.9910429936305732 

Epoch 49 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656947124156223; val_accuracy: 0.9910429936305732 

Epoch 50 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04656973884553666; val_accuracy: 0.9910429936305732 

plots/no_subspace_training/lenet/2020-01-18 18:47:45/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.14; acc: 0.47
Batch: 40; loss: 1.53; acc: 0.39
Batch: 60; loss: 1.28; acc: 0.45
Batch: 80; loss: 0.71; acc: 0.75
Batch: 100; loss: 0.68; acc: 0.67
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10421887367584143; val_accuracy: 0.9681528662420382 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.08224069417282275; val_accuracy: 0.9756170382165605 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.92
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05067417996039816; val_accuracy: 0.9848726114649682 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.05082203640015262; val_accuracy: 0.9845740445859873 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06888962481289533; val_accuracy: 0.9806926751592356 

Epoch 6 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04021073215801245; val_accuracy: 0.9891520700636943 

Epoch 7 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.03900188701167987; val_accuracy: 0.9896496815286624 

Epoch 8 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.041003767567075744; val_accuracy: 0.9893511146496815 

Epoch 9 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.039564129226146987; val_accuracy: 0.9896496815286624 

Epoch 10 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04085578223702255; val_accuracy: 0.9888535031847133 

Epoch 11 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.040327434778023676; val_accuracy: 0.988953025477707 

Epoch 12 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04021859873726869; val_accuracy: 0.9890525477707006 

Epoch 13 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04014997362236308; val_accuracy: 0.9893511146496815 

Epoch 14 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04011710690464943; val_accuracy: 0.9892515923566879 

Epoch 15 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04016650790811344; val_accuracy: 0.9892515923566879 

Epoch 16 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04016694148919385; val_accuracy: 0.9892515923566879 

Epoch 17 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04016885159027045; val_accuracy: 0.9892515923566879 

Epoch 18 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017113187130849; val_accuracy: 0.9892515923566879 

Epoch 19 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.040173350079993536; val_accuracy: 0.9892515923566879 

Epoch 20 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017656033111226; val_accuracy: 0.9892515923566879 

Epoch 21 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0401766825538532; val_accuracy: 0.9892515923566879 

Epoch 22 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017683581277064; val_accuracy: 0.9892515923566879 

Epoch 23 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017696997067731; val_accuracy: 0.9892515923566879 

Epoch 24 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0401771071183074; val_accuracy: 0.9892515923566879 

Epoch 25 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0401772116637154; val_accuracy: 0.9892515923566879 

Epoch 26 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0401771605062257; val_accuracy: 0.9892515923566879 

Epoch 27 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 0.98
Batch: 540; loss: 0.01; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017710540989402; val_accuracy: 0.9892515923566879 

Epoch 28 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.040177071265354275; val_accuracy: 0.9892515923566879 

Epoch 29 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017701524363202; val_accuracy: 0.9892515923566879 

Epoch 30 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017695378820608; val_accuracy: 0.9892515923566879 

Epoch 31 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.040176949588356506; val_accuracy: 0.9892515923566879 

Epoch 32 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017694372754947; val_accuracy: 0.9892515923566879 

Epoch 33 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.040176938317573754; val_accuracy: 0.9892515923566879 

Epoch 34 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.040176933690620836; val_accuracy: 0.9892515923566879 

Epoch 35 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0401769249587302; val_accuracy: 0.9892515923566879 

Epoch 36 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692728407064; val_accuracy: 0.9892515923566879 

Epoch 37 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692619258431; val_accuracy: 0.9892515923566879 

Epoch 38 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.040176925765480966; val_accuracy: 0.9892515923566879 

Epoch 39 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692628749617; val_accuracy: 0.9892515923566879 

Epoch 40 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692673832748; val_accuracy: 0.9892515923566879 

Epoch 41 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692673832748; val_accuracy: 0.9892515923566879 

Epoch 42 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692664341562; val_accuracy: 0.9892515923566879 

Epoch 43 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692640613599; val_accuracy: 0.9892515923566879 

Epoch 44 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692640613599; val_accuracy: 0.9892515923566879 

Epoch 45 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692631122413; val_accuracy: 0.9892515923566879 

Epoch 46 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692631122413; val_accuracy: 0.9892515923566879 

Epoch 47 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692631122413; val_accuracy: 0.9892515923566879 

Epoch 48 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692631122413; val_accuracy: 0.9892515923566879 

Epoch 49 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692631122413; val_accuracy: 0.9892515923566879 

Epoch 50 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04017692631122413; val_accuracy: 0.9892515923566879 

plots/no_subspace_training/lenet/2020-01-18 18:52:45/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.47
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2996691706453919; val_accuracy: 0.9097332802547771 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.20973124523546285; val_accuracy: 0.9399880573248408 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.22788726379442367; val_accuracy: 0.9272492038216561 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12341860738719344; val_accuracy: 0.9632762738853503 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14492690511000383; val_accuracy: 0.9568073248407644 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08504724163253596; val_accuracy: 0.9741242038216561 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07713115834601365; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.08895722370903203; val_accuracy: 0.9749203821656051 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.95
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.083664060327088; val_accuracy: 0.9748208598726115 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06835302534946211; val_accuracy: 0.9794984076433121 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05865442330480381; val_accuracy: 0.98328025477707 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06335611477684064; val_accuracy: 0.9818869426751592 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07558507548206171; val_accuracy: 0.9781050955414012 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06696917244773003; val_accuracy: 0.9806926751592356 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05738391302478541; val_accuracy: 0.9830812101910829 

Epoch 16 start
The current lr is: 0.008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06526000688599932; val_accuracy: 0.9791998407643312 

Epoch 17 start
The current lr is: 0.008
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07773015697955325; val_accuracy: 0.977109872611465 

Epoch 18 start
The current lr is: 0.008
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09812395637676974; val_accuracy: 0.9718351910828026 

Epoch 19 start
The current lr is: 0.008
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053020449580660295; val_accuracy: 0.984375 

Epoch 20 start
The current lr is: 0.008
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06684382952702274; val_accuracy: 0.9800955414012739 

Epoch 21 start
The current lr is: 0.008
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047692782726067647; val_accuracy: 0.9864649681528662 

Epoch 22 start
The current lr is: 0.008
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05197201804465549; val_accuracy: 0.9853702229299363 

Epoch 23 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04892351205466659; val_accuracy: 0.986265923566879 

Epoch 24 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0504072262982654; val_accuracy: 0.9857683121019108 

Epoch 25 start
The current lr is: 0.008
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04955738106635726; val_accuracy: 0.9858678343949044 

Epoch 26 start
The current lr is: 0.008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0491995193586228; val_accuracy: 0.9864649681528662 

Epoch 27 start
The current lr is: 0.008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05347408809859282; val_accuracy: 0.9856687898089171 

Epoch 28 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05426522069106436; val_accuracy: 0.9841759554140127 

Epoch 29 start
The current lr is: 0.008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050855532430919115; val_accuracy: 0.98546974522293 

Epoch 30 start
The current lr is: 0.008
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051331025542347294; val_accuracy: 0.9861664012738853 

Epoch 31 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049448899972211025; val_accuracy: 0.9860668789808917 

Epoch 32 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04958767852016315; val_accuracy: 0.9863654458598726 

Epoch 33 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.06793034354307849; val_accuracy: 0.9818869426751592 

Epoch 34 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05130088457446189; val_accuracy: 0.9863654458598726 

Epoch 35 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04948098558908815; val_accuracy: 0.9864649681528662 

Epoch 36 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051271943956803366; val_accuracy: 0.9859673566878981 

Epoch 37 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04965212931678553; val_accuracy: 0.9865644904458599 

Epoch 38 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04930585413981395; val_accuracy: 0.9867635350318471 

Epoch 39 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04945632374020898; val_accuracy: 0.9861664012738853 

Epoch 40 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048864736441214376; val_accuracy: 0.9869625796178344 

Epoch 41 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05120302233726356; val_accuracy: 0.9866640127388535 

Epoch 42 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053711587646204956; val_accuracy: 0.9859673566878981 

Epoch 43 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04995536970294964; val_accuracy: 0.9874601910828026 

Epoch 44 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05446293494503969; val_accuracy: 0.9846735668789809 

Epoch 45 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051644172732997096; val_accuracy: 0.987062101910828 

Epoch 46 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050512825275302695; val_accuracy: 0.9873606687898089 

Epoch 47 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05653143821248583; val_accuracy: 0.9855692675159236 

Epoch 48 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05062927039945202; val_accuracy: 0.9864649681528662 

Epoch 49 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05070438250234932; val_accuracy: 0.987062101910828 

Epoch 50 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05258655135229135; val_accuracy: 0.9868630573248408 

plots/no_subspace_training/lenet/2020-01-18 18:57:56/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.47
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.29966508929327035; val_accuracy: 0.9096337579617835 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.20993876701612382; val_accuracy: 0.9404856687898089 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2295669108439403; val_accuracy: 0.9270501592356688 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12215106293653986; val_accuracy: 0.9636743630573248 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1407920428949177; val_accuracy: 0.9582006369426752 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08632388094048592; val_accuracy: 0.9735270700636943 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07656173583618395; val_accuracy: 0.9772093949044586 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.08926828438119525; val_accuracy: 0.9748208598726115 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.08349381399097716; val_accuracy: 0.9757165605095541 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06824297707077044; val_accuracy: 0.9799960191082803 

Epoch 11 start
The current lr is: 0.008
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05875022462598837; val_accuracy: 0.98328025477707 

Epoch 12 start
The current lr is: 0.008
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060600283442978645; val_accuracy: 0.9826831210191083 

Epoch 13 start
The current lr is: 0.008
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07059603905791689; val_accuracy: 0.9801950636942676 

Epoch 14 start
The current lr is: 0.008
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06722209583612004; val_accuracy: 0.9799960191082803 

Epoch 15 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05676248992324635; val_accuracy: 0.9829816878980892 

Epoch 16 start
The current lr is: 0.008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07419541238504611; val_accuracy: 0.9776074840764332 

Epoch 17 start
The current lr is: 0.008
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07821369593500331; val_accuracy: 0.9773089171974523 

Epoch 18 start
The current lr is: 0.008
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12451373016948153; val_accuracy: 0.964171974522293 

Epoch 19 start
The current lr is: 0.008
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05485504531082074; val_accuracy: 0.9851711783439491 

Epoch 20 start
The current lr is: 0.008
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06941085095238533; val_accuracy: 0.979796974522293 

Epoch 21 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048674643015975405; val_accuracy: 0.9866640127388535 

Epoch 22 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05173223997187463; val_accuracy: 0.9853702229299363 

Epoch 23 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04943731230250589; val_accuracy: 0.9863654458598726 

Epoch 24 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049398892149803746; val_accuracy: 0.9857683121019108 

Epoch 25 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050623759247694806; val_accuracy: 0.9859673566878981 

Epoch 26 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05001309957758636; val_accuracy: 0.9858678343949044 

Epoch 27 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.057378954307478705; val_accuracy: 0.9847730891719745 

Epoch 28 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05478411367174926; val_accuracy: 0.9846735668789809 

Epoch 29 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051426710169406455; val_accuracy: 0.986265923566879 

Epoch 30 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05095143976864541; val_accuracy: 0.9860668789808917 

Epoch 31 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05026115932662016; val_accuracy: 0.9856687898089171 

Epoch 32 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04971615153892785; val_accuracy: 0.9866640127388535 

Epoch 33 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.92
Batch: 140; loss: 0.01; acc: 0.98
Val Epoch over. val_loss: 0.05916792557687516; val_accuracy: 0.9840764331210191 

Epoch 34 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05097842942567388; val_accuracy: 0.9865644904458599 

Epoch 35 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05401204298635957; val_accuracy: 0.9851711783439491 

Epoch 36 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05271235108375549; val_accuracy: 0.986265923566879 

Epoch 37 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049138028483102274; val_accuracy: 0.9866640127388535 

Epoch 38 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04867228490721648; val_accuracy: 0.9861664012738853 

Epoch 39 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04901091426990594; val_accuracy: 0.9868630573248408 

Epoch 40 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.01; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04959250606928661; val_accuracy: 0.9861664012738853 

Epoch 41 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05030553650324512; val_accuracy: 0.9865644904458599 

Epoch 42 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05190851325821725; val_accuracy: 0.9866640127388535 

Epoch 43 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04990461910036719; val_accuracy: 0.9872611464968153 

Epoch 44 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0513904582068419; val_accuracy: 0.9861664012738853 

Epoch 45 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04994157198697898; val_accuracy: 0.987062101910828 

Epoch 46 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050326692260754334; val_accuracy: 0.9867635350318471 

Epoch 47 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05729757519854102; val_accuracy: 0.9849721337579618 

Epoch 48 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049885792717052874; val_accuracy: 0.9864649681528662 

Epoch 49 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04985165140431398; val_accuracy: 0.9866640127388535 

Epoch 50 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05096101286304984; val_accuracy: 0.9869625796178344 

plots/no_subspace_training/lenet/2020-01-18 19:03:02/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.45
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.29955443573795304; val_accuracy: 0.9102308917197452 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.20911920329282999; val_accuracy: 0.9399880573248408 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.91
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2334230325546614; val_accuracy: 0.9255573248407644 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12402827624872231; val_accuracy: 0.9637738853503185 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14096899131301102; val_accuracy: 0.9584992038216561 

Epoch 6 start
The current lr is: 0.008
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08505353836971484; val_accuracy: 0.9743232484076433 

Epoch 7 start
The current lr is: 0.008
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07604697764299478; val_accuracy: 0.977109872611465 

Epoch 8 start
The current lr is: 0.008
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.08288320917992076; val_accuracy: 0.9763136942675159 

Epoch 9 start
The current lr is: 0.008
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.08434054248367145; val_accuracy: 0.9748208598726115 

Epoch 10 start
The current lr is: 0.008
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0696085230417692; val_accuracy: 0.9795979299363057 

Epoch 11 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06092172374675987; val_accuracy: 0.9826831210191083 

Epoch 12 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06304186338167282; val_accuracy: 0.9818869426751592 

Epoch 13 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07014729156141068; val_accuracy: 0.9794984076433121 

Epoch 14 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06695545628477054; val_accuracy: 0.9802945859872612 

Epoch 15 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05843708170637204; val_accuracy: 0.9822850318471338 

Epoch 16 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06302075716815177; val_accuracy: 0.9810907643312102 

Epoch 17 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06853303707139508; val_accuracy: 0.9794984076433121 

Epoch 18 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10365373785993096; val_accuracy: 0.9691480891719745 

Epoch 19 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05577258796543832; val_accuracy: 0.9834792993630573 

Epoch 20 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07117023708144571; val_accuracy: 0.9792993630573248 

Epoch 21 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05123030223474381; val_accuracy: 0.9848726114649682 

Epoch 22 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05320777639651754; val_accuracy: 0.9844745222929936 

Epoch 23 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05197113655080461; val_accuracy: 0.9863654458598726 

Epoch 24 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05103866530546717; val_accuracy: 0.9853702229299363 

Epoch 25 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05224135537056407; val_accuracy: 0.9849721337579618 

Epoch 26 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05100427736427374; val_accuracy: 0.9858678343949044 

Epoch 27 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.05749753183999639; val_accuracy: 0.984375 

Epoch 28 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05828304845056716; val_accuracy: 0.984375 

Epoch 29 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051808699538373644; val_accuracy: 0.9851711783439491 

Epoch 30 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.97
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05218721532328113; val_accuracy: 0.9860668789808917 

Epoch 31 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05135462350051874; val_accuracy: 0.9852707006369427 

Epoch 32 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05149622738456271; val_accuracy: 0.98546974522293 

Epoch 33 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057022150368637343; val_accuracy: 0.9846735668789809 

Epoch 34 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050910275287119444; val_accuracy: 0.9853702229299363 

Epoch 35 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05333079411915154; val_accuracy: 0.9851711783439491 

Epoch 36 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052234677728384166; val_accuracy: 0.98546974522293 

Epoch 37 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049796774793582355; val_accuracy: 0.9859673566878981 

Epoch 38 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05042119075063687; val_accuracy: 0.9857683121019108 

Epoch 39 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049458879645273186; val_accuracy: 0.9863654458598726 

Epoch 40 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05109160402967672; val_accuracy: 0.9856687898089171 

Epoch 41 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05178770736144606; val_accuracy: 0.9851711783439491 

Epoch 42 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05134425255333542; val_accuracy: 0.9859673566878981 

Epoch 43 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050335336737572005; val_accuracy: 0.9864649681528662 

Epoch 44 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052132240905883205; val_accuracy: 0.9852707006369427 

Epoch 45 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049993515394295855; val_accuracy: 0.9861664012738853 

Epoch 46 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04944003805233415; val_accuracy: 0.9858678343949044 

Epoch 47 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.95
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051930061855893224; val_accuracy: 0.9849721337579618 

Epoch 48 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04943341847248138; val_accuracy: 0.9856687898089171 

Epoch 49 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04968792968874524; val_accuracy: 0.9858678343949044 

Epoch 50 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05049341698740698; val_accuracy: 0.9855692675159236 

plots/no_subspace_training/lenet/2020-01-18 19:08:08/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.47
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.64
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.54; acc: 0.8
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3006230741739273; val_accuracy: 0.9098328025477707 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.20806254724123677; val_accuracy: 0.9407842356687898 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.91
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.22652125460611786; val_accuracy: 0.9280453821656051 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12673571366508296; val_accuracy: 0.9628781847133758 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14536187280515198; val_accuracy: 0.9566082802547771 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08570680480189384; val_accuracy: 0.9737261146496815 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07723329920962357; val_accuracy: 0.9764132165605095 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.08849647119166745; val_accuracy: 0.9751194267515924 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.08339007920140673; val_accuracy: 0.9754179936305732 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06740763864129971; val_accuracy: 0.980593152866242 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058793917867788086; val_accuracy: 0.9836783439490446 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06621055665669168; val_accuracy: 0.9814888535031847 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07587803883632277; val_accuracy: 0.9782046178343949 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07011789797692541; val_accuracy: 0.9793988853503185 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05747987102171418; val_accuracy: 0.9834792993630573 

Epoch 16 start
The current lr is: 0.004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05431805031409689; val_accuracy: 0.9845740445859873 

Epoch 17 start
The current lr is: 0.004
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0559027157486624; val_accuracy: 0.9835788216560509 

Epoch 18 start
The current lr is: 0.004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06955702208979114; val_accuracy: 0.9798964968152867 

Epoch 19 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052480239349945335; val_accuracy: 0.9849721337579618 

Epoch 20 start
The current lr is: 0.004
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058798139879278316; val_accuracy: 0.9830812101910829 

Epoch 21 start
The current lr is: 0.004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04972322361104807; val_accuracy: 0.9865644904458599 

Epoch 22 start
The current lr is: 0.004
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05071200167012822; val_accuracy: 0.9861664012738853 

Epoch 23 start
The current lr is: 0.004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04968715703506379; val_accuracy: 0.9865644904458599 

Epoch 24 start
The current lr is: 0.004
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04864082649160342; val_accuracy: 0.9859673566878981 

Epoch 25 start
The current lr is: 0.004
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050649477489245164; val_accuracy: 0.9855692675159236 

Epoch 26 start
The current lr is: 0.004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04972733970090842; val_accuracy: 0.986265923566879 

Epoch 27 start
The current lr is: 0.004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05417610375080139; val_accuracy: 0.9855692675159236 

Epoch 28 start
The current lr is: 0.004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0546384317574987; val_accuracy: 0.9842754777070064 

Epoch 29 start
The current lr is: 0.004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05079584758562647; val_accuracy: 0.98546974522293 

Epoch 30 start
The current lr is: 0.004
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05173885632472433; val_accuracy: 0.9860668789808917 

Epoch 31 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049044213524669596; val_accuracy: 0.9863654458598726 

Epoch 32 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04968264338317191; val_accuracy: 0.9859673566878981 

Epoch 33 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05045756472239069; val_accuracy: 0.9856687898089171 

Epoch 34 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04895679429648029; val_accuracy: 0.9865644904458599 

Epoch 35 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04936102298414631; val_accuracy: 0.9863654458598726 

Epoch 36 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05013608313195265; val_accuracy: 0.9858678343949044 

Epoch 37 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048380747390021184; val_accuracy: 0.9865644904458599 

Epoch 38 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04908453326696043; val_accuracy: 0.9865644904458599 

Epoch 39 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048143992330997615; val_accuracy: 0.9868630573248408 

Epoch 40 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04897027026126339; val_accuracy: 0.986265923566879 

Epoch 41 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05030694807980471; val_accuracy: 0.9865644904458599 

Epoch 42 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0497316244728626; val_accuracy: 0.9865644904458599 

Epoch 43 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048993849735351126; val_accuracy: 0.987062101910828 

Epoch 44 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05063523347400556; val_accuracy: 0.9857683121019108 

Epoch 45 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04909281688890639; val_accuracy: 0.9867635350318471 

Epoch 46 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0484048778748816; val_accuracy: 0.9865644904458599 

Epoch 47 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048904755597661255; val_accuracy: 0.9866640127388535 

Epoch 48 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04860380049913552; val_accuracy: 0.9868630573248408 

Epoch 49 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04863901080409433; val_accuracy: 0.9866640127388535 

Epoch 50 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04894386583073124; val_accuracy: 0.9865644904458599 

plots/no_subspace_training/lenet/2020-01-18 19:13:19/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.45
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2999354449996523; val_accuracy: 0.9104299363057324 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2080409943250714; val_accuracy: 0.9404856687898089 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.23126804892709302; val_accuracy: 0.9263535031847133 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12594157137024176; val_accuracy: 0.963077229299363 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1460207329624018; val_accuracy: 0.9559116242038217 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08714938054608691; val_accuracy: 0.9730294585987261 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07698214103936389; val_accuracy: 0.9768113057324841 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.08197097625037666; val_accuracy: 0.9764132165605095 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.0865675825032459; val_accuracy: 0.974422770700637 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06850820324223512; val_accuracy: 0.9796974522292994 

Epoch 11 start
The current lr is: 0.004
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05857485770040257; val_accuracy: 0.9830812101910829 

Epoch 12 start
The current lr is: 0.004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05768387831130605; val_accuracy: 0.9829816878980892 

Epoch 13 start
The current lr is: 0.004
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061494624562514054; val_accuracy: 0.9821855095541401 

Epoch 14 start
The current lr is: 0.004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06025668069910092; val_accuracy: 0.9823845541401274 

Epoch 15 start
The current lr is: 0.004
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05662917483384442; val_accuracy: 0.9835788216560509 

Epoch 16 start
The current lr is: 0.004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058892646199388866; val_accuracy: 0.9827826433121019 

Epoch 17 start
The current lr is: 0.004
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06065148603954133; val_accuracy: 0.9820859872611465 

Epoch 18 start
The current lr is: 0.004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07637022767856622; val_accuracy: 0.9773089171974523 

Epoch 19 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055319804366037346; val_accuracy: 0.9837778662420382 

Epoch 20 start
The current lr is: 0.004
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06515467420789846; val_accuracy: 0.9815883757961783 

Epoch 21 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051790720765378065; val_accuracy: 0.9855692675159236 

Epoch 22 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05236239828596449; val_accuracy: 0.9851711783439491 

Epoch 23 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051781872844999764; val_accuracy: 0.9858678343949044 

Epoch 24 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051162007246996946; val_accuracy: 0.9860668789808917 

Epoch 25 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05268327633191826; val_accuracy: 0.9846735668789809 

Epoch 26 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051696335576522125; val_accuracy: 0.9853702229299363 

Epoch 27 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0537060447701603; val_accuracy: 0.9852707006369427 

Epoch 28 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05360277867905653; val_accuracy: 0.9851711783439491 

Epoch 29 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051835806577638456; val_accuracy: 0.9858678343949044 

Epoch 30 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05197267364829209; val_accuracy: 0.9858678343949044 

Epoch 31 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0513912034547253; val_accuracy: 0.9852707006369427 

Epoch 32 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051630911340189586; val_accuracy: 0.9855692675159236 

Epoch 33 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05164391853532214; val_accuracy: 0.9851711783439491 

Epoch 34 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05104341732848222; val_accuracy: 0.986265923566879 

Epoch 35 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051357919290946547; val_accuracy: 0.98546974522293 

Epoch 36 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050815245765409654; val_accuracy: 0.9856687898089171 

Epoch 37 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05074624669779638; val_accuracy: 0.9858678343949044 

Epoch 38 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05084599746735233; val_accuracy: 0.9858678343949044 

Epoch 39 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05039604024807359; val_accuracy: 0.9856687898089171 

Epoch 40 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050800329940334246; val_accuracy: 0.9857683121019108 

Epoch 41 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050886211691388655; val_accuracy: 0.9855692675159236 

Epoch 42 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05081538288931178; val_accuracy: 0.9858678343949044 

Epoch 43 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050622403170842274; val_accuracy: 0.9864649681528662 

Epoch 44 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050640599481808914; val_accuracy: 0.9859673566878981 

Epoch 45 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05056411160784922; val_accuracy: 0.9859673566878981 

Epoch 46 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05042141576291649; val_accuracy: 0.9858678343949044 

Epoch 47 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050413451187170236; val_accuracy: 0.9857683121019108 

Epoch 48 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0505368714784361; val_accuracy: 0.9857683121019108 

Epoch 49 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05053404801685339; val_accuracy: 0.9858678343949044 

Epoch 50 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05059755228127644; val_accuracy: 0.9856687898089171 

plots/no_subspace_training/lenet/2020-01-18 19:18:29/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.45
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.29931525240658197; val_accuracy: 0.9098328025477707 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.20760702645512902; val_accuracy: 0.9404856687898089 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.23058093806645671; val_accuracy: 0.9260549363057324 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12549228140502977; val_accuracy: 0.9634753184713376 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14435865412092513; val_accuracy: 0.9574044585987261 

Epoch 6 start
The current lr is: 0.004
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08349762047836735; val_accuracy: 0.9751194267515924 

Epoch 7 start
The current lr is: 0.004
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07706860939313652; val_accuracy: 0.9777070063694268 

Epoch 8 start
The current lr is: 0.004
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07810757892906287; val_accuracy: 0.9770103503184714 

Epoch 9 start
The current lr is: 0.004
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07979462575760617; val_accuracy: 0.975218949044586 

Epoch 10 start
The current lr is: 0.004
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.91
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0734104087018663; val_accuracy: 0.9781050955414012 

Epoch 11 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06847246444434117; val_accuracy: 0.978702229299363 

Epoch 12 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06749408509416185; val_accuracy: 0.9795979299363057 

Epoch 13 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.94
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06843681977527916; val_accuracy: 0.9796974522292994 

Epoch 14 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06829034112346401; val_accuracy: 0.9791998407643312 

Epoch 15 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06661958800284726; val_accuracy: 0.9804936305732485 

Epoch 16 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06548365333657356; val_accuracy: 0.9796974522292994 

Epoch 17 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06478056273642618; val_accuracy: 0.9802945859872612 

Epoch 18 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06509948379484712; val_accuracy: 0.9800955414012739 

Epoch 19 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06543893037233384; val_accuracy: 0.980593152866242 

Epoch 20 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06570376434428676; val_accuracy: 0.9808917197452229 

Epoch 21 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06393997791181703; val_accuracy: 0.9802945859872612 

Epoch 22 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06431222913466442; val_accuracy: 0.980593152866242 

Epoch 23 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06408061227126485; val_accuracy: 0.9810907643312102 

Epoch 24 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06375835755258609; val_accuracy: 0.9806926751592356 

Epoch 25 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06402120936163672; val_accuracy: 0.9810907643312102 

Epoch 26 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.063863031589871; val_accuracy: 0.9807921974522293 

Epoch 27 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06383709305790579; val_accuracy: 0.9807921974522293 

Epoch 28 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06371595610858528; val_accuracy: 0.980593152866242 

Epoch 29 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06372511061798235; val_accuracy: 0.9807921974522293 

Epoch 30 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06362426634522; val_accuracy: 0.9807921974522293 

Epoch 31 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06364001675396208; val_accuracy: 0.9808917197452229 

Epoch 32 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06364218240520757; val_accuracy: 0.9810907643312102 

Epoch 33 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06359291793244659; val_accuracy: 0.9807921974522293 

Epoch 34 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0635902582649972; val_accuracy: 0.9808917197452229 

Epoch 35 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06358558864920003; val_accuracy: 0.9808917197452229 

Epoch 36 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06356368807091076; val_accuracy: 0.9808917197452229 

Epoch 37 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0635628992110301; val_accuracy: 0.9808917197452229 

Epoch 38 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06355236311721954; val_accuracy: 0.9807921974522293 

Epoch 39 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06354434695688023; val_accuracy: 0.9808917197452229 

Epoch 40 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06353816802904105; val_accuracy: 0.9808917197452229 

Epoch 41 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06353259629979255; val_accuracy: 0.9808917197452229 

Epoch 42 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.95
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06353084612519118; val_accuracy: 0.9808917197452229 

Epoch 43 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06352935166685444; val_accuracy: 0.9808917197452229 

Epoch 44 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.23; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06352705983029809; val_accuracy: 0.9808917197452229 

Epoch 45 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0635254278684118; val_accuracy: 0.9808917197452229 

Epoch 46 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06352486838675608; val_accuracy: 0.9808917197452229 

Epoch 47 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06352415358185008; val_accuracy: 0.9808917197452229 

Epoch 48 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.94
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06352358548694355; val_accuracy: 0.9808917197452229 

Epoch 49 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06352310096192512; val_accuracy: 0.9808917197452229 

Epoch 50 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06352289557267146; val_accuracy: 0.9808917197452229 

plots/no_subspace_training/lenet/2020-01-18 19:23:33/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.45
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.75
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.29911630031219716; val_accuracy: 0.9096337579617835 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2081335341190077; val_accuracy: 0.9400875796178344 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.09; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.23308280907618772; val_accuracy: 0.9250597133757962 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12095296361548885; val_accuracy: 0.9637738853503185 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14004666498227483; val_accuracy: 0.9589968152866242 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08868600240653488; val_accuracy: 0.9731289808917197 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07675629187446491; val_accuracy: 0.9773089171974523 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.0877640666618089; val_accuracy: 0.9746218152866242 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.0839130808688273; val_accuracy: 0.975218949044586 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.067700005308458; val_accuracy: 0.980593152866242 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05942947078187754; val_accuracy: 0.98328025477707 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06321734619463325; val_accuracy: 0.9821855095541401 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07506440501588925; val_accuracy: 0.9780055732484076 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06837436352755613; val_accuracy: 0.979796974522293 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05713691664444413; val_accuracy: 0.9831807324840764 

Epoch 16 start
The current lr is: 0.002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051588679029113925; val_accuracy: 0.9848726114649682 

Epoch 17 start
The current lr is: 0.002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05110131975287085; val_accuracy: 0.9861664012738853 

Epoch 18 start
The current lr is: 0.002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053628077813584335; val_accuracy: 0.984375 

Epoch 19 start
The current lr is: 0.002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05145013175762383; val_accuracy: 0.9852707006369427 

Epoch 20 start
The current lr is: 0.002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05322921688959097; val_accuracy: 0.9849721337579618 

Epoch 21 start
The current lr is: 0.002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04987405506288929; val_accuracy: 0.9867635350318471 

Epoch 22 start
The current lr is: 0.002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05044366224746036; val_accuracy: 0.986265923566879 

Epoch 23 start
The current lr is: 0.002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04980512494873849; val_accuracy: 0.986265923566879 

Epoch 24 start
The current lr is: 0.002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04885684179177709; val_accuracy: 0.9864649681528662 

Epoch 25 start
The current lr is: 0.002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0509159396266102; val_accuracy: 0.9857683121019108 

Epoch 26 start
The current lr is: 0.002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04987332635339658; val_accuracy: 0.9861664012738853 

Epoch 27 start
The current lr is: 0.002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05193331068868091; val_accuracy: 0.9859673566878981 

Epoch 28 start
The current lr is: 0.002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051204738485965; val_accuracy: 0.9853702229299363 

Epoch 29 start
The current lr is: 0.002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05009014973310149; val_accuracy: 0.9856687898089171 

Epoch 30 start
The current lr is: 0.002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05030634183033257; val_accuracy: 0.9861664012738853 

Epoch 31 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04930881762011036; val_accuracy: 0.9857683121019108 

Epoch 32 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04940917259853357; val_accuracy: 0.9859673566878981 

Epoch 33 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049104851522263446; val_accuracy: 0.9863654458598726 

Epoch 34 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04899897720593556; val_accuracy: 0.986265923566879 

Epoch 35 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04931813931673955; val_accuracy: 0.9860668789808917 

Epoch 36 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048904443124107494; val_accuracy: 0.9859673566878981 

Epoch 37 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04890336556609269; val_accuracy: 0.986265923566879 

Epoch 38 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048942276460539766; val_accuracy: 0.9861664012738853 

Epoch 39 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04877228116628471; val_accuracy: 0.986265923566879 

Epoch 40 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048850084233815504; val_accuracy: 0.9859673566878981 

Epoch 41 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049221911105760345; val_accuracy: 0.9858678343949044 

Epoch 42 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04910471809992365; val_accuracy: 0.9860668789808917 

Epoch 43 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0489433715060638; val_accuracy: 0.9865644904458599 

Epoch 44 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04903035886158609; val_accuracy: 0.9857683121019108 

Epoch 45 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04895182753539389; val_accuracy: 0.986265923566879 

Epoch 46 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04881963418547515; val_accuracy: 0.9863654458598726 

Epoch 47 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04877808004332956; val_accuracy: 0.9864649681528662 

Epoch 48 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0488133534409438; val_accuracy: 0.986265923566879 

Epoch 49 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04879102070525194; val_accuracy: 0.9863654458598726 

Epoch 50 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04883279720214522; val_accuracy: 0.986265923566879 

plots/no_subspace_training/lenet/2020-01-18 19:28:35/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.47
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2993362013986156; val_accuracy: 0.9098328025477707 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.21336608273303434; val_accuracy: 0.9396894904458599 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.22794936768188598; val_accuracy: 0.9271496815286624 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12136865516377103; val_accuracy: 0.9635748407643312 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14515613596055918; val_accuracy: 0.9578025477707006 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0867048592609205; val_accuracy: 0.9740246815286624 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07701587093294046; val_accuracy: 0.9767117834394905 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.08975347107762743; val_accuracy: 0.9739251592356688 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.95
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.08365361907394828; val_accuracy: 0.9757165605095541 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06727441604350023; val_accuracy: 0.9804936305732485 

Epoch 11 start
The current lr is: 0.002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05871047940888223; val_accuracy: 0.9831807324840764 

Epoch 12 start
The current lr is: 0.002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057628899883882256; val_accuracy: 0.9830812101910829 

Epoch 13 start
The current lr is: 0.002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.94
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058999572281435035; val_accuracy: 0.98328025477707 

Epoch 14 start
The current lr is: 0.002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05837159074700562; val_accuracy: 0.9834792993630573 

Epoch 15 start
The current lr is: 0.002
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05777974198957917; val_accuracy: 0.9833797770700637 

Epoch 16 start
The current lr is: 0.002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05695224868928551; val_accuracy: 0.9831807324840764 

Epoch 17 start
The current lr is: 0.002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05630593269968488; val_accuracy: 0.9830812101910829 

Epoch 18 start
The current lr is: 0.002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058801762807141444; val_accuracy: 0.9833797770700637 

Epoch 19 start
The current lr is: 0.002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05656194622824146; val_accuracy: 0.9839769108280255 

Epoch 20 start
The current lr is: 0.002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059658885144504015; val_accuracy: 0.9834792993630573 

Epoch 21 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05426971033025699; val_accuracy: 0.9844745222929936 

Epoch 22 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05469229031994844; val_accuracy: 0.9841759554140127 

Epoch 23 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05443601099074267; val_accuracy: 0.9852707006369427 

Epoch 24 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05411900447052755; val_accuracy: 0.9853702229299363 

Epoch 25 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054602606186441555; val_accuracy: 0.984375 

Epoch 26 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05438881025762315; val_accuracy: 0.9848726114649682 

Epoch 27 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05451383299318848; val_accuracy: 0.9846735668789809 

Epoch 28 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05432547225504165; val_accuracy: 0.9849721337579618 

Epoch 29 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05419982084707849; val_accuracy: 0.9851711783439491 

Epoch 30 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05402286896470246; val_accuracy: 0.9849721337579618 

Epoch 31 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05399759839864293; val_accuracy: 0.9849721337579618 

Epoch 32 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05404818449525317; val_accuracy: 0.9847730891719745 

Epoch 33 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05396305878830564; val_accuracy: 0.9848726114649682 

Epoch 34 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05395193454945923; val_accuracy: 0.9846735668789809 

Epoch 35 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05402028743343748; val_accuracy: 0.9844745222929936 

Epoch 36 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05389960791180088; val_accuracy: 0.9848726114649682 

Epoch 37 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053943820962101034; val_accuracy: 0.9846735668789809 

Epoch 38 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053917682735593456; val_accuracy: 0.9846735668789809 

Epoch 39 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053902059247729124; val_accuracy: 0.9846735668789809 

Epoch 40 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.2; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05387655220878352; val_accuracy: 0.9850716560509554 

Epoch 41 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053878374255386885; val_accuracy: 0.9850716560509554 

Epoch 42 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053880622361306175; val_accuracy: 0.9850716560509554 

Epoch 43 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053880376065043124; val_accuracy: 0.9849721337579618 

Epoch 44 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053878293248118866; val_accuracy: 0.9849721337579618 

Epoch 45 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053878528273599166; val_accuracy: 0.9849721337579618 

Epoch 46 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053878007824443706; val_accuracy: 0.9849721337579618 

Epoch 47 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05387469820061307; val_accuracy: 0.9849721337579618 

Epoch 48 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05387477669271694; val_accuracy: 0.9849721337579618 

Epoch 49 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05387276128693751; val_accuracy: 0.9849721337579618 

Epoch 50 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.95
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0538734325747581; val_accuracy: 0.9849721337579618 

plots/no_subspace_training/lenet/2020-01-18 19:33:41/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.45
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3004593585801732; val_accuracy: 0.9091361464968153 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2078507787957313; val_accuracy: 0.9403861464968153 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.23067383752886655; val_accuracy: 0.9261544585987261 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12227408137101277; val_accuracy: 0.9639729299363057 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13811850770833387; val_accuracy: 0.9588972929936306 

Epoch 6 start
The current lr is: 0.002
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08311684567267728; val_accuracy: 0.9745222929936306 

Epoch 7 start
The current lr is: 0.002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07916754303844112; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07881624681080222; val_accuracy: 0.9768113057324841 

Epoch 9 start
The current lr is: 0.002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07793204486370087; val_accuracy: 0.9761146496815286 

Epoch 10 start
The current lr is: 0.002
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.91
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07553542687729665; val_accuracy: 0.9770103503184714 

Epoch 11 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07385031665396538; val_accuracy: 0.9776074840764332 

Epoch 12 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07357472261995267; val_accuracy: 0.9777070063694268 

Epoch 13 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07339485533013465; val_accuracy: 0.9775079617834395 

Epoch 14 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07357631982037216; val_accuracy: 0.9778065286624203 

Epoch 15 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07339646887912113; val_accuracy: 0.977906050955414 

Epoch 16 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07280185982395129; val_accuracy: 0.9776074840764332 

Epoch 17 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07265116378759882; val_accuracy: 0.9776074840764332 

Epoch 18 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07271281171851098; val_accuracy: 0.977906050955414 

Epoch 19 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.95
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07266814717251784; val_accuracy: 0.9777070063694268 

Epoch 20 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07256639160358222; val_accuracy: 0.9777070063694268 

Epoch 21 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07255531026489416; val_accuracy: 0.9777070063694268 

Epoch 22 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0725541087282691; val_accuracy: 0.9778065286624203 

Epoch 23 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07255176751382032; val_accuracy: 0.9778065286624203 

Epoch 24 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07253753045087408; val_accuracy: 0.9778065286624203 

Epoch 25 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252430270431907; val_accuracy: 0.9777070063694268 

Epoch 26 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252408730186474; val_accuracy: 0.9778065286624203 

Epoch 27 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252360417679617; val_accuracy: 0.9778065286624203 

Epoch 28 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252379471234455; val_accuracy: 0.9778065286624203 

Epoch 29 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252362365745435; val_accuracy: 0.9778065286624203 

Epoch 30 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252283894996735; val_accuracy: 0.9778065286624203 

Epoch 31 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252276477635286; val_accuracy: 0.9778065286624203 

Epoch 32 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252261358176827; val_accuracy: 0.9778065286624203 

Epoch 33 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252269212132806; val_accuracy: 0.9778065286624203 

Epoch 34 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252257568821026; val_accuracy: 0.9778065286624203 

Epoch 35 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252230310136346; val_accuracy: 0.9778065286624203 

Epoch 36 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252231487043344; val_accuracy: 0.9778065286624203 

Epoch 37 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252233302232566; val_accuracy: 0.9778065286624203 

Epoch 38 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0725223667160341; val_accuracy: 0.9778065286624203 

Epoch 39 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252242691387796; val_accuracy: 0.9778065286624203 

Epoch 40 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252246834290256; val_accuracy: 0.9778065286624203 

Epoch 41 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252246188889643; val_accuracy: 0.9778065286624203 

Epoch 42 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.94
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252244153030359; val_accuracy: 0.9778065286624203 

Epoch 43 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252243286959685; val_accuracy: 0.9778065286624203 

Epoch 44 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.94
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.072522441269296; val_accuracy: 0.9778065286624203 

Epoch 45 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252244637080818; val_accuracy: 0.9778065286624203 

Epoch 46 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252244535050574; val_accuracy: 0.9778065286624203 

Epoch 47 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252244461493887; val_accuracy: 0.9778065286624203 

Epoch 48 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.92
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.92
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252244781821397; val_accuracy: 0.9778065286624203 

Epoch 49 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252244713010302; val_accuracy: 0.9778065286624203 

Epoch 50 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07252244822158935; val_accuracy: 0.9778065286624203 

plots/no_subspace_training/lenet/2020-01-18 19:38:43/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.45
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2991456339598461; val_accuracy: 0.9105294585987261 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.20788782396039385; val_accuracy: 0.9405851910828026 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.91
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.23134924433413584; val_accuracy: 0.9260549363057324 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12112362371983042; val_accuracy: 0.9646695859872612 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13923441284116667; val_accuracy: 0.9582006369426752 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08522677563937606; val_accuracy: 0.9740246815286624 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07754663382745852; val_accuracy: 0.9772093949044586 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.08468255927418447; val_accuracy: 0.9759156050955414 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.95
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.08534719146028825; val_accuracy: 0.975218949044586 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06810067748283125; val_accuracy: 0.9798964968152867 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059075857850776355; val_accuracy: 0.9828821656050956 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06338079926219715; val_accuracy: 0.9819864649681529 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07425447069345766; val_accuracy: 0.9781050955414012 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0691227247096171; val_accuracy: 0.9800955414012739 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05731424013045943; val_accuracy: 0.9829816878980892 

Epoch 16 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05149456633219294; val_accuracy: 0.9846735668789809 

Epoch 17 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05070579704490437; val_accuracy: 0.9864649681528662 

Epoch 18 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05138142660829672; val_accuracy: 0.9857683121019108 

Epoch 19 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05134903902935374; val_accuracy: 0.9856687898089171 

Epoch 20 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05196760587726429; val_accuracy: 0.98546974522293 

Epoch 21 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05011956341517199; val_accuracy: 0.986265923566879 

Epoch 22 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050696829440677244; val_accuracy: 0.9860668789808917 

Epoch 23 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050172143467482486; val_accuracy: 0.9863654458598726 

Epoch 24 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049505344123407534; val_accuracy: 0.9863654458598726 

Epoch 25 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05105634064526315; val_accuracy: 0.9856687898089171 

Epoch 26 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050125920990849754; val_accuracy: 0.9863654458598726 

Epoch 27 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051123336099894945; val_accuracy: 0.9869625796178344 

Epoch 28 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050808743306785635; val_accuracy: 0.9860668789808917 

Epoch 29 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05021678093986906; val_accuracy: 0.9860668789808917 

Epoch 30 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05028566461839494; val_accuracy: 0.9864649681528662 

Epoch 31 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04972592048394452; val_accuracy: 0.9858678343949044 

Epoch 32 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049754532611673806; val_accuracy: 0.9860668789808917 

Epoch 33 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04957759311529481; val_accuracy: 0.9861664012738853 

Epoch 34 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04952997681061933; val_accuracy: 0.9865644904458599 

Epoch 35 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04974816599564188; val_accuracy: 0.9861664012738853 

Epoch 36 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049505785771995596; val_accuracy: 0.986265923566879 

Epoch 37 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049553024825776455; val_accuracy: 0.9861664012738853 

Epoch 38 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04955443595150474; val_accuracy: 0.9861664012738853 

Epoch 39 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049512654044635736; val_accuracy: 0.9860668789808917 

Epoch 40 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049506223529197604; val_accuracy: 0.9861664012738853 

Epoch 41 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049612497092242455; val_accuracy: 0.9858678343949044 

Epoch 42 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049589112068817116; val_accuracy: 0.986265923566879 

Epoch 43 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04956220137845179; val_accuracy: 0.986265923566879 

Epoch 44 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049487211237287825; val_accuracy: 0.9860668789808917 

Epoch 45 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04953079514063088; val_accuracy: 0.9861664012738853 

Epoch 46 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04951805737652597; val_accuracy: 0.9861664012738853 

Epoch 47 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049506358992142285; val_accuracy: 0.986265923566879 

Epoch 48 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04950405377301441; val_accuracy: 0.986265923566879 

Epoch 49 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04949856547128623; val_accuracy: 0.9861664012738853 

Epoch 50 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04950076854153044; val_accuracy: 0.9861664012738853 

plots/no_subspace_training/lenet/2020-01-18 19:43:48/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.47
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.75; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.56; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.299374453486151; val_accuracy: 0.9098328025477707 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.21032358072460836; val_accuracy: 0.9399880573248408 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2233907361129287; val_accuracy: 0.9294386942675159 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12115831806021891; val_accuracy: 0.9635748407643312 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14345254134505417; val_accuracy: 0.9583996815286624 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08475878246270926; val_accuracy: 0.974422770700637 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07736061629690942; val_accuracy: 0.9770103503184714 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.08792587981861867; val_accuracy: 0.9748208598726115 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.08331296507530152; val_accuracy: 0.9756170382165605 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06774458424395817; val_accuracy: 0.980593152866242 

Epoch 11 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05861113818397947; val_accuracy: 0.9829816878980892 

Epoch 12 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05782416569674091; val_accuracy: 0.9834792993630573 

Epoch 13 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05830104252809932; val_accuracy: 0.9834792993630573 

Epoch 14 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0583813628952974; val_accuracy: 0.9829816878980892 

Epoch 15 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058391089129979444; val_accuracy: 0.9834792993630573 

Epoch 16 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05712964630145936; val_accuracy: 0.9836783439490446 

Epoch 17 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05672739628868498; val_accuracy: 0.9836783439490446 

Epoch 18 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057470618088723745; val_accuracy: 0.9839769108280255 

Epoch 19 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05737826190177043; val_accuracy: 0.9834792993630573 

Epoch 20 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05845865884878833; val_accuracy: 0.9836783439490446 

Epoch 21 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05578993078155123; val_accuracy: 0.9841759554140127 

Epoch 22 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05591670532894742; val_accuracy: 0.9845740445859873 

Epoch 23 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.95
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0558295704566749; val_accuracy: 0.9845740445859873 

Epoch 24 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055683796620293026; val_accuracy: 0.9846735668789809 

Epoch 25 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05578835621760909; val_accuracy: 0.9846735668789809 

Epoch 26 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05582513833406624; val_accuracy: 0.9844745222929936 

Epoch 27 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05582912420486189; val_accuracy: 0.9841759554140127 

Epoch 28 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05570902803521247; val_accuracy: 0.9845740445859873 

Epoch 29 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05570019544309871; val_accuracy: 0.9842754777070064 

Epoch 30 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0555882443478153; val_accuracy: 0.9846735668789809 

Epoch 31 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05559737057347966; val_accuracy: 0.9847730891719745 

Epoch 32 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055612009944050175; val_accuracy: 0.9847730891719745 

Epoch 33 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05562107948360929; val_accuracy: 0.9846735668789809 

Epoch 34 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05562210628750977; val_accuracy: 0.9846735668789809 

Epoch 35 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05562762356108161; val_accuracy: 0.9846735668789809 

Epoch 36 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561991313555438; val_accuracy: 0.9846735668789809 

Epoch 37 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05562402618823537; val_accuracy: 0.9846735668789809 

Epoch 38 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05562135489408378; val_accuracy: 0.9846735668789809 

Epoch 39 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561854543199964; val_accuracy: 0.9846735668789809 

Epoch 40 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055614370900164745; val_accuracy: 0.9846735668789809 

Epoch 41 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.01; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561334129636455; val_accuracy: 0.9846735668789809 

Epoch 42 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561281875914829; val_accuracy: 0.9846735668789809 

Epoch 43 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561253568454153; val_accuracy: 0.9846735668789809 

Epoch 44 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561218982574287; val_accuracy: 0.9846735668789809 

Epoch 45 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055611848641353046; val_accuracy: 0.9846735668789809 

Epoch 46 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561150823998603; val_accuracy: 0.9846735668789809 

Epoch 47 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0556111581101539; val_accuracy: 0.9846735668789809 

Epoch 48 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561093475883174; val_accuracy: 0.9846735668789809 

Epoch 49 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561070787204299; val_accuracy: 0.9846735668789809 

Epoch 50 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05561073978615415; val_accuracy: 0.9846735668789809 

plots/no_subspace_training/lenet/2020-01-18 19:48:59/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.45
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.75
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.78
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2993376705392151; val_accuracy: 0.9094347133757962 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2109592500955436; val_accuracy: 0.9402866242038217 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.91
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.23546725888825526; val_accuracy: 0.9247611464968153 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12293491014249765; val_accuracy: 0.9631767515923567 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14038134038827982; val_accuracy: 0.9581011146496815 

Epoch 6 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0822805254987091; val_accuracy: 0.9756170382165605 

Epoch 7 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07978748663026057; val_accuracy: 0.9763136942675159 

Epoch 8 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07908842489597903; val_accuracy: 0.9767117834394905 

Epoch 9 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0783435385792878; val_accuracy: 0.9762141719745223 

Epoch 10 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.91
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07686721932166701; val_accuracy: 0.9766122611464968 

Epoch 11 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07617082477659937; val_accuracy: 0.9768113057324841 

Epoch 12 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0759997617239785; val_accuracy: 0.9770103503184714 

Epoch 13 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07590483791034693; val_accuracy: 0.9767117834394905 

Epoch 14 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07583888427941662; val_accuracy: 0.9775079617834395 

Epoch 15 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07574966023112559; val_accuracy: 0.977109872611465 

Epoch 16 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07564789398460631; val_accuracy: 0.9770103503184714 

Epoch 17 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07558633579285282; val_accuracy: 0.9769108280254777 

Epoch 18 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07556764219122328; val_accuracy: 0.9770103503184714 

Epoch 19 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.95
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07555206459324071; val_accuracy: 0.977109872611465 

Epoch 20 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07553160297358112; val_accuracy: 0.9772093949044586 

Epoch 21 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552973014913547; val_accuracy: 0.9772093949044586 

Epoch 22 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552832431474309; val_accuracy: 0.9772093949044586 

Epoch 23 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.94
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552688526120155; val_accuracy: 0.9772093949044586 

Epoch 24 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552504390003575; val_accuracy: 0.9772093949044586 

Epoch 25 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0755229421244685; val_accuracy: 0.9772093949044586 

Epoch 26 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552278661139451; val_accuracy: 0.977109872611465 

Epoch 27 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0755226835845762; val_accuracy: 0.977109872611465 

Epoch 28 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552268230326616; val_accuracy: 0.977109872611465 

Epoch 29 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552265881258212; val_accuracy: 0.977109872611465 

Epoch 30 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552256712773044; val_accuracy: 0.977109872611465 

Epoch 31 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552255704334587; val_accuracy: 0.977109872611465 

Epoch 32 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552256148047508; val_accuracy: 0.977109872611465 

Epoch 33 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552259624194188; val_accuracy: 0.977109872611465 

Epoch 34 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0755226101465286; val_accuracy: 0.977109872611465 

Epoch 35 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262533242536; val_accuracy: 0.977109872611465 

Epoch 36 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262371892382; val_accuracy: 0.977109872611465 

Epoch 37 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262272234934; val_accuracy: 0.977109872611465 

Epoch 38 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262250879768; val_accuracy: 0.977109872611465 

Epoch 39 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262298335695; val_accuracy: 0.977109872611465 

Epoch 40 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262262743749; val_accuracy: 0.977109872611465 

Epoch 41 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262241388583; val_accuracy: 0.977109872611465 

Epoch 42 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.94
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0755226217732308; val_accuracy: 0.977109872611465 

Epoch 43 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262129867153; val_accuracy: 0.977109872611465 

Epoch 44 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.94
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0755226221054223; val_accuracy: 0.977109872611465 

Epoch 45 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262239015786; val_accuracy: 0.977109872611465 

Epoch 46 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262239015786; val_accuracy: 0.977109872611465 

Epoch 47 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262179695876; val_accuracy: 0.977109872611465 

Epoch 48 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.91
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.92
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262208169433; val_accuracy: 0.977109872611465 

Epoch 49 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.92
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262260370952; val_accuracy: 0.977109872611465 

Epoch 50 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07552262212915026; val_accuracy: 0.977109872611465 

plots/no_subspace_training/lenet/2020-01-18 19:54:03/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.47
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2992516921678926; val_accuracy: 0.9096337579617835 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.21080737513531544; val_accuracy: 0.940187101910828 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.22775328866425593; val_accuracy: 0.9274482484076433 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12503902765975636; val_accuracy: 0.9627786624203821 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1426734831777348; val_accuracy: 0.957703025477707 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08531253595071234; val_accuracy: 0.9738256369426752 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07809015662427161; val_accuracy: 0.9763136942675159 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.0900283465577159; val_accuracy: 0.9742237261146497 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.0842895358800888; val_accuracy: 0.9754179936305732 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0673772405809278; val_accuracy: 0.9801950636942676 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05849356778488038; val_accuracy: 0.9829816878980892 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06286006866936471; val_accuracy: 0.9822850318471338 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07342280160374702; val_accuracy: 0.9789012738853503 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06986956262759342; val_accuracy: 0.979796974522293 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05707744516100094; val_accuracy: 0.9822850318471338 

Epoch 16 start
The current lr is: 0.001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051363404223303884; val_accuracy: 0.9848726114649682 

Epoch 17 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05064009538122043; val_accuracy: 0.98546974522293 

Epoch 18 start
The current lr is: 0.001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050710836722023166; val_accuracy: 0.9860668789808917 

Epoch 19 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05122684893809307; val_accuracy: 0.98546974522293 

Epoch 20 start
The current lr is: 0.001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05130188928762819; val_accuracy: 0.9851711783439491 

Epoch 21 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050048281005613364; val_accuracy: 0.9859673566878981 

Epoch 22 start
The current lr is: 0.001
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05058766593029545; val_accuracy: 0.9863654458598726 

Epoch 23 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.01; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050068809324578877; val_accuracy: 0.9861664012738853 

Epoch 24 start
The current lr is: 0.001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04948079306039081; val_accuracy: 0.9860668789808917 

Epoch 25 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05064629499984395; val_accuracy: 0.98546974522293 

Epoch 26 start
The current lr is: 0.001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05001024510355512; val_accuracy: 0.986265923566879 

Epoch 27 start
The current lr is: 0.001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05057766578000063; val_accuracy: 0.9859673566878981 

Epoch 28 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05017307457650543; val_accuracy: 0.9860668789808917 

Epoch 29 start
The current lr is: 0.001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049913190542512635; val_accuracy: 0.986265923566879 

Epoch 30 start
The current lr is: 0.001
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04983573247957381; val_accuracy: 0.9861664012738853 

Epoch 31 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049465416865364; val_accuracy: 0.9860668789808917 

Epoch 32 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0495077681959055; val_accuracy: 0.9859673566878981 

Epoch 33 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04941643399607604; val_accuracy: 0.9859673566878981 

Epoch 34 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049373778734047705; val_accuracy: 0.9858678343949044 

Epoch 35 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04948314516597493; val_accuracy: 0.9859673566878981 

Epoch 36 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04936295444039023; val_accuracy: 0.9860668789808917 

Epoch 37 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04938664263600757; val_accuracy: 0.9858678343949044 

Epoch 38 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04937589393964239; val_accuracy: 0.9859673566878981 

Epoch 39 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049364743196660545; val_accuracy: 0.9860668789808917 

Epoch 40 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049358349364654275; val_accuracy: 0.9858678343949044 

Epoch 41 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049389077623368825; val_accuracy: 0.9858678343949044 

Epoch 42 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04937636947176259; val_accuracy: 0.9859673566878981 

Epoch 43 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04936178541107542; val_accuracy: 0.9860668789808917 

Epoch 44 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049323000274835875; val_accuracy: 0.9860668789808917 

Epoch 45 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04933880210207526; val_accuracy: 0.9859673566878981 

Epoch 46 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04933737019065079; val_accuracy: 0.9859673566878981 

Epoch 47 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04933530182405642; val_accuracy: 0.9859673566878981 

Epoch 48 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049334865324436476; val_accuracy: 0.9859673566878981 

Epoch 49 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04933385949605589; val_accuracy: 0.9859673566878981 

Epoch 50 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049334015104041735; val_accuracy: 0.9859673566878981 

plots/no_subspace_training/lenet/2020-01-18 19:59:11/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.45
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.26; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.2993516918200596; val_accuracy: 0.9098328025477707 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2087447889813572; val_accuracy: 0.9404856687898089 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.1; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.22558342893222336; val_accuracy: 0.9274482484076433 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12301967482847773; val_accuracy: 0.9636743630573248 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1444907143569676; val_accuracy: 0.9573049363057324 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08622985489808829; val_accuracy: 0.9738256369426752 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07695517769664716; val_accuracy: 0.9768113057324841 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.08453237410089012; val_accuracy: 0.9761146496815286 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.95
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.08210280516250118; val_accuracy: 0.9759156050955414 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06802698034959234; val_accuracy: 0.9795979299363057 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05842528036635393; val_accuracy: 0.9826831210191083 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05797308299002374; val_accuracy: 0.98328025477707 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05804369007800795; val_accuracy: 0.9834792993630573 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.94
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058077039422502946; val_accuracy: 0.9827826433121019 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05833267185623479; val_accuracy: 0.9835788216560509 

Epoch 16 start
The current lr is: 0.001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057009708017680294; val_accuracy: 0.9833797770700637 

Epoch 17 start
The current lr is: 0.001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05669688741872265; val_accuracy: 0.9833797770700637 

Epoch 18 start
The current lr is: 0.001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05706101010559471; val_accuracy: 0.9834792993630573 

Epoch 19 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05741903190590014; val_accuracy: 0.9829816878980892 

Epoch 20 start
The current lr is: 0.001
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057697213426896723; val_accuracy: 0.9838773885350318 

Epoch 21 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05613376643912048; val_accuracy: 0.9837778662420382 

Epoch 22 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05615802339402733; val_accuracy: 0.9839769108280255 

Epoch 23 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.95
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056096072006187624; val_accuracy: 0.9841759554140127 

Epoch 24 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05600936473554866; val_accuracy: 0.9841759554140127 

Epoch 25 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05603168964101251; val_accuracy: 0.9840764331210191 

Epoch 26 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05607602095148366; val_accuracy: 0.9841759554140127 

Epoch 27 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05607754279189049; val_accuracy: 0.9839769108280255 

Epoch 28 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05599670683976951; val_accuracy: 0.9840764331210191 

Epoch 29 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056008467059226553; val_accuracy: 0.9840764331210191 

Epoch 30 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055953533477654124; val_accuracy: 0.9840764331210191 

Epoch 31 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595351364107648; val_accuracy: 0.9840764331210191 

Epoch 32 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595412686656995; val_accuracy: 0.9840764331210191 

Epoch 33 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.95
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595715324969808; val_accuracy: 0.9841759554140127 

Epoch 34 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595631959142199; val_accuracy: 0.9841759554140127 

Epoch 35 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595629339575008; val_accuracy: 0.9841759554140127 

Epoch 36 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595391623343632; val_accuracy: 0.9841759554140127 

Epoch 37 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595480532023558; val_accuracy: 0.9841759554140127 

Epoch 38 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595357198813918; val_accuracy: 0.9841759554140127 

Epoch 39 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055952123965427376; val_accuracy: 0.9841759554140127 

Epoch 40 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055950981582615786; val_accuracy: 0.9841759554140127 

Epoch 41 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.01; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055950617002453774; val_accuracy: 0.9841759554140127 

Epoch 42 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595046433673543; val_accuracy: 0.9841759554140127 

Epoch 43 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.94
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595040259657392; val_accuracy: 0.9841759554140127 

Epoch 44 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595032567051566; val_accuracy: 0.9841759554140127 

Epoch 45 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595026767937241; val_accuracy: 0.9841759554140127 

Epoch 46 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0559501449820722; val_accuracy: 0.9841759554140127 

Epoch 47 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055950154615625455; val_accuracy: 0.9841759554140127 

Epoch 48 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0559500961736509; val_accuracy: 0.9841759554140127 

Epoch 49 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595000054995725; val_accuracy: 0.9840764331210191 

Epoch 50 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05595002916588145; val_accuracy: 0.9840764331210191 

plots/no_subspace_training/lenet/2020-01-18 20:04:15/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.24; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.52
Batch: 180; loss: 2.18; acc: 0.33
Batch: 200; loss: 2.1; acc: 0.33
Batch: 220; loss: 2.0; acc: 0.47
Batch: 240; loss: 1.75; acc: 0.58
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.22; acc: 0.64
Batch: 300; loss: 1.16; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.56; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.29894719258615166; val_accuracy: 0.9103304140127388 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.20977791842476579; val_accuracy: 0.9399880573248408 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.09; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.22950409483283188; val_accuracy: 0.9268511146496815 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12255665524655086; val_accuracy: 0.9640724522292994 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14822996471812772; val_accuracy: 0.955812101910828 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08213731539761944; val_accuracy: 0.9751194267515924 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08011502713249748; val_accuracy: 0.9762141719745223 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07951678409223344; val_accuracy: 0.9766122611464968 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.1; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07874936059971524; val_accuracy: 0.9765127388535032 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.91
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07757783833013218; val_accuracy: 0.9761146496815286 

Epoch 11 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07726135802496771; val_accuracy: 0.9765127388535032 

Epoch 12 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07722077497346386; val_accuracy: 0.9766122611464968 

Epoch 13 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07713961798199423; val_accuracy: 0.9764132165605095 

Epoch 14 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.94
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07705982390103067; val_accuracy: 0.9767117834394905 

Epoch 15 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0770031491377551; val_accuracy: 0.9766122611464968 

Epoch 16 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07697500153237087; val_accuracy: 0.9767117834394905 

Epoch 17 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0769498942147015; val_accuracy: 0.9767117834394905 

Epoch 18 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07693665766507198; val_accuracy: 0.9767117834394905 

Epoch 19 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07692654302735237; val_accuracy: 0.9767117834394905 

Epoch 20 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691632747460322; val_accuracy: 0.9766122611464968 

Epoch 21 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691551379527256; val_accuracy: 0.9766122611464968 

Epoch 22 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691478007917951; val_accuracy: 0.9766122611464968 

Epoch 23 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691407877548485; val_accuracy: 0.9766122611464968 

Epoch 24 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691329976270912; val_accuracy: 0.9766122611464968 

Epoch 25 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691245091853628; val_accuracy: 0.9766122611464968 

Epoch 26 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691248696131311; val_accuracy: 0.9766122611464968 

Epoch 27 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691250300141657; val_accuracy: 0.9766122611464968 

Epoch 28 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0769125610637437; val_accuracy: 0.9766122611464968 

Epoch 29 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691254791845183; val_accuracy: 0.9766122611464968 

Epoch 30 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.94
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0769125305495824; val_accuracy: 0.9766122611464968 

Epoch 31 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0769125316647967; val_accuracy: 0.9766122611464968 

Epoch 32 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.95
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253119023742; val_accuracy: 0.9766122611464968 

Epoch 33 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0769125319258043; val_accuracy: 0.9766122611464968 

Epoch 34 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253515280735; val_accuracy: 0.9766122611464968 

Epoch 35 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253206817208; val_accuracy: 0.9766122611464968 

Epoch 36 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253413250491; val_accuracy: 0.9766122611464968 

Epoch 37 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0769125342036888; val_accuracy: 0.9766122611464968 

Epoch 38 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.94
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253569855053; val_accuracy: 0.9766122611464968 

Epoch 39 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253446469641; val_accuracy: 0.9766122611464968 

Epoch 40 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253448842438; val_accuracy: 0.9766122611464968 

Epoch 41 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253510535143; val_accuracy: 0.9766122611464968 

Epoch 42 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253501043957; val_accuracy: 0.9766122611464968 

Epoch 43 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.95
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253491552771; val_accuracy: 0.9766122611464968 

Epoch 44 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.27; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.94
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253501043957; val_accuracy: 0.9766122611464968 

Epoch 45 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253501043957; val_accuracy: 0.9766122611464968 

Epoch 46 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253501043957; val_accuracy: 0.9766122611464968 

Epoch 47 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253501043957; val_accuracy: 0.9766122611464968 

Epoch 48 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.91
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253501043957; val_accuracy: 0.9766122611464968 

Epoch 49 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253501043957; val_accuracy: 0.9766122611464968 

Epoch 50 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07691253501043957; val_accuracy: 0.9766122611464968 

plots/no_subspace_training/lenet/2020-01-18 20:09:20/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.2799397000841273; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.2144418112031974; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814965592827766; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.56
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484059795452531; val_accuracy: 0.7580613057324841 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818381169988851; val_accuracy: 0.8250398089171974 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.45923994595457795; val_accuracy: 0.8603702229299363 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3832538699743095; val_accuracy: 0.8882364649681529 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3440699291172301; val_accuracy: 0.9011743630573248 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2982063126886726; val_accuracy: 0.9134156050955414 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2682480718110018; val_accuracy: 0.9246616242038217 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.2528006431593257; val_accuracy: 0.9290406050955414 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.23659448069371994; val_accuracy: 0.9325238853503185 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2168575219667641; val_accuracy: 0.9398885350318471 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.21151255695208623; val_accuracy: 0.9391918789808917 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18915970440787874; val_accuracy: 0.9468550955414012 

Epoch 16 start
The current lr is: 0.0008
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17891603295400643; val_accuracy: 0.9498407643312102 

Epoch 17 start
The current lr is: 0.0008
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.89
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17387862104898805; val_accuracy: 0.9511345541401274 

Epoch 18 start
The current lr is: 0.0008
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.07; acc: 1.0
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16937862896615533; val_accuracy: 0.9524283439490446 

Epoch 19 start
The current lr is: 0.0008
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16026582182118088; val_accuracy: 0.9549164012738853 

Epoch 20 start
The current lr is: 0.0008
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15953764903127768; val_accuracy: 0.9554140127388535 

Epoch 21 start
The current lr is: 0.0008
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.07; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14989736870785428; val_accuracy: 0.957703025477707 

Epoch 22 start
The current lr is: 0.0008
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1476582721540123; val_accuracy: 0.956906847133758 

Epoch 23 start
The current lr is: 0.0008
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.88
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14112393439385543; val_accuracy: 0.9600915605095541 

Epoch 24 start
The current lr is: 0.0008
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.88
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1368395767299233; val_accuracy: 0.9623805732484076 

Epoch 25 start
The current lr is: 0.0008
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1336584222164883; val_accuracy: 0.9624800955414012 

Epoch 26 start
The current lr is: 0.0008
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.91
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1302324744166842; val_accuracy: 0.9640724522292994 

Epoch 27 start
The current lr is: 0.0008
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13096793107451146; val_accuracy: 0.9621815286624203 

Epoch 28 start
The current lr is: 0.0008
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12980889398485992; val_accuracy: 0.963077229299363 

Epoch 29 start
The current lr is: 0.0008
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.07; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.08; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1271307896229492; val_accuracy: 0.9654657643312102 

Epoch 30 start
The current lr is: 0.0008
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11954547374681303; val_accuracy: 0.9658638535031847 

Epoch 31 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11905060774961095; val_accuracy: 0.9656648089171974 

Epoch 32 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11729431520127187; val_accuracy: 0.9666600318471338 

Epoch 33 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.07; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11617866737447727; val_accuracy: 0.9665605095541401 

Epoch 34 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.07; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.112190023942548; val_accuracy: 0.9677547770700637 

Epoch 35 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.94
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11244327280741588; val_accuracy: 0.9681528662420382 

Epoch 36 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.27; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.07; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10912629705705461; val_accuracy: 0.96875 

Epoch 37 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.91
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10898104663582364; val_accuracy: 0.96875 

Epoch 38 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10913828212269552; val_accuracy: 0.9685509554140127 

Epoch 39 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10513402478899925; val_accuracy: 0.9699442675159236 

Epoch 40 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1077350553148871; val_accuracy: 0.9690485668789809 

Epoch 41 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1091605959709283; val_accuracy: 0.9686504777070064 

Epoch 42 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10728940424645782; val_accuracy: 0.9689490445859873 

Epoch 43 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.92
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10837639787584354; val_accuracy: 0.9690485668789809 

Epoch 44 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10279171555569977; val_accuracy: 0.9695461783439491 

Epoch 45 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09952465257352325; val_accuracy: 0.9710390127388535 

Epoch 46 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09716834668900556; val_accuracy: 0.9711385350318471 

Epoch 47 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09897017353184663; val_accuracy: 0.9713375796178344 

Epoch 48 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.92
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09689094147579685; val_accuracy: 0.9716361464968153 

Epoch 49 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09738375163476938; val_accuracy: 0.9707404458598726 

Epoch 50 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09826207796859134; val_accuracy: 0.9710390127388535 

plots/no_subspace_training/lenet/2020-01-18 20:14:26/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.2799396955283586; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214439440684713; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.881473412938938; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9483960065872047; val_accuracy: 0.7580613057324841 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818200935224059; val_accuracy: 0.8253383757961783 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.45923572645825184; val_accuracy: 0.8602707006369427 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3832516898015502; val_accuracy: 0.8882364649681529 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3442159116173246; val_accuracy: 0.9012738853503185 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2981266645584137; val_accuracy: 0.9137141719745223 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26856481075097044; val_accuracy: 0.9248606687898089 

Epoch 11 start
The current lr is: 0.0008
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25359165153590735; val_accuracy: 0.9293391719745223 

Epoch 12 start
The current lr is: 0.0008
Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2393065644962013; val_accuracy: 0.9326234076433121 

Epoch 13 start
The current lr is: 0.0008
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.98
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22372976583754942; val_accuracy: 0.9381966560509554 

Epoch 14 start
The current lr is: 0.0008
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.88
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.21759502869692576; val_accuracy: 0.9373009554140127 

Epoch 15 start
The current lr is: 0.0008
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.1; acc: 1.0
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20012320331327474; val_accuracy: 0.944765127388535 

Epoch 16 start
The current lr is: 0.0008
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18950166702745067; val_accuracy: 0.9471536624203821 

Epoch 17 start
The current lr is: 0.0008
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.18372880655584062; val_accuracy: 0.9482484076433121 

Epoch 18 start
The current lr is: 0.0008
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17799763927224335; val_accuracy: 0.9499402866242038 

Epoch 19 start
The current lr is: 0.0008
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1679424304681219; val_accuracy: 0.9526273885350318 

Epoch 20 start
The current lr is: 0.0008
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.21; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16655799105858346; val_accuracy: 0.953125 

Epoch 21 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15697159914738812; val_accuracy: 0.9559116242038217 

Epoch 22 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.45; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15434662143515934; val_accuracy: 0.9559116242038217 

Epoch 23 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.88
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14848762304539892; val_accuracy: 0.9583996815286624 

Epoch 24 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.88
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.92
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14384737769794312; val_accuracy: 0.9597929936305732 

Epoch 25 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1408923333807356; val_accuracy: 0.9601910828025477 

Epoch 26 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13775562347879836; val_accuracy: 0.9618829617834395 

Epoch 27 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13712557300830344; val_accuracy: 0.9609872611464968 

Epoch 28 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.88
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13585692748522302; val_accuracy: 0.9613853503184714 

Epoch 29 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.07; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.09; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.133032430153174; val_accuracy: 0.9628781847133758 

Epoch 30 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12712126884870467; val_accuracy: 0.964171974522293 

Epoch 31 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12625190278716908; val_accuracy: 0.9646695859872612 

Epoch 32 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12467697542753949; val_accuracy: 0.9648686305732485 

Epoch 33 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.08; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12239181610998834; val_accuracy: 0.9658638535031847 

Epoch 34 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11963892089808063; val_accuracy: 0.9665605095541401 

Epoch 35 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.119254758664567; val_accuracy: 0.9669585987261147 

Epoch 36 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11683075692338549; val_accuracy: 0.9675557324840764 

Epoch 37 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11605340149846806; val_accuracy: 0.9672571656050956 

Epoch 38 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.08; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1158808850606156; val_accuracy: 0.9673566878980892 

Epoch 39 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.89
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11251330520411965; val_accuracy: 0.9678542993630573 

Epoch 40 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11431557613952904; val_accuracy: 0.9678542993630573 

Epoch 41 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11241635872395175; val_accuracy: 0.9673566878980892 

Epoch 42 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11209765693564323; val_accuracy: 0.9675557324840764 

Epoch 43 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11205363518018631; val_accuracy: 0.9680533439490446 

Epoch 44 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.36; acc: 0.95
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.15; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.91
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11024925022558042; val_accuracy: 0.9670581210191083 

Epoch 45 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10746603563522837; val_accuracy: 0.9690485668789809 

Epoch 46 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10520846705148175; val_accuracy: 0.96984474522293 

Epoch 47 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10582029449332292; val_accuracy: 0.9690485668789809 

Epoch 48 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10446816056397311; val_accuracy: 0.9696457006369427 

Epoch 49 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10457246083837406; val_accuracy: 0.9689490445859873 

Epoch 50 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10516206571345876; val_accuracy: 0.9695461783439491 

plots/no_subspace_training/lenet/2020-01-18 20:19:29/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.2799397000841273; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.2144412341391204; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814858387989604; val_accuracy: 0.502687101910828 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484522919745961; val_accuracy: 0.7579617834394905 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818533231118682; val_accuracy: 0.8251393312101911 

Epoch 6 start
The current lr is: 0.0008
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.78
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.52; acc: 0.8
Batch: 340; loss: 0.59; acc: 0.8
Batch: 360; loss: 0.56; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.78
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.78
Batch: 540; loss: 0.85; acc: 0.78
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.81
Batch: 680; loss: 0.46; acc: 0.78
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.56; train_accuracy: 0.83 

Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.47459945139611603; val_accuracy: 0.8571855095541401 

Epoch 7 start
The current lr is: 0.0008
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.8
Batch: 180; loss: 0.73; acc: 0.75
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.52; acc: 0.78
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.61; acc: 0.78
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.56; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.36; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.54; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.72; acc: 0.77
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.86 

Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.95
Val Epoch over. val_loss: 0.4076445915137127; val_accuracy: 0.8802746815286624 

Epoch 8 start
The current lr is: 0.0008
Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.31; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.77
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.35; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.366621733376175; val_accuracy: 0.8936106687898089 

Epoch 9 start
The current lr is: 0.0008
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.8
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.32526703985633365; val_accuracy: 0.90625 

Epoch 10 start
The current lr is: 0.0008
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2956866229036052; val_accuracy: 0.915406050955414 

Epoch 11 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.27928697327329854; val_accuracy: 0.9210788216560509 

Epoch 12 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2637135044784303; val_accuracy: 0.9255573248407644 

Epoch 13 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.98
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.53; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24906899343440486; val_accuracy: 0.9312300955414012 

Epoch 14 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2407451491019908; val_accuracy: 0.9309315286624203 

Epoch 15 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22541877836178822; val_accuracy: 0.9382961783439491 

Epoch 16 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2157840107798956; val_accuracy: 0.9400875796178344 

Epoch 17 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20869181862777206; val_accuracy: 0.9421775477707006 

Epoch 18 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.41; acc: 0.81
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20249372969862003; val_accuracy: 0.9435708598726115 

Epoch 19 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.19556487662492283; val_accuracy: 0.9463574840764332 

Epoch 20 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1923542166496538; val_accuracy: 0.9467555732484076 

Epoch 21 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18639609746777328; val_accuracy: 0.9473527070063694 

Epoch 22 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.48; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18241800823409088; val_accuracy: 0.948546974522293 

Epoch 23 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17760430442489636; val_accuracy: 0.9506369426751592 

Epoch 24 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17262016137124628; val_accuracy: 0.9522292993630573 

Epoch 25 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.09; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16906188694155141; val_accuracy: 0.9526273885350318 

Epoch 26 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.09; acc: 1.0
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16586323329217875; val_accuracy: 0.9538216560509554 

Epoch 27 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.09; acc: 1.0
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16347100851452276; val_accuracy: 0.9539211783439491 

Epoch 28 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.86
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16256895545087044; val_accuracy: 0.95421974522293 

Epoch 29 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.09; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15895790037266008; val_accuracy: 0.9548168789808917 

Epoch 30 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15648362379829595; val_accuracy: 0.9552149681528662 

Epoch 31 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.07; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1541194798318064; val_accuracy: 0.9565087579617835 

Epoch 32 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.11; acc: 1.0
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15292214398171491; val_accuracy: 0.956906847133758 

Epoch 33 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.11; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15032062651055633; val_accuracy: 0.9576035031847133 

Epoch 34 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.08; acc: 1.0
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1486142468727698; val_accuracy: 0.9585987261146497 

Epoch 35 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14718442958346598; val_accuracy: 0.9580015923566879 

Epoch 36 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14587794398521162; val_accuracy: 0.9593949044585988 

Epoch 37 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1450876454069356; val_accuracy: 0.9590963375796179 

Epoch 38 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14360407331756725; val_accuracy: 0.9596934713375797 

Epoch 39 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14235756721845857; val_accuracy: 0.9594944267515924 

Epoch 40 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14206049308940105; val_accuracy: 0.9598925159235668 

Epoch 41 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14009326212345416; val_accuracy: 0.9604896496815286 

Epoch 42 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13992716986567352; val_accuracy: 0.9606886942675159 

Epoch 43 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13968276638229182; val_accuracy: 0.9607882165605095 

Epoch 44 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.88
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13927841872261587; val_accuracy: 0.9598925159235668 

Epoch 45 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1376267018116963; val_accuracy: 0.9615843949044586 

Epoch 46 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.08; acc: 1.0
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.06; acc: 1.0
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1357178028411926; val_accuracy: 0.9620820063694268 

Epoch 47 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13518564028155272; val_accuracy: 0.9615843949044586 

Epoch 48 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.21; acc: 0.89
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13458351949882355; val_accuracy: 0.9612858280254777 

Epoch 49 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13442406874553414; val_accuracy: 0.961484872611465 

Epoch 50 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1343384887429939; val_accuracy: 0.9618829617834395 

plots/no_subspace_training/lenet/2020-01-18 20:24:28/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.2799395300020837; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214437495371339; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.881465942237028; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.83
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9483801054347093; val_accuracy: 0.7579617834394905 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818343955999726; val_accuracy: 0.8251393312101911 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.45926004050263936; val_accuracy: 0.8601711783439491 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3832356834392639; val_accuracy: 0.8885350318471338 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.34406572513899225; val_accuracy: 0.9013734076433121 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2980669265150265; val_accuracy: 0.9137141719745223 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2685374072782553; val_accuracy: 0.9246616242038217 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.2528523131255891; val_accuracy: 0.929140127388535 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.23659034490964975; val_accuracy: 0.9324243630573248 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21670365146106216; val_accuracy: 0.9402866242038217 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.21148687591598292; val_accuracy: 0.9394904458598726 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18920700634076337; val_accuracy: 0.946656050955414 

Epoch 16 start
The current lr is: 0.0004
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18168673977540556; val_accuracy: 0.9496417197452229 

Epoch 17 start
The current lr is: 0.0004
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17691701489269354; val_accuracy: 0.950437898089172 

Epoch 18 start
The current lr is: 0.0004
Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17426359152812868; val_accuracy: 0.9507364649681529 

Epoch 19 start
The current lr is: 0.0004
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17077270722028556; val_accuracy: 0.9523288216560509 

Epoch 20 start
The current lr is: 0.0004
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1688986362260618; val_accuracy: 0.9537221337579618 

Epoch 21 start
The current lr is: 0.0004
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.43; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16496028473517696; val_accuracy: 0.9535230891719745 

Epoch 22 start
The current lr is: 0.0004
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16253935175526674; val_accuracy: 0.9540207006369427 

Epoch 23 start
The current lr is: 0.0004
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.15; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1584407732270326; val_accuracy: 0.9557125796178344 

Epoch 24 start
The current lr is: 0.0004
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15491918422234285; val_accuracy: 0.9556130573248408 

Epoch 25 start
The current lr is: 0.0004
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15231769467899753; val_accuracy: 0.9572054140127388 

Epoch 26 start
The current lr is: 0.0004
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14974041128898882; val_accuracy: 0.9575039808917197 

Epoch 27 start
The current lr is: 0.0004
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.98
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1480666298395509; val_accuracy: 0.9581011146496815 

Epoch 28 start
The current lr is: 0.0004
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14732059888589155; val_accuracy: 0.9585987261146497 

Epoch 29 start
The current lr is: 0.0004
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.08; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1440393567132722; val_accuracy: 0.9590963375796179 

Epoch 30 start
The current lr is: 0.0004
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14099161613993583; val_accuracy: 0.9601910828025477 

Epoch 31 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13929684737780293; val_accuracy: 0.9599920382165605 

Epoch 32 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.1; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.91
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1385155692222012; val_accuracy: 0.9598925159235668 

Epoch 33 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.1; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.91
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13718715979225315; val_accuracy: 0.9609872611464968 

Epoch 34 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.06; acc: 1.0
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13652965507120085; val_accuracy: 0.9604896496815286 

Epoch 35 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13577576909380354; val_accuracy: 0.9612858280254777 

Epoch 36 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13496745370660618; val_accuracy: 0.9623805732484076 

Epoch 37 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13481117789722552; val_accuracy: 0.9618829617834395 

Epoch 38 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13371832289133861; val_accuracy: 0.9620820063694268 

Epoch 39 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13311930888207854; val_accuracy: 0.9625796178343949 

Epoch 40 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1329946485199746; val_accuracy: 0.9628781847133758 

Epoch 41 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13171534513117403; val_accuracy: 0.9624800955414012 

Epoch 42 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13162831886179127; val_accuracy: 0.9628781847133758 

Epoch 43 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13122113938828942; val_accuracy: 0.9629777070063694 

Epoch 44 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.94
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13128379694405634; val_accuracy: 0.9628781847133758 

Epoch 45 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12972855359126048; val_accuracy: 0.9631767515923567 

Epoch 46 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.07; acc: 1.0
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12841858660244637; val_accuracy: 0.9635748407643312 

Epoch 47 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1281204250208132; val_accuracy: 0.9638734076433121 

Epoch 48 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.92
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1279438603076206; val_accuracy: 0.9637738853503185 

Epoch 49 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12823405807280236; val_accuracy: 0.9636743630573248 

Epoch 50 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12790929559309772; val_accuracy: 0.9637738853503185 

plots/no_subspace_training/lenet/2020-01-18 20:29:33/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.2799396909725895; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214441287289759; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.881481259491793; val_accuracy: 0.502687101910828 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.83
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484453770765073; val_accuracy: 0.7579617834394905 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818739015205651; val_accuracy: 0.8249402866242038 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.459304102287171; val_accuracy: 0.8602707006369427 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3832827294423322; val_accuracy: 0.8883359872611465 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.344309651642848; val_accuracy: 0.9012738853503185 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2980957940980128; val_accuracy: 0.9138136942675159 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.268595204469125; val_accuracy: 0.9247611464968153 

Epoch 11 start
The current lr is: 0.0004
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.86
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25783005997443653; val_accuracy: 0.9289410828025477 

Epoch 12 start
The current lr is: 0.0004
Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24876423868214248; val_accuracy: 0.9314291401273885 

Epoch 13 start
The current lr is: 0.0004
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.98
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.52; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.24100351530560263; val_accuracy: 0.9338176751592356 

Epoch 14 start
The current lr is: 0.0004
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.98
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.2354191179583027; val_accuracy: 0.932921974522293 

Epoch 15 start
The current lr is: 0.0004
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.22734507234992496; val_accuracy: 0.9376990445859873 

Epoch 16 start
The current lr is: 0.0004
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2202327176453961; val_accuracy: 0.9386942675159236 

Epoch 17 start
The current lr is: 0.0004
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21360739137811266; val_accuracy: 0.9411823248407644 

Epoch 18 start
The current lr is: 0.0004
Batch: 0; loss: 0.42; acc: 0.81
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2085593755647635; val_accuracy: 0.9429737261146497 

Epoch 19 start
The current lr is: 0.0004
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2029728209896452; val_accuracy: 0.9443670382165605 

Epoch 20 start
The current lr is: 0.0004
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1996868769074701; val_accuracy: 0.944765127388535 

Epoch 21 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.19537831819171358; val_accuracy: 0.9465565286624203 

Epoch 22 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.49; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1938506490106036; val_accuracy: 0.9460589171974523 

Epoch 23 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19143920185364735; val_accuracy: 0.9465565286624203 

Epoch 24 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1893429839686983; val_accuracy: 0.9473527070063694 

Epoch 25 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.91
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.11; acc: 1.0
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18746235088747779; val_accuracy: 0.9476512738853503 

Epoch 26 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1855498045018524; val_accuracy: 0.9483479299363057 

Epoch 27 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.11; acc: 1.0
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18404665404254464; val_accuracy: 0.948546974522293 

Epoch 28 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.09; acc: 1.0
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.86
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18279546646365694; val_accuracy: 0.949343152866242 

Epoch 29 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.07; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18068905404893457; val_accuracy: 0.9500398089171974 

Epoch 30 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.15; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.08; acc: 1.0
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17960297150217044; val_accuracy: 0.949343152866242 

Epoch 31 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17787159122763926; val_accuracy: 0.9499402866242038 

Epoch 32 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.14; acc: 1.0
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.09; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17736131716875514; val_accuracy: 0.9496417197452229 

Epoch 33 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17653663793377056; val_accuracy: 0.9508359872611465 

Epoch 34 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.11; acc: 1.0
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.176025980620817; val_accuracy: 0.9507364649681529 

Epoch 35 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.86
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.89
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1752505433170279; val_accuracy: 0.9501393312101911 

Epoch 36 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1745723719904377; val_accuracy: 0.9511345541401274 

Epoch 37 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17463121753019892; val_accuracy: 0.9507364649681529 

Epoch 38 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.09; acc: 1.0
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.08; acc: 1.0
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1734506520116405; val_accuracy: 0.951234076433121 

Epoch 39 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1731334619556263; val_accuracy: 0.9509355095541401 

Epoch 40 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17248314512288496; val_accuracy: 0.9517316878980892 

Epoch 41 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1722186869781488; val_accuracy: 0.9514331210191083 

Epoch 42 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1718857814172271; val_accuracy: 0.9517316878980892 

Epoch 43 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17182256471199595; val_accuracy: 0.9515326433121019 

Epoch 44 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.84
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17158754955336547; val_accuracy: 0.9513335987261147 

Epoch 45 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17143325199177312; val_accuracy: 0.9514331210191083 

Epoch 46 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.08; acc: 1.0
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.170882574644438; val_accuracy: 0.9519307324840764 

Epoch 47 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1706253669586531; val_accuracy: 0.9518312101910829 

Epoch 48 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.35; acc: 0.84
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.26; acc: 0.84
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17041744071112316; val_accuracy: 0.9521297770700637 

Epoch 49 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1704199926061615; val_accuracy: 0.9517316878980892 

Epoch 50 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17023531774616546; val_accuracy: 0.9516321656050956 

plots/no_subspace_training/lenet/2020-01-18 20:34:42/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.279939571004005; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214438078509774; val_accuracy: 0.3807722929936306 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.38
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814667349408387; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.56
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9483971478073461; val_accuracy: 0.7580613057324841 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818839640754043; val_accuracy: 0.8246417197452229 

Epoch 6 start
The current lr is: 0.0004
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.85; acc: 0.7
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.67
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.61; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.56; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.78
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.78
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.77
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.8
Batch: 700; loss: 0.66; acc: 0.78
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.83 

Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.36; acc: 0.92
Val Epoch over. val_loss: 0.5129176742711644; val_accuracy: 0.8508160828025477 

Epoch 7 start
The current lr is: 0.0004
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.78
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.77
Batch: 180; loss: 0.78; acc: 0.75
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.77
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.8
Batch: 320; loss: 0.63; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.73
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.77; acc: 0.77
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.58; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.46949483710489454; val_accuracy: 0.8617635350318471 

Epoch 8 start
The current lr is: 0.0004
Batch: 0; loss: 0.79; acc: 0.78
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.95
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.72
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.4; acc: 0.83
Batch: 540; loss: 0.53; acc: 0.8
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.43446570872121554; val_accuracy: 0.8753980891719745 

Epoch 9 start
The current lr is: 0.0004
Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.81
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.83
Batch: 500; loss: 0.44; acc: 0.94
Batch: 520; loss: 0.53; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.75
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.4056066220543187; val_accuracy: 0.8824641719745223 

Epoch 10 start
The current lr is: 0.0004
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.66; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.78
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3799266994568952; val_accuracy: 0.890625 

Epoch 11 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.78
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.3708559556087111; val_accuracy: 0.8919187898089171 

Epoch 12 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.67; acc: 0.78
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.89
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.36198332322061444; val_accuracy: 0.8930135350318471 

Epoch 13 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.98
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.62; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.35383877827293553; val_accuracy: 0.8972929936305732 

Epoch 14 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.81
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3466219220571457; val_accuracy: 0.8982882165605095 

Epoch 15 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.14; acc: 1.0
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.83
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3397408289134882; val_accuracy: 0.9021695859872612 

Epoch 16 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3367438997812332; val_accuracy: 0.9018710191082803 

Epoch 17 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3334980810144145; val_accuracy: 0.9023686305732485 

Epoch 18 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.33079567100781543; val_accuracy: 0.9029657643312102 

Epoch 19 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.84
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3281517261342638; val_accuracy: 0.9044585987261147 

Epoch 20 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3256058672051521; val_accuracy: 0.9070461783439491 

Epoch 21 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.71; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.32421164565784916; val_accuracy: 0.9074442675159236 

Epoch 22 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.94
Batch: 540; loss: 0.59; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.97
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3233635868331429; val_accuracy: 0.9070461783439491 

Epoch 23 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.64; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.81
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3221728156801242; val_accuracy: 0.9070461783439491 

Epoch 24 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.78
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3210628168408278; val_accuracy: 0.9077428343949044 

Epoch 25 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.73; acc: 0.8
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.8; acc: 0.75
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.51; acc: 0.8
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3200238648873226; val_accuracy: 0.9086385350318471 

Epoch 26 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.81
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31967945657907776; val_accuracy: 0.9083399681528662 

Epoch 27 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.8
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.81
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31936270212102086; val_accuracy: 0.9083399681528662 

Epoch 28 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31896576204687166; val_accuracy: 0.9085390127388535 

Epoch 29 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.84
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.68; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3184878825192239; val_accuracy: 0.908937101910828 

Epoch 30 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.18; acc: 1.0
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.64; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31812564587327324; val_accuracy: 0.9087380573248408 

Epoch 31 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.74; acc: 0.83
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.84
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.317958117243211; val_accuracy: 0.9088375796178344 

Epoch 32 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3177852960908489; val_accuracy: 0.9086385350318471 

Epoch 33 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.83
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31761684249730626; val_accuracy: 0.9088375796178344 

Epoch 34 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.63; acc: 0.8
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.55; acc: 0.81
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.317477436154891; val_accuracy: 0.9087380573248408 

Epoch 35 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3172937121930396; val_accuracy: 0.9087380573248408 

Epoch 36 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.83
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.37; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.97
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31722915205796054; val_accuracy: 0.9090366242038217 

Epoch 37 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31717598376570233; val_accuracy: 0.908937101910828 

Epoch 38 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.84
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.8
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3171065753907155; val_accuracy: 0.908937101910828 

Epoch 39 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3170407128751658; val_accuracy: 0.9090366242038217 

Epoch 40 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.8
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3169767672468902; val_accuracy: 0.9091361464968153 

Epoch 41 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.81
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31695261137310865; val_accuracy: 0.9091361464968153 

Epoch 42 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31692669165741866; val_accuracy: 0.9091361464968153 

Epoch 43 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.7; acc: 0.77
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31690514956120475; val_accuracy: 0.9091361464968153 

Epoch 44 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.78
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31688130333734926; val_accuracy: 0.9091361464968153 

Epoch 45 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.84
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.92
Batch: 460; loss: 0.69; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.57; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.8
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.83
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3168576955320729; val_accuracy: 0.9091361464968153 

Epoch 46 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31684867382808857; val_accuracy: 0.9091361464968153 

Epoch 47 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.95
Batch: 300; loss: 0.45; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31683902646515777; val_accuracy: 0.9091361464968153 

Epoch 48 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.4; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3168298161238622; val_accuracy: 0.9091361464968153 

Epoch 49 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.84
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3168209136291674; val_accuracy: 0.9092356687898089 

Epoch 50 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.95
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31681209873811456; val_accuracy: 0.9092356687898089 

plots/no_subspace_training/lenet/2020-01-18 20:39:47/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.279939697046948; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214441240213479; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814856474566612; val_accuracy: 0.502687101910828 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484202801042302; val_accuracy: 0.7580613057324841 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818543267098202; val_accuracy: 0.8249402866242038 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.45923852398517023; val_accuracy: 0.8599721337579618 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.383231178922638; val_accuracy: 0.8884355095541401 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.34427490166038466; val_accuracy: 0.9013734076433121 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2981248992453715; val_accuracy: 0.9138136942675159 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26862793643573285; val_accuracy: 0.924562101910828 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.25304372160203137; val_accuracy: 0.9287420382165605 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.23676722769619554; val_accuracy: 0.9322253184713376 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21693865197953904; val_accuracy: 0.9399880573248408 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21163084901347282; val_accuracy: 0.9391918789808917 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1892849066929453; val_accuracy: 0.946656050955414 

Epoch 16 start
The current lr is: 0.0002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18341650448407337; val_accuracy: 0.9491441082802548 

Epoch 17 start
The current lr is: 0.0002
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18018827708378718; val_accuracy: 0.9501393312101911 

Epoch 18 start
The current lr is: 0.0002
Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17896063398024079; val_accuracy: 0.9492436305732485 

Epoch 19 start
The current lr is: 0.0002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1770549884457497; val_accuracy: 0.9502388535031847 

Epoch 20 start
The current lr is: 0.0002
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1753547311920649; val_accuracy: 0.9516321656050956 

Epoch 21 start
The current lr is: 0.0002
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.46; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17372467456634638; val_accuracy: 0.9517316878980892 

Epoch 22 start
The current lr is: 0.0002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.47; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17248777957383993; val_accuracy: 0.9510350318471338 

Epoch 23 start
The current lr is: 0.0002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.16; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17007924478714634; val_accuracy: 0.9513335987261147 

Epoch 24 start
The current lr is: 0.0002
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.86
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1680820549179794; val_accuracy: 0.9526273885350318 

Epoch 25 start
The current lr is: 0.0002
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.08; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16627899013981698; val_accuracy: 0.9529259554140127 

Epoch 26 start
The current lr is: 0.0002
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.09; acc: 1.0
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16456547253735507; val_accuracy: 0.9535230891719745 

Epoch 27 start
The current lr is: 0.0002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.09; acc: 1.0
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16327647050949418; val_accuracy: 0.9540207006369427 

Epoch 28 start
The current lr is: 0.0002
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.86
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16220476933915146; val_accuracy: 0.9543192675159236 

Epoch 29 start
The current lr is: 0.0002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.09; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1601086102995523; val_accuracy: 0.9545183121019108 

Epoch 30 start
The current lr is: 0.0002
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15897013337179355; val_accuracy: 0.9548168789808917 

Epoch 31 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.07; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.91
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1575743335827141; val_accuracy: 0.9557125796178344 

Epoch 32 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.12; acc: 1.0
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.08; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15736970001724876; val_accuracy: 0.9556130573248408 

Epoch 33 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.11; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15691893341340077; val_accuracy: 0.955812101910828 

Epoch 34 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.08; acc: 1.0
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15675638905566208; val_accuracy: 0.9556130573248408 

Epoch 35 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15634149009255088; val_accuracy: 0.9561106687898089 

Epoch 36 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.07; acc: 1.0
Batch: 580; loss: 0.14; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15596539237696655; val_accuracy: 0.9564092356687898 

Epoch 37 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15616810665863334; val_accuracy: 0.9562101910828026 

Epoch 38 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15550609682775607; val_accuracy: 0.9564092356687898 

Epoch 39 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15542191826993493; val_accuracy: 0.9565087579617835 

Epoch 40 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15508173315957852; val_accuracy: 0.9562101910828026 

Epoch 41 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15490889468580293; val_accuracy: 0.956906847133758 

Epoch 42 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.16; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15455747528630456; val_accuracy: 0.9562101910828026 

Epoch 43 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15444876569186805; val_accuracy: 0.9563097133757962 

Epoch 44 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.44; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.07; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.86
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.88
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15428010182111127; val_accuracy: 0.9570063694267515 

Epoch 45 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15399315834614882; val_accuracy: 0.9567078025477707 

Epoch 46 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15359090218783183; val_accuracy: 0.9574044585987261 

Epoch 47 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.09; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15349848990797238; val_accuracy: 0.9573049363057324 

Epoch 48 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.23; acc: 0.86
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.1; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.153457639964333; val_accuracy: 0.9573049363057324 

Epoch 49 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15344499004114964; val_accuracy: 0.9571058917197452 

Epoch 50 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15341721067003383; val_accuracy: 0.9573049363057324 

plots/no_subspace_training/lenet/2020-01-18 20:44:52/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.2799397031213067; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214441291845528; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814797310312843; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.56
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484183757927767; val_accuracy: 0.7579617834394905 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818714186264451; val_accuracy: 0.8246417197452229 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.45927308091692104; val_accuracy: 0.8602707006369427 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.38324157893657684; val_accuracy: 0.8884355095541401 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.34422121856622634; val_accuracy: 0.9013734076433121 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2980701384271026; val_accuracy: 0.9136146496815286 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2685047354383074; val_accuracy: 0.924562101910828 

Epoch 11 start
The current lr is: 0.0002
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.26132774054055; val_accuracy: 0.9267515923566879 

Epoch 12 start
The current lr is: 0.0002
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.256195291829337; val_accuracy: 0.9286425159235668 

Epoch 13 start
The current lr is: 0.0002
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.98
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.25194943682023674; val_accuracy: 0.9293391719745223 

Epoch 14 start
The current lr is: 0.0002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.98
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.2483607149048216; val_accuracy: 0.9300358280254777 

Epoch 15 start
The current lr is: 0.0002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.08; acc: 1.0
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24457993204141878; val_accuracy: 0.9317277070063694 

Epoch 16 start
The current lr is: 0.0002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.24042430168883816; val_accuracy: 0.933718152866242 

Epoch 17 start
The current lr is: 0.0002
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2356289213962236; val_accuracy: 0.9351114649681529 

Epoch 18 start
The current lr is: 0.0002
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23261210898969584; val_accuracy: 0.9363057324840764 

Epoch 19 start
The current lr is: 0.0002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.97
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.88
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.09; acc: 1.0
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.22894617772785722; val_accuracy: 0.9372014331210191 

Epoch 20 start
The current lr is: 0.0002
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2259842534875794; val_accuracy: 0.9383957006369427 

Epoch 21 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.57; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22458087194971976; val_accuracy: 0.9384952229299363 

Epoch 22 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.53; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22421277978807497; val_accuracy: 0.9383957006369427 

Epoch 23 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22324268968336902; val_accuracy: 0.939390923566879 

Epoch 24 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22250121585123098; val_accuracy: 0.9394904458598726 

Epoch 25 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22183765634704547; val_accuracy: 0.9396894904458599 

Epoch 26 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.1; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22128102404012043; val_accuracy: 0.9397890127388535 

Epoch 27 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.08; acc: 1.0
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22091323267787125; val_accuracy: 0.9395899681528662 

Epoch 28 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.84
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22006707042930232; val_accuracy: 0.9394904458598726 

Epoch 29 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21932580303044835; val_accuracy: 0.9403861464968153 

Epoch 30 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.1; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21875985991802943; val_accuracy: 0.9404856687898089 

Epoch 31 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.84
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21854132591823863; val_accuracy: 0.9405851910828026 

Epoch 32 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.84
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21841934729059032; val_accuracy: 0.9407842356687898 

Epoch 33 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.88
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21825842478662538; val_accuracy: 0.9410828025477707 

Epoch 34 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.17; acc: 1.0
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2181802127676405; val_accuracy: 0.9408837579617835 

Epoch 35 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.08; acc: 1.0
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21800451913172272; val_accuracy: 0.9408837579617835 

Epoch 36 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.13; acc: 1.0
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.08; acc: 1.0
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21785640671469603; val_accuracy: 0.9411823248407644 

Epoch 37 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21784309826933654; val_accuracy: 0.9408837579617835 

Epoch 38 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21763955080395292; val_accuracy: 0.9410828025477707 

Epoch 39 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.08; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21753053832205999; val_accuracy: 0.941281847133758 

Epoch 40 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21738169686334907; val_accuracy: 0.9409832802547771 

Epoch 41 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21736336639447576; val_accuracy: 0.9410828025477707 

Epoch 42 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2173398878116896; val_accuracy: 0.9411823248407644 

Epoch 43 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.09; acc: 1.0
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2173247550892982; val_accuracy: 0.9410828025477707 

Epoch 44 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.12; acc: 1.0
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.84
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21730302494897205; val_accuracy: 0.9410828025477707 

Epoch 45 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.11; acc: 1.0
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.83
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21728261422579456; val_accuracy: 0.9411823248407644 

Epoch 46 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.98
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21725855874502734; val_accuracy: 0.9411823248407644 

Epoch 47 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.09; acc: 1.0
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21723009638820484; val_accuracy: 0.9411823248407644 

Epoch 48 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.44; acc: 0.83
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.83
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.84
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.11; acc: 1.0
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2172037990419728; val_accuracy: 0.9411823248407644 

Epoch 49 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21718037372846513; val_accuracy: 0.9411823248407644 

Epoch 50 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.43; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21716035354384192; val_accuracy: 0.9411823248407644 

plots/no_subspace_training/lenet/2020-01-18 20:49:58/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.279939701602717; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214440303243649; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.38
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.881465435787371; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.83
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.948395145167211; val_accuracy: 0.7579617834394905 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.81
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818651804498806; val_accuracy: 0.8250398089171974 

Epoch 6 start
The current lr is: 0.0002
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.85; acc: 0.69
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.95
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.69
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.74; acc: 0.78
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.78
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.77
Batch: 480; loss: 0.62; acc: 0.78
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.77
Batch: 540; loss: 0.9; acc: 0.77
Batch: 560; loss: 0.69; acc: 0.8
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.6; train_accuracy: 0.82 

Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.67
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.5384774884790372; val_accuracy: 0.8435509554140127 

Epoch 7 start
The current lr is: 0.0002
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.77
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.81; acc: 0.77
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.77
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.69; acc: 0.73
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.65; acc: 0.77
Batch: 500; loss: 0.73; acc: 0.73
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.78
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.8; acc: 0.73
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.8
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.83 

Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.7
Batch: 140; loss: 0.35; acc: 0.92
Val Epoch over. val_loss: 0.5115483824614506; val_accuracy: 0.8509156050955414 

Epoch 8 start
The current lr is: 0.0002
Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.76; acc: 0.7
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.43; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.8
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.58; acc: 0.77
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.58; acc: 0.88
Batch: 400; loss: 0.75; acc: 0.77
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.8
Batch: 460; loss: 0.68; acc: 0.73
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.55; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.8
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4876152507628605; val_accuracy: 0.8587778662420382 

Epoch 9 start
The current lr is: 0.0002
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.61; acc: 0.78
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.63; acc: 0.78
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.8
Batch: 500; loss: 0.49; acc: 0.94
Batch: 520; loss: 0.59; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.72
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.46770413181964; val_accuracy: 0.8647492038216561 

Epoch 10 start
The current lr is: 0.0002
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.8
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.73; acc: 0.8
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.78
Batch: 600; loss: 0.48; acc: 0.91
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.8
Batch: 740; loss: 0.55; acc: 0.77
Batch: 760; loss: 0.45; acc: 0.77
Batch: 780; loss: 0.4; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.92
Val Epoch over. val_loss: 0.44889237451705205; val_accuracy: 0.8710191082802548 

Epoch 11 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.64; acc: 0.77
Batch: 340; loss: 0.61; acc: 0.73
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.78; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.28; acc: 0.92
Val Epoch over. val_loss: 0.4454628367712543; val_accuracy: 0.8719148089171974 

Epoch 12 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.48; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.8
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.8
Batch: 500; loss: 0.55; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.78; acc: 0.72
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.94
Val Epoch over. val_loss: 0.4421500012183645; val_accuracy: 0.8732085987261147 

Epoch 13 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.97
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.95
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.75
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.59; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.43859492175897974; val_accuracy: 0.8742038216560509 

Epoch 14 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.34; acc: 0.94
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.5; acc: 0.92
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.75
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.8
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.86 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.43554932999003465; val_accuracy: 0.8755971337579618 

Epoch 15 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.83
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.81
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.66; acc: 0.78
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.81
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.86 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.4325995775544719; val_accuracy: 0.8758957006369427 

Epoch 16 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.5; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.46; acc: 0.94
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.63; acc: 0.77
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.71; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.66; acc: 0.78
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.4318757785163867; val_accuracy: 0.8757961783439491 

Epoch 17 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.78
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.64; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.81
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.94
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.53; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.431167069039527; val_accuracy: 0.8764928343949044 

Epoch 18 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.68; acc: 0.75
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.63; acc: 0.77
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.8
Batch: 580; loss: 0.65; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.43052266082566254; val_accuracy: 0.8765923566878981 

Epoch 19 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.77
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.75
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4299453130573224; val_accuracy: 0.8769904458598726 

Epoch 20 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.8
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4293251038546775; val_accuracy: 0.8770899681528662 

Epoch 21 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.78; acc: 0.83
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42919493291028743; val_accuracy: 0.8771894904458599 

Epoch 22 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.61; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.92
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.81
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.429067497325551; val_accuracy: 0.8773885350318471 

Epoch 23 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.81
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.8
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42893943522766137; val_accuracy: 0.8772890127388535 

Epoch 24 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.78
Batch: 180; loss: 0.73; acc: 0.77
Batch: 200; loss: 0.68; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.81
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.83
Batch: 580; loss: 0.49; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42881063253256924; val_accuracy: 0.8772890127388535 

Epoch 25 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.8
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.89; acc: 0.77
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.69; acc: 0.78
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.94; acc: 0.73
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.64; acc: 0.78
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.77
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.4; acc: 0.94
Batch: 720; loss: 0.59; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4286876363549263; val_accuracy: 0.8773885350318471 

Epoch 26 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.62; acc: 0.73
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.63; acc: 0.84
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.95
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.54; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42866408065625816; val_accuracy: 0.8773885350318471 

Epoch 27 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.8
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.59; acc: 0.78
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42864105371153277; val_accuracy: 0.8773885350318471 

Epoch 28 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.74; acc: 0.73
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.55; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4286185988954678; val_accuracy: 0.8773885350318471 

Epoch 29 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.46; acc: 0.81
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.64; acc: 0.77
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.77; acc: 0.8
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285953623853671; val_accuracy: 0.8773885350318471 

Epoch 30 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.8
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.67; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.61; acc: 0.77
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.8
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.72; acc: 0.8
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285722791579119; val_accuracy: 0.8773885350318471 

Epoch 31 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.78
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.82; acc: 0.8
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.78
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.5; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.66; acc: 0.72
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285692174913018; val_accuracy: 0.8773885350318471 

Epoch 32 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.54; acc: 0.8
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.8
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42856617803406566; val_accuracy: 0.8773885350318471 

Epoch 33 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.75
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.81; acc: 0.75
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.6; acc: 0.8
Batch: 300; loss: 0.38; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.57; acc: 0.75
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.6; acc: 0.88
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.81
Batch: 520; loss: 0.84; acc: 0.78
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285631197842823; val_accuracy: 0.8773885350318471 

Epoch 34 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.79; acc: 0.73
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.55; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42856005783293655; val_accuracy: 0.8773885350318471 

Epoch 35 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.81
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.77
Batch: 340; loss: 0.51; acc: 0.81
Batch: 360; loss: 0.33; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.83
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.71; acc: 0.8
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285569768992199; val_accuracy: 0.8773885350318471 

Epoch 36 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.85; acc: 0.72
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.55; acc: 0.78
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855678394341923; val_accuracy: 0.8773885350318471 

Epoch 37 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.6; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.78
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.68; acc: 0.81
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.8
Batch: 620; loss: 0.62; acc: 0.75
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.7; acc: 0.81
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855661632908376; val_accuracy: 0.8773885350318471 

Epoch 38 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.52; acc: 0.81
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.83
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.78
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.8
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.62; acc: 0.83
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285563937607844; val_accuracy: 0.8773885350318471 

Epoch 39 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.87; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.81
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.78
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855623810534266; val_accuracy: 0.8773885350318471 

Epoch 40 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.83
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.76; acc: 0.73
Batch: 300; loss: 0.58; acc: 0.8
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.75
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855602967890966; val_accuracy: 0.8773885350318471 

Epoch 41 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.75
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.65; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855601952334116; val_accuracy: 0.8773885350318471 

Epoch 42 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.67; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.78
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.81
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285560136388062; val_accuracy: 0.8773885350318471 

Epoch 43 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.81
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.55; acc: 0.8
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.86; acc: 0.7
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.68; acc: 0.84
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.8
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855600576112224; val_accuracy: 0.8773885350318471 

Epoch 44 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.74; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.4; acc: 0.94
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.68; acc: 0.75
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855600927286086; val_accuracy: 0.8773885350318471 

Epoch 45 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.95
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.81
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.75; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 0.55; acc: 0.8
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.8; acc: 0.78
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.428556000066411; val_accuracy: 0.8773885350318471 

Epoch 46 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.6; acc: 0.8
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.78
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.77
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.75
Batch: 580; loss: 0.71; acc: 0.77
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285560051916511; val_accuracy: 0.8773885350318471 

Epoch 47 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.78
Batch: 540; loss: 0.65; acc: 0.8
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855600177482434; val_accuracy: 0.8773885350318471 

Epoch 48 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.81
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.95
Batch: 600; loss: 0.61; acc: 0.8
Batch: 620; loss: 0.7; acc: 0.73
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.78
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.42855600348323775; val_accuracy: 0.8773885350318471 

Epoch 49 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.78
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.8
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.77
Batch: 600; loss: 0.84; acc: 0.81
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.72; acc: 0.75
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285560060458578; val_accuracy: 0.8773885350318471 

Epoch 50 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.95
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.4285560047170918; val_accuracy: 0.8773885350318471 

plots/no_subspace_training/lenet/2020-01-18 20:55:03/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.2799396924911792; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214439887150078; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.88147267718224; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484578257153748; val_accuracy: 0.7578622611464968 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.581871959718929; val_accuracy: 0.8250398089171974 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.45926094719558763; val_accuracy: 0.8603702229299363 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.38327925116013567; val_accuracy: 0.8884355095541401 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.3442947693216573; val_accuracy: 0.9012738853503185 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.298113524818876; val_accuracy: 0.9138136942675159 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2686539689541622; val_accuracy: 0.9244625796178344 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25303746971070384; val_accuracy: 0.9290406050955414 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2367582338251126; val_accuracy: 0.9324243630573248 

Epoch 13 start
The current lr is: 0.001
slurmstepd: error: _is_a_lwp: open() /proc/130017/status failed: No such file or directory
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21696307839956253; val_accuracy: 0.9399880573248408 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21148556577646807; val_accuracy: 0.9392914012738853 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1892860787119835; val_accuracy: 0.9467555732484076 

Epoch 16 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18390920962307863; val_accuracy: 0.9490445859872612 

Epoch 17 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1817702831830948; val_accuracy: 0.9495421974522293 

Epoch 18 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.37; acc: 0.81
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.08; acc: 1.0
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18091637475095737; val_accuracy: 0.9489450636942676 

Epoch 19 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17950101415063166; val_accuracy: 0.9495421974522293 

Epoch 20 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17824712756333078; val_accuracy: 0.9505374203821656 

Epoch 21 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.47; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17713623894912423; val_accuracy: 0.9505374203821656 

Epoch 22 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.47; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1764541423766856; val_accuracy: 0.9501393312101911 

Epoch 23 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.16; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17468243308222978; val_accuracy: 0.9503383757961783 

Epoch 24 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.86
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17333765686222702; val_accuracy: 0.9507364649681529 

Epoch 25 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.09; acc: 1.0
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1720836163753537; val_accuracy: 0.9515326433121019 

Epoch 26 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17086070370237538; val_accuracy: 0.9515326433121019 

Epoch 27 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.1; acc: 1.0
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17005540117336687; val_accuracy: 0.9521297770700637 

Epoch 28 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.86
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.26; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16894679825017406; val_accuracy: 0.9522292993630573 

Epoch 29 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.1; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16760269747038556; val_accuracy: 0.9528264331210191 

Epoch 30 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1667772608862561; val_accuracy: 0.9529259554140127 

Epoch 31 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16589958647823638; val_accuracy: 0.953125 

Epoch 32 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.13; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.08; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1657964185971743; val_accuracy: 0.953125 

Epoch 33 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16558549467735229; val_accuracy: 0.953125 

Epoch 34 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.09; acc: 1.0
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16553690839724935; val_accuracy: 0.9529259554140127 

Epoch 35 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1653069639519142; val_accuracy: 0.9533240445859873 

Epoch 36 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.86
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.08; acc: 1.0
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16512748527868537; val_accuracy: 0.9532245222929936 

Epoch 37 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.08; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16522637244527508; val_accuracy: 0.9530254777070064 

Epoch 38 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.08; acc: 1.0
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16491568211916904; val_accuracy: 0.9535230891719745 

Epoch 39 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.08; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1648345617399474; val_accuracy: 0.9534235668789809 

Epoch 40 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1646526709531143; val_accuracy: 0.9532245222929936 

Epoch 41 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16459941508094217; val_accuracy: 0.9535230891719745 

Epoch 42 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.164409027761145; val_accuracy: 0.9536226114649682 

Epoch 43 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16437066196920766; val_accuracy: 0.9532245222929936 

Epoch 44 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.07; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.86
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1642091187891687; val_accuracy: 0.9534235668789809 

Epoch 45 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16414440743577707; val_accuracy: 0.9534235668789809 

Epoch 46 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.08; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16403912838287416; val_accuracy: 0.9536226114649682 

Epoch 47 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16397912871495934; val_accuracy: 0.9535230891719745 

Epoch 48 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.25; acc: 0.86
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1639446252660387; val_accuracy: 0.9535230891719745 

Epoch 49 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1639208854858283; val_accuracy: 0.9535230891719745 

Epoch 50 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16390471867505152; val_accuracy: 0.9535230891719745 

plots/no_subspace_training/lenet/2020-01-18 21:00:09/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.279939517853366; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214434280516995; val_accuracy: 0.3808718152866242 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814550471153988; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9483840233960729; val_accuracy: 0.7580613057324841 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818500089797245; val_accuracy: 0.8247412420382165 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.45923640166118646; val_accuracy: 0.8603702229299363 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3832344698962892; val_accuracy: 0.8885350318471338 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.34425728886750095; val_accuracy: 0.9012738853503185 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2981490150188944; val_accuracy: 0.9136146496815286 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2683509971210911; val_accuracy: 0.9248606687898089 

Epoch 11 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2627283359883697; val_accuracy: 0.9257563694267515 

Epoch 12 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25928991112359767; val_accuracy: 0.926453025477707 

Epoch 13 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.98
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.256361490981594; val_accuracy: 0.927547770700637 

Epoch 14 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.98
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2538061615103369; val_accuracy: 0.9282444267515924 

Epoch 15 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.08; acc: 1.0
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2514487446588316; val_accuracy: 0.9294386942675159 

Epoch 16 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24859176137170214; val_accuracy: 0.9311305732484076 

Epoch 17 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24520484341462706; val_accuracy: 0.932921974522293 

Epoch 18 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2429793252117315; val_accuracy: 0.9327229299363057 

Epoch 19 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.88
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.240265528321456; val_accuracy: 0.9335191082802548 

Epoch 20 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.2; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23796322730601213; val_accuracy: 0.9341162420382165 

Epoch 21 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.6; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23728468610792403; val_accuracy: 0.9347133757961783 

Epoch 22 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.53; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23707030381366706; val_accuracy: 0.9350119426751592 

Epoch 23 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2366439270175946; val_accuracy: 0.9350119426751592 

Epoch 24 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23625348766992807; val_accuracy: 0.9350119426751592 

Epoch 25 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.84
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23594356938057645; val_accuracy: 0.9354100318471338 

Epoch 26 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.8
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23568713318580276; val_accuracy: 0.9351114649681529 

Epoch 27 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.235484343733947; val_accuracy: 0.9353105095541401 

Epoch 28 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23510401313946505; val_accuracy: 0.9353105095541401 

Epoch 29 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.11; acc: 1.0
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23470773415011206; val_accuracy: 0.935609076433121 

Epoch 30 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.11; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.53; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2344416179901855; val_accuracy: 0.935609076433121 

Epoch 31 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.84
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23439557912053577; val_accuracy: 0.935609076433121 

Epoch 32 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23435402255813786; val_accuracy: 0.935609076433121 

Epoch 33 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23430765123598896; val_accuracy: 0.9357085987261147 

Epoch 34 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2342704577478254; val_accuracy: 0.9357085987261147 

Epoch 35 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23422335596031443; val_accuracy: 0.9357085987261147 

Epoch 36 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.15; acc: 1.0
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.09; acc: 1.0
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2341778880803828; val_accuracy: 0.9357085987261147 

Epoch 37 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23414823669157211; val_accuracy: 0.9357085987261147 

Epoch 38 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23410129922020967; val_accuracy: 0.9357085987261147 

Epoch 39 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.09; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23405971601130857; val_accuracy: 0.9357085987261147 

Epoch 40 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23401702157441218; val_accuracy: 0.9358081210191083 

Epoch 41 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.98
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2340123699444115; val_accuracy: 0.9358081210191083 

Epoch 42 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.49; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23400719005875525; val_accuracy: 0.9358081210191083 

Epoch 43 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23400352017325202; val_accuracy: 0.9358081210191083 

Epoch 44 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23399908399316155; val_accuracy: 0.9358081210191083 

Epoch 45 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.12; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.83
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23399466518194054; val_accuracy: 0.9358081210191083 

Epoch 46 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23399018092899565; val_accuracy: 0.9358081210191083 

Epoch 47 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2339851258049725; val_accuracy: 0.9358081210191083 

Epoch 48 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.83
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.13; acc: 1.0
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.86
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23398052523754964; val_accuracy: 0.9358081210191083 

Epoch 49 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2339760015012732; val_accuracy: 0.9358081210191083 

Epoch 50 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.98
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.23397185743614368; val_accuracy: 0.9358081210191083 

plots/no_subspace_training/lenet/2020-01-18 21:05:14/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.279939694009769; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.2144394497962514; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814678321218794; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.56
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.83
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484170599348226; val_accuracy: 0.7578622611464968 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818540696885176; val_accuracy: 0.8248407643312102 

Epoch 6 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.84; acc: 0.69
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.77
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.69
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.75; acc: 0.77
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.77
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.63; acc: 0.77
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.9; acc: 0.77
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.51; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.81
Train Epoch over. train_loss: 0.6; train_accuracy: 0.82 

Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.67
Batch: 140; loss: 0.39; acc: 0.91
Val Epoch over. val_loss: 0.5488256448583239; val_accuracy: 0.8398686305732485 

Epoch 7 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.75
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.82; acc: 0.77
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.75
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.7; acc: 0.73
Batch: 380; loss: 0.55; acc: 0.81
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.66; acc: 0.73
Batch: 500; loss: 0.74; acc: 0.73
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 0.56; acc: 0.8
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 0.57; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.58; train_accuracy: 0.83 

Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.7
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.529380116682903; val_accuracy: 0.8459394904458599 

Epoch 8 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.69
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.77
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.77; acc: 0.75
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.8
Batch: 460; loss: 0.7; acc: 0.73
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.78
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.73
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.8
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.78
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.56; acc: 0.77
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.91
Train Epoch over. train_loss: 0.56; train_accuracy: 0.83 

Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.69
Batch: 140; loss: 0.35; acc: 0.92
Val Epoch over. val_loss: 0.5116057017236758; val_accuracy: 0.8528065286624203 

Epoch 9 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.78
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.66; acc: 0.75
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.58; acc: 0.8
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.8
Batch: 500; loss: 0.52; acc: 0.91
Batch: 520; loss: 0.61; acc: 0.78
Batch: 540; loss: 0.75; acc: 0.72
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.84 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4964293682840979; val_accuracy: 0.8569864649681529 

Epoch 10 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.77; acc: 0.75
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.77
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.77
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.78
Batch: 740; loss: 0.59; acc: 0.77
Batch: 760; loss: 0.48; acc: 0.77
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.48190702991500783; val_accuracy: 0.86046974522293 

Epoch 11 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.75
Batch: 340; loss: 0.65; acc: 0.72
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.81; acc: 0.77
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.77
Batch: 660; loss: 0.62; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.77
Batch: 700; loss: 0.66; acc: 0.8
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.75
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.48011682747275963; val_accuracy: 0.862062101910828 

Epoch 12 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.52; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.77
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.46; acc: 0.81
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.8
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.78
Batch: 500; loss: 0.59; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.82; acc: 0.7
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.71; acc: 0.77
Batch: 760; loss: 0.61; acc: 0.8
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.4784404841388107; val_accuracy: 0.8619625796178344 

Epoch 13 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.5; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.75; acc: 0.73
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.64; acc: 0.8
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.47660580799458135; val_accuracy: 0.863156847133758 

Epoch 14 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.81
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.84
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.64; acc: 0.75
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.4749750275710586; val_accuracy: 0.8626592356687898 

Epoch 15 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.8
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.8
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.63; acc: 0.78
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.81
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.6; acc: 0.81
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.81
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.4733383684021652; val_accuracy: 0.8643511146496815 

Epoch 16 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.71; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.83
Batch: 220; loss: 0.58; acc: 0.77
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.62; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.68; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.92
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.69
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.76; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.71; acc: 0.75
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.53; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4730969762346547; val_accuracy: 0.8641520700636943 

Epoch 17 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.89
Batch: 160; loss: 0.59; acc: 0.78
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.59; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.77
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.78
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4728530321721059; val_accuracy: 0.8641520700636943 

Epoch 18 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.72; acc: 0.7
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.82; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.81
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.83
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.68; acc: 0.75
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.78
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.77
Batch: 580; loss: 0.69; acc: 0.77
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.84
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4726192187161962; val_accuracy: 0.8643511146496815 

Epoch 19 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.77
Batch: 220; loss: 0.6; acc: 0.8
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.76; acc: 0.75
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.53; acc: 0.81
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4723991050272231; val_accuracy: 0.8646496815286624 

Epoch 20 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.51; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.5; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.78
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.81
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.51; acc: 0.81
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.57; acc: 0.77
Batch: 460; loss: 0.71; acc: 0.77
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.72; acc: 0.75
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.52; acc: 0.81
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.61; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47218088160274896; val_accuracy: 0.8646496815286624 

Epoch 21 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.79; acc: 0.83
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.66; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4721534671677146; val_accuracy: 0.8646496815286624 

Epoch 22 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.83
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.83
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.77; acc: 0.8
Batch: 480; loss: 0.53; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.92
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47212627435186105; val_accuracy: 0.8646496815286624 

Epoch 23 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.77; acc: 0.83
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.55; acc: 0.8
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.78
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.78
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.73; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.78
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720988377074527; val_accuracy: 0.8646496815286624 

Epoch 24 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.65; acc: 0.75
Batch: 180; loss: 0.78; acc: 0.77
Batch: 200; loss: 0.72; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.78
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.78
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.83
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47207100157904774; val_accuracy: 0.8647492038216561 

Epoch 25 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.69; acc: 0.75
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.94; acc: 0.73
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.73
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.8
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.98; acc: 0.72
Batch: 400; loss: 0.47; acc: 0.94
Batch: 420; loss: 0.69; acc: 0.77
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.81
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.45; acc: 0.94
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47204367702553984; val_accuracy: 0.8647492038216561 

Epoch 26 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.67; acc: 0.73
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.78
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.83
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.56; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720420478635533; val_accuracy: 0.8647492038216561 

Epoch 27 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.55; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.74; acc: 0.81
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.68; acc: 0.77
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.8
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.73; acc: 0.75
Batch: 660; loss: 0.61; acc: 0.77
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.63; acc: 0.77
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720403817808552; val_accuracy: 0.8647492038216561 

Epoch 28 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.8
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.79; acc: 0.75
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203878678713634; val_accuracy: 0.8647492038216561 

Epoch 29 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.5; acc: 0.81
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.81
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.7; acc: 0.75
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.6; acc: 0.78
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.5; acc: 0.81
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720371307650949; val_accuracy: 0.8647492038216561 

Epoch 30 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.73; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.77
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.78
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.78
Batch: 640; loss: 0.74; acc: 0.8
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720354089691381; val_accuracy: 0.8647492038216561 

Epoch 31 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.79; acc: 0.78
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.62; acc: 0.77
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.83; acc: 0.8
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.62; acc: 0.81
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.6; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.78
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.78
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.81
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.71; acc: 0.7
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720353833429373; val_accuracy: 0.8647492038216561 

Epoch 32 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.78
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.78
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.78
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.78
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.81
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.84
Batch: 720; loss: 0.66; acc: 0.8
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720353471815206; val_accuracy: 0.8647492038216561 

Epoch 33 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.73
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.85; acc: 0.73
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.66; acc: 0.78
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.75
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.62; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.88; acc: 0.75
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.77
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.81
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.88
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203532639582446; val_accuracy: 0.8647492038216561 

Epoch 34 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.77
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.84; acc: 0.72
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.8
Batch: 280; loss: 0.7; acc: 0.77
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.57; acc: 0.77
Batch: 460; loss: 0.56; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.54; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203528843108256; val_accuracy: 0.8647492038216561 

Epoch 35 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.77
Batch: 340; loss: 0.56; acc: 0.78
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.78
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.77; acc: 0.77
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720352579643772; val_accuracy: 0.8647492038216561 

Epoch 36 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.54; acc: 0.92
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.7; acc: 0.75
Batch: 260; loss: 0.54; acc: 0.8
Batch: 280; loss: 0.66; acc: 0.78
Batch: 300; loss: 0.64; acc: 0.81
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.9; acc: 0.7
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525644578753; val_accuracy: 0.8647492038216561 

Epoch 37 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.64; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.52; acc: 0.78
Batch: 600; loss: 0.53; acc: 0.75
Batch: 620; loss: 0.68; acc: 0.77
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.8
Batch: 700; loss: 0.75; acc: 0.8
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720352534086082; val_accuracy: 0.8647492038216561 

Epoch 38 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.59; acc: 0.8
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.61; acc: 0.8
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.68; acc: 0.75
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.8
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525891349574; val_accuracy: 0.8647492038216561 

Epoch 39 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.86
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.47; acc: 0.8
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.78
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.81
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203526327944106; val_accuracy: 0.8647492038216561 

Epoch 40 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.81
Batch: 200; loss: 0.62; acc: 0.78
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.8; acc: 0.72
Batch: 300; loss: 0.64; acc: 0.77
Batch: 320; loss: 0.48; acc: 0.83
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.73
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.58; acc: 0.8
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720352610964684; val_accuracy: 0.8647492038216561 

Epoch 41 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 0.49; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.62; acc: 0.73
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.81
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.94
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.472035260716821; val_accuracy: 0.8647492038216561 

Epoch 42 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.78
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.81
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.81
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525957787873; val_accuracy: 0.8647492038216561 

Epoch 43 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.54; acc: 0.78
Batch: 160; loss: 0.6; acc: 0.78
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.9; acc: 0.7
Batch: 220; loss: 0.69; acc: 0.78
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.72; acc: 0.83
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.71; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.53; acc: 0.8
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.6; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720352591982313; val_accuracy: 0.8647492038216561 

Epoch 44 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.76; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.56; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.55; acc: 0.8
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.64; acc: 0.84
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.77
Batch: 660; loss: 0.69; acc: 0.73
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.4720352590084076; val_accuracy: 0.8647492038216561 

Epoch 45 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.8
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.78
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.77; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.82; acc: 0.75
Batch: 520; loss: 0.59; acc: 0.8
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.81
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525862876017; val_accuracy: 0.8647492038216561 

Epoch 46 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.77
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.78
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.67; acc: 0.8
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.68; acc: 0.75
Batch: 540; loss: 0.43; acc: 0.91
Batch: 560; loss: 0.64; acc: 0.73
Batch: 580; loss: 0.76; acc: 0.72
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.64; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.63; acc: 0.78
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525862876017; val_accuracy: 0.8647492038216561 

Epoch 47 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.88
Batch: 260; loss: 0.55; acc: 0.8
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.8
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.75; acc: 0.8
Batch: 520; loss: 0.55; acc: 0.77
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.72; acc: 0.8
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525862876017; val_accuracy: 0.8647492038216561 

Epoch 48 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.73; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.51; acc: 0.78
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.71; acc: 0.8
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.78
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.75; acc: 0.83
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.66; acc: 0.78
Batch: 620; loss: 0.76; acc: 0.72
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.64; acc: 0.84
Batch: 720; loss: 0.57; acc: 0.75
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525862876017; val_accuracy: 0.8647492038216561 

Epoch 49 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.6; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.6; acc: 0.75
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.74; acc: 0.75
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.8
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.77
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.72; acc: 0.77
Batch: 600; loss: 0.88; acc: 0.77
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.76; acc: 0.73
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525862876017; val_accuracy: 0.8647492038216561 

Epoch 50 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.57; acc: 0.86
Batch: 340; loss: 0.7; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.92
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.6; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.62; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.47203525862876017; val_accuracy: 0.8647492038216561 

plots/no_subspace_training/lenet/2020-01-18 21:10:19/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.2799396864168204; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214439489279583; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814725306383364; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484281684183011; val_accuracy: 0.7579617834394905 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818670274345739; val_accuracy: 0.8246417197452229 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.45925604243567036; val_accuracy: 0.8602707006369427 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3832494937785112; val_accuracy: 0.8883359872611465 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.34431284376580246; val_accuracy: 0.9011743630573248 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2981184325685167; val_accuracy: 0.9138136942675159 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26857622110160295; val_accuracy: 0.9246616242038217 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25306483182557826; val_accuracy: 0.9288415605095541 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.23670353842483963; val_accuracy: 0.9324243630573248 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.5; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2169246642500352; val_accuracy: 0.9399880573248408 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2116154900686756; val_accuracy: 0.9392914012738853 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18927161775197193; val_accuracy: 0.946656050955414 

Epoch 16 start
The current lr is: 0.0001
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18407108293027635; val_accuracy: 0.9488455414012739 

Epoch 17 start
The current lr is: 0.0001
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1824852516222152; val_accuracy: 0.9495421974522293 

Epoch 18 start
The current lr is: 0.0001
Batch: 0; loss: 0.38; acc: 0.81
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.08; acc: 1.0
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1817253633242124; val_accuracy: 0.9484474522292994 

Epoch 19 start
The current lr is: 0.0001
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18057966360430808; val_accuracy: 0.9494426751592356 

Epoch 20 start
The current lr is: 0.0001
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1795880786220359; val_accuracy: 0.9501393312101911 

Epoch 21 start
The current lr is: 0.0001
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.47; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17865721556316516; val_accuracy: 0.9495421974522293 

Epoch 22 start
The current lr is: 0.0001
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.98
Batch: 460; loss: 0.48; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.17820549846454792; val_accuracy: 0.9498407643312102 

Epoch 23 start
The current lr is: 0.0001
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.17; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17675978078204355; val_accuracy: 0.950437898089172 

Epoch 24 start
The current lr is: 0.0001
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.86
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1757060606625809; val_accuracy: 0.9508359872611465 

Epoch 25 start
The current lr is: 0.0001
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.09; acc: 1.0
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1747232475762914; val_accuracy: 0.9508359872611465 

Epoch 26 start
The current lr is: 0.0001
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1737635447198798; val_accuracy: 0.9509355095541401 

Epoch 27 start
The current lr is: 0.0001
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.1; acc: 1.0
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.84
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17320321469455008; val_accuracy: 0.9515326433121019 

Epoch 28 start
The current lr is: 0.0001
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.86
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17213084698197947; val_accuracy: 0.9515326433121019 

Epoch 29 start
The current lr is: 0.0001
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.1; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.19; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17111492100035308; val_accuracy: 0.9518312101910829 

Epoch 30 start
The current lr is: 0.0001
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.08; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1703846141173961; val_accuracy: 0.9519307324840764 

Epoch 31 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16983847550241052; val_accuracy: 0.9518312101910829 

Epoch 32 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.13; acc: 1.0
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.09; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16973925671380036; val_accuracy: 0.9519307324840764 

Epoch 33 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16961277734227242; val_accuracy: 0.95203025477707 

Epoch 34 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.1; acc: 1.0
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16957198365763493; val_accuracy: 0.95203025477707 

Epoch 35 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16943748515977222; val_accuracy: 0.9521297770700637 

Epoch 36 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.86
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1693272989978836; val_accuracy: 0.9522292993630573 

Epoch 37 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.09; acc: 1.0
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16936046710819197; val_accuracy: 0.9521297770700637 

Epoch 38 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.08; acc: 1.0
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16919847537472749; val_accuracy: 0.9517316878980892 

Epoch 39 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.08; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16912851800584489; val_accuracy: 0.9517316878980892 

Epoch 40 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16901569222663618; val_accuracy: 0.9523288216560509 

Epoch 41 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1689754407971528; val_accuracy: 0.9522292993630573 

Epoch 42 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16886773881069414; val_accuracy: 0.9523288216560509 

Epoch 43 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16883390515473237; val_accuracy: 0.95203025477707 

Epoch 44 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.86
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16872333282023477; val_accuracy: 0.9521297770700637 

Epoch 45 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1686837726575174; val_accuracy: 0.9522292993630573 

Epoch 46 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.08; acc: 1.0
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16865304088706423; val_accuracy: 0.9521297770700637 

Epoch 47 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16862717655244147; val_accuracy: 0.9521297770700637 

Epoch 48 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.26; acc: 0.84
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1686081053202699; val_accuracy: 0.9522292993630573 

Epoch 49 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16859235287092295; val_accuracy: 0.9523288216560509 

Epoch 50 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.39; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16858016706670925; val_accuracy: 0.9523288216560509 

plots/no_subspace_training/lenet/2020-01-18 21:15:25/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.279939689454; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.2144412341391204; val_accuracy: 0.38067277070063693 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814841189961524; val_accuracy: 0.502687101910828 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484238457527889; val_accuracy: 0.7579617834394905 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818637563924122; val_accuracy: 0.8249402866242038 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.67; acc: 0.72
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.78
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.84; acc: 0.77
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.8
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.92
Val Epoch over. val_loss: 0.4592792857794245; val_accuracy: 0.8603702229299363 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.95
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.3832604917371349; val_accuracy: 0.8885350318471338 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.95
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.8
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.34418181870963166; val_accuracy: 0.9015724522292994 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.29810587440136893; val_accuracy: 0.9137141719745223 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.26857245196202756; val_accuracy: 0.9246616242038217 

Epoch 11 start
The current lr is: 0.0001
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.26355067260895565; val_accuracy: 0.9255573248407644 

Epoch 12 start
The current lr is: 0.0001
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.26090983377330623; val_accuracy: 0.9262539808917197 

Epoch 13 start
The current lr is: 0.0001
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.98
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2585632479304721; val_accuracy: 0.9270501592356688 

Epoch 14 start
The current lr is: 0.0001
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.98
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25647893638178043; val_accuracy: 0.9274482484076433 

Epoch 15 start
The current lr is: 0.0001
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.09; acc: 1.0
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2546703561570994; val_accuracy: 0.9282444267515924 

Epoch 16 start
The current lr is: 0.0001
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2524251560117029; val_accuracy: 0.9296377388535032 

Epoch 17 start
The current lr is: 0.0001
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24977824286480618; val_accuracy: 0.9314291401273885 

Epoch 18 start
The current lr is: 0.0001
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.98
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24787314777161665; val_accuracy: 0.9315286624203821 

Epoch 19 start
The current lr is: 0.0001
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.83
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24566488349988203; val_accuracy: 0.9326234076433121 

Epoch 20 start
The current lr is: 0.0001
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.24376998656684426; val_accuracy: 0.9320262738853503 

Epoch 21 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24331590393261546; val_accuracy: 0.9333200636942676 

Epoch 22 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.98
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.98
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24313858614131145; val_accuracy: 0.9335191082802548 

Epoch 23 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24289421905662603; val_accuracy: 0.9335191082802548 

Epoch 24 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.84
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24264870773834787; val_accuracy: 0.9336186305732485 

Epoch 25 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.68; acc: 0.8
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24245115929538277; val_accuracy: 0.9339171974522293 

Epoch 26 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.78
Batch: 40; loss: 0.12; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.33; acc: 0.83
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2422763966735761; val_accuracy: 0.9333200636942676 

Epoch 27 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2421322190647672; val_accuracy: 0.9334195859872612 

Epoch 28 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24191971229994372; val_accuracy: 0.933718152866242 

Epoch 29 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.12; acc: 1.0
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24166842350724396; val_accuracy: 0.9338176751592356 

Epoch 30 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.12; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.54; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24149483176553324; val_accuracy: 0.9340167197452229 

Epoch 31 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.86
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2414741938945594; val_accuracy: 0.9340167197452229 

Epoch 32 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24145453579866202; val_accuracy: 0.9340167197452229 

Epoch 33 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24143345573335695; val_accuracy: 0.9340167197452229 

Epoch 34 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24141436990848772; val_accuracy: 0.9340167197452229 

Epoch 35 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24139282282940142; val_accuracy: 0.9340167197452229 

Epoch 36 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.15; acc: 1.0
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.09; acc: 1.0
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24137161880921407; val_accuracy: 0.9340167197452229 

Epoch 37 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24135431243925337; val_accuracy: 0.9340167197452229 

Epoch 38 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24133315857998125; val_accuracy: 0.9340167197452229 

Epoch 39 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.97
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.61; acc: 0.84
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.98
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.09; acc: 1.0
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24131250806200277; val_accuracy: 0.9340167197452229 

Epoch 40 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.84
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24129209786084047; val_accuracy: 0.9340167197452229 

Epoch 41 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24129063951646446; val_accuracy: 0.9340167197452229 

Epoch 42 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24128898199956128; val_accuracy: 0.9340167197452229 

Epoch 43 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.241287804143444; val_accuracy: 0.9340167197452229 

Epoch 44 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.57; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24128640383766714; val_accuracy: 0.9340167197452229 

Epoch 45 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.84
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.12; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.94
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.41; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.83
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24128506999391658; val_accuracy: 0.9340167197452229 

Epoch 46 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24128365734959864; val_accuracy: 0.9340167197452229 

Epoch 47 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24128210250359433; val_accuracy: 0.9340167197452229 

Epoch 48 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.83
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.13; acc: 1.0
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24128071811928112; val_accuracy: 0.9340167197452229 

Epoch 49 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2412793293452946; val_accuracy: 0.9340167197452229 

Epoch 50 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.45; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2412780284122297; val_accuracy: 0.9340167197452229 

plots/no_subspace_training/lenet/2020-01-18 21:20:30/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.06
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.29; acc: 0.08
Batch: 520; loss: 2.3; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.06
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.08
Val Epoch over. val_loss: 2.279939689454; val_accuracy: 0.11464968152866242 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.19
Batch: 140; loss: 2.28; acc: 0.08
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.09
Batch: 220; loss: 2.27; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.17
Batch: 260; loss: 2.27; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.25; acc: 0.25
Batch: 400; loss: 2.26; acc: 0.25
Batch: 420; loss: 2.26; acc: 0.28
Batch: 440; loss: 2.24; acc: 0.31
Batch: 460; loss: 2.25; acc: 0.25
Batch: 480; loss: 2.25; acc: 0.34
Batch: 500; loss: 2.24; acc: 0.33
Batch: 520; loss: 2.24; acc: 0.3
Batch: 540; loss: 2.25; acc: 0.22
Batch: 560; loss: 2.24; acc: 0.42
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.34
Batch: 620; loss: 2.24; acc: 0.38
Batch: 640; loss: 2.25; acc: 0.27
Batch: 660; loss: 2.24; acc: 0.23
Batch: 680; loss: 2.25; acc: 0.3
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.22; acc: 0.3
Batch: 740; loss: 2.22; acc: 0.44
Batch: 760; loss: 2.23; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.41
Train Epoch over. train_loss: 2.25; train_accuracy: 0.23 

Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.21; acc: 0.45
Batch: 40; loss: 2.19; acc: 0.58
Batch: 60; loss: 2.2; acc: 0.44
Batch: 80; loss: 2.19; acc: 0.5
Batch: 100; loss: 2.23; acc: 0.39
Batch: 120; loss: 2.21; acc: 0.44
Batch: 140; loss: 2.21; acc: 0.3
Val Epoch over. val_loss: 2.214439887150078; val_accuracy: 0.38057324840764334 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 2.22; acc: 0.41
Batch: 20; loss: 2.2; acc: 0.39
Batch: 40; loss: 2.2; acc: 0.5
Batch: 60; loss: 2.2; acc: 0.39
Batch: 80; loss: 2.2; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.34
Batch: 120; loss: 2.2; acc: 0.41
Batch: 140; loss: 2.22; acc: 0.27
Batch: 160; loss: 2.19; acc: 0.38
Batch: 180; loss: 2.18; acc: 0.44
Batch: 200; loss: 2.15; acc: 0.5
Batch: 220; loss: 2.19; acc: 0.36
Batch: 240; loss: 2.17; acc: 0.39
Batch: 260; loss: 2.16; acc: 0.47
Batch: 280; loss: 2.16; acc: 0.38
Batch: 300; loss: 2.13; acc: 0.45
Batch: 320; loss: 2.14; acc: 0.45
Batch: 340; loss: 2.18; acc: 0.38
Batch: 360; loss: 2.13; acc: 0.48
Batch: 380; loss: 2.13; acc: 0.39
Batch: 400; loss: 2.12; acc: 0.47
Batch: 420; loss: 2.12; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.58
Batch: 460; loss: 2.12; acc: 0.36
Batch: 480; loss: 2.12; acc: 0.3
Batch: 500; loss: 2.08; acc: 0.44
Batch: 520; loss: 2.03; acc: 0.5
Batch: 540; loss: 2.04; acc: 0.41
Batch: 560; loss: 2.04; acc: 0.45
Batch: 580; loss: 2.02; acc: 0.47
Batch: 600; loss: 2.1; acc: 0.34
Batch: 620; loss: 2.08; acc: 0.41
Batch: 640; loss: 2.06; acc: 0.44
Batch: 660; loss: 2.04; acc: 0.42
Batch: 680; loss: 1.92; acc: 0.5
Batch: 700; loss: 1.94; acc: 0.55
Batch: 720; loss: 1.87; acc: 0.58
Batch: 740; loss: 1.96; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.45
Batch: 780; loss: 1.85; acc: 0.47
Train Epoch over. train_loss: 2.1; train_accuracy: 0.44 

Batch: 0; loss: 1.91; acc: 0.53
Batch: 20; loss: 1.83; acc: 0.48
Batch: 40; loss: 1.74; acc: 0.69
Batch: 60; loss: 1.82; acc: 0.56
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.89; acc: 0.59
Batch: 120; loss: 1.86; acc: 0.58
Batch: 140; loss: 1.84; acc: 0.55
Val Epoch over. val_loss: 1.8814739186293001; val_accuracy: 0.5027866242038217 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.78; acc: 0.52
Batch: 20; loss: 1.91; acc: 0.44
Batch: 40; loss: 1.78; acc: 0.58
Batch: 60; loss: 1.79; acc: 0.56
Batch: 80; loss: 1.87; acc: 0.52
Batch: 100; loss: 1.7; acc: 0.59
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.68; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.55
Batch: 180; loss: 1.65; acc: 0.62
Batch: 200; loss: 1.68; acc: 0.52
Batch: 220; loss: 1.69; acc: 0.52
Batch: 240; loss: 1.68; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.7
Batch: 280; loss: 1.63; acc: 0.64
Batch: 300; loss: 1.51; acc: 0.7
Batch: 320; loss: 1.51; acc: 0.59
Batch: 340; loss: 1.55; acc: 0.64
Batch: 360; loss: 1.46; acc: 0.66
Batch: 380; loss: 1.48; acc: 0.61
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.46; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.72
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 1.31; acc: 0.69
Batch: 500; loss: 1.28; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.7
Batch: 540; loss: 1.26; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.75
Batch: 620; loss: 1.09; acc: 0.75
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.72
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.72
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 1.09; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.72
Train Epoch over. train_loss: 1.43; train_accuracy: 0.63 

Batch: 0; loss: 1.07; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.72
Batch: 40; loss: 0.69; acc: 0.92
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 1.07; acc: 0.64
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 0.9484343247808469; val_accuracy: 0.7579617834394905 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.84; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 0.99; acc: 0.64
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.87; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.76; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.7
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.77; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.81; acc: 0.81
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.76; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Val Epoch over. val_loss: 0.5818737308690503; val_accuracy: 0.8250398089171974 

Epoch 6 start
The current lr is: 0.0001
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.84; acc: 0.69
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.69
Batch: 260; loss: 0.54; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.75; acc: 0.77
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.64; acc: 0.77
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.63; acc: 0.77
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.9; acc: 0.77
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.56; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.51; acc: 0.8
Batch: 700; loss: 0.7; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.81
Train Epoch over. train_loss: 0.61; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.66
Batch: 140; loss: 0.4; acc: 0.91
Val Epoch over. val_loss: 0.5535324232973111; val_accuracy: 0.8388734076433121 

Epoch 7 start
The current lr is: 0.0001
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.75
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.66; acc: 0.75
Batch: 180; loss: 0.83; acc: 0.75
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.75
Batch: 260; loss: 0.56; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.71; acc: 0.73
Batch: 380; loss: 0.56; acc: 0.81
Batch: 400; loss: 0.74; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.66; acc: 0.73
Batch: 500; loss: 0.75; acc: 0.73
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.7; acc: 0.77
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.86
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.75
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.59; train_accuracy: 0.82 

Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.67
Batch: 140; loss: 0.37; acc: 0.91
Val Epoch over. val_loss: 0.5377910626921684; val_accuracy: 0.8427547770700637 

Epoch 8 start
The current lr is: 0.0001
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.69
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.94
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.77
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.78
Batch: 460; loss: 0.7; acc: 0.73
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.77; acc: 0.78
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.73
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.8
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.58; acc: 0.77
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.75
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.57; train_accuracy: 0.83 

Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.67
Batch: 140; loss: 0.36; acc: 0.91
Val Epoch over. val_loss: 0.5232548294173684; val_accuracy: 0.8485270700636943 

Epoch 9 start
The current lr is: 0.0001
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.8
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.65; acc: 0.8
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.67; acc: 0.73
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.78
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.51; acc: 0.77
Batch: 500; loss: 0.53; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.72
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.56; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.56; train_accuracy: 0.83 

Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.7
Batch: 140; loss: 0.35; acc: 0.92
Val Epoch over. val_loss: 0.5105794497356293; val_accuracy: 0.8524084394904459 

Epoch 10 start
The current lr is: 0.0001
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.79; acc: 0.73
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.77
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.75
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.75
Batch: 740; loss: 0.62; acc: 0.77
Batch: 760; loss: 0.5; acc: 0.77
Batch: 780; loss: 0.42; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.84 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.7
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4984069679193436; val_accuracy: 0.8556926751592356 

Epoch 11 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.75
Batch: 340; loss: 0.67; acc: 0.72
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.49; acc: 0.78
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.75
Batch: 660; loss: 0.64; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.77
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.52; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.73
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49719071900768647; val_accuracy: 0.8577826433121019 

Epoch 12 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 0.54; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.61; acc: 0.77
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.48; acc: 0.81
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.8
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.81
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.84; acc: 0.7
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.73; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4960903781614486; val_accuracy: 0.8575835987261147 

Epoch 13 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.71; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.77
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.92
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.81
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.78
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.8
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.77; acc: 0.73
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.81
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.65; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4949189690267964; val_accuracy: 0.8577826433121019 

Epoch 14 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.62; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.78
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.7; acc: 0.8
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.71; acc: 0.78
Batch: 460; loss: 0.56; acc: 0.89
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.75
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.64; acc: 0.75
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49382400911325103; val_accuracy: 0.8581807324840764 

Epoch 15 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.46; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.81
Batch: 420; loss: 0.65; acc: 0.78
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.5; acc: 0.8
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.72; acc: 0.73
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4927246744275852; val_accuracy: 0.8589769108280255 

Epoch 16 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.83
Batch: 220; loss: 0.59; acc: 0.77
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.67
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.77; acc: 0.77
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.72
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.66; acc: 0.8
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.81
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4926072868761743; val_accuracy: 0.8589769108280255 

Epoch 17 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.89
Batch: 160; loss: 0.61; acc: 0.75
Batch: 180; loss: 0.55; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.61; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.75
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.75
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4924870909778935; val_accuracy: 0.8589769108280255 

Epoch 18 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.74; acc: 0.7
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.85; acc: 0.81
Batch: 160; loss: 0.49; acc: 0.8
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.8
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.7; acc: 0.75
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.78
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.78
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.83
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49236955431995877; val_accuracy: 0.8588773885350318 

Epoch 19 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.68; acc: 0.75
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.79; acc: 0.75
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.88
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.492255232326544; val_accuracy: 0.8588773885350318 

Epoch 20 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.65; acc: 0.77
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.8
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.8
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.59; acc: 0.77
Batch: 460; loss: 0.73; acc: 0.77
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.74; acc: 0.75
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.56; acc: 0.8
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4921421047039093; val_accuracy: 0.8589769108280255 

Epoch 21 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.8; acc: 0.83
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.92
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.78
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.48; acc: 0.81
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49213346051182716; val_accuracy: 0.8589769108280255 

Epoch 22 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.66; acc: 0.75
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.83; acc: 0.73
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.54; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.78; acc: 0.8
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.91
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.94
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4921248163197451; val_accuracy: 0.8590764331210191 

Epoch 23 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.78
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.78
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.75; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.48; acc: 0.81
Batch: 500; loss: 0.59; acc: 0.78
Batch: 520; loss: 0.48; acc: 0.92
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.55; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.78
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49211608082245867; val_accuracy: 0.8590764331210191 

Epoch 24 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.44; acc: 0.91
Batch: 160; loss: 0.67; acc: 0.73
Batch: 180; loss: 0.81; acc: 0.73
Batch: 200; loss: 0.73; acc: 0.83
Batch: 220; loss: 0.58; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.66; acc: 0.78
Batch: 300; loss: 0.6; acc: 0.86
Batch: 320; loss: 0.56; acc: 0.8
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.73; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.8
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.61; acc: 0.81
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.81
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4921072136824298; val_accuracy: 0.8590764331210191 

Epoch 25 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.96; acc: 0.73
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.75; acc: 0.72
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.8
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.91
Batch: 380; loss: 1.01; acc: 0.73
Batch: 400; loss: 0.49; acc: 0.94
Batch: 420; loss: 0.7; acc: 0.77
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.64; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.47; acc: 0.92
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920985095060555; val_accuracy: 0.8590764331210191 

Epoch 26 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.69; acc: 0.73
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.64; acc: 0.78
Batch: 240; loss: 0.59; acc: 0.88
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.45; acc: 0.92
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.75; acc: 0.81
Batch: 760; loss: 0.56; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.81
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49209835043378697; val_accuracy: 0.8590764331210191 

Epoch 27 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.78
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.81
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.76; acc: 0.81
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.77
Batch: 600; loss: 0.56; acc: 0.78
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.75; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.64; acc: 0.75
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.56; acc: 0.81
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49209820540847293; val_accuracy: 0.8590764331210191 

Epoch 28 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.65; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.78
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.81; acc: 0.75
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.81
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.71; acc: 0.77
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920980789858824; val_accuracy: 0.8590764331210191 

Epoch 29 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.8
Batch: 60; loss: 0.58; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.41; acc: 0.83
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.81
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.72; acc: 0.73
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.77
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.57; acc: 0.84
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.65; acc: 0.84
Batch: 780; loss: 0.81; acc: 0.75
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920979026396563; val_accuracy: 0.8590764331210191 

Epoch 30 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.85; acc: 0.75
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.74; acc: 0.8
Batch: 300; loss: 0.67; acc: 0.75
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.78
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.78
Batch: 640; loss: 0.75; acc: 0.8
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977174666277; val_accuracy: 0.8590764331210191 

Epoch 31 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.77
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.83; acc: 0.78
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.78
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.75
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.64; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.81
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.74; acc: 0.69
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.46; acc: 0.78
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49209772202239677; val_accuracy: 0.8590764331210191 

Epoch 32 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.59; acc: 0.78
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.8
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.75
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.78
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.59; acc: 0.78
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.77
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.71; acc: 0.84
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.62; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977126261231; val_accuracy: 0.8590764331210191 

Epoch 33 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.7
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.87; acc: 0.73
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.77
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.64; acc: 0.75
Batch: 420; loss: 0.42; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.8
Batch: 520; loss: 0.9; acc: 0.73
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.78
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.64; acc: 0.77
Batch: 620; loss: 0.63; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.52; acc: 0.78
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.81
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.52; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49209772240204414; val_accuracy: 0.8590764331210191 

Epoch 34 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.72; acc: 0.75
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.87; acc: 0.72
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.77
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.8
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.48; acc: 0.91
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49209771926995294; val_accuracy: 0.8590764331210191 

Epoch 35 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.73
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.67; acc: 0.77
Batch: 340; loss: 0.58; acc: 0.78
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.78
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.63; acc: 0.8
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.77; acc: 0.78
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.79; acc: 0.77
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49209772449010497; val_accuracy: 0.8590764331210191 

Epoch 36 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.73; acc: 0.73
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.8
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.69; acc: 0.8
Batch: 460; loss: 0.93; acc: 0.7
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.61; acc: 0.81
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.62; acc: 0.77
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49209772202239677; val_accuracy: 0.8590764331210191 

Epoch 37 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.74; acc: 0.75
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.94
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.55; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.75
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.53; acc: 0.8
Batch: 700; loss: 0.77; acc: 0.78
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977233511627; val_accuracy: 0.8590764331210191 

Epoch 38 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.77
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.8
Batch: 280; loss: 0.78; acc: 0.81
Batch: 300; loss: 0.79; acc: 0.77
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.49; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.7; acc: 0.75
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.49209772183257305; val_accuracy: 0.8590764331210191 

Epoch 39 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.91; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.66; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.95
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.92
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.65; acc: 0.78
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.32; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977246799287; val_accuracy: 0.8590764331210191 

Epoch 40 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.82; acc: 0.72
Batch: 300; loss: 0.66; acc: 0.75
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.63; acc: 0.73
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.51; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.61; acc: 0.8
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.8
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.7; acc: 0.78
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 41 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.78
Batch: 260; loss: 0.51; acc: 0.8
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.8
Batch: 580; loss: 0.64; acc: 0.72
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.65; acc: 0.77
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.92
Batch: 740; loss: 0.73; acc: 0.81
Batch: 760; loss: 0.6; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 42 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.71; acc: 0.75
Batch: 20; loss: 0.55; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.72; acc: 0.78
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.78
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.75
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 43 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.78
Batch: 160; loss: 0.62; acc: 0.77
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.91; acc: 0.69
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.78
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.77
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.72; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.58; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 44 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.77; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.8
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.58; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.71; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.78
Batch: 480; loss: 0.44; acc: 0.94
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.78
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.73
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 45 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.78
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.69; acc: 0.78
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.63; acc: 0.8
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.78
Batch: 420; loss: 0.46; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.77; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.84; acc: 0.72
Batch: 520; loss: 0.61; acc: 0.8
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.78
Batch: 600; loss: 0.66; acc: 0.8
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.86; acc: 0.73
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 46 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.65; acc: 0.77
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.78
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.73; acc: 0.78
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.69; acc: 0.75
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.73
Batch: 580; loss: 0.79; acc: 0.7
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.81
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.61; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 47 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.81; acc: 0.77
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.58; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 0.77; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.73
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.59; acc: 0.81
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 48 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.8
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.75; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.8; acc: 0.78
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.71; acc: 0.83
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.78
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.73; acc: 0.78
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.59; acc: 0.77
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.77; acc: 0.8
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.67; acc: 0.78
Batch: 620; loss: 0.79; acc: 0.72
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.66; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.75
Batch: 740; loss: 0.55; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 49 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.77
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.72
Batch: 240; loss: 0.51; acc: 0.89
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.56; acc: 0.8
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.63; acc: 0.77
Batch: 340; loss: 0.68; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.9; acc: 0.75
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.67; acc: 0.75
Batch: 740; loss: 0.77; acc: 0.72
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

Epoch 50 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 0.61; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.78
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.59; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.91
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.77; acc: 0.77
Batch: 640; loss: 0.71; acc: 0.78
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.66; acc: 0.8
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.88
Train Epoch over. train_loss: 0.54; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.92
Val Epoch over. val_loss: 0.4920977213580138; val_accuracy: 0.8590764331210191 

plots/no_subspace_training/lenet/2020-01-18 21:25:35/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
