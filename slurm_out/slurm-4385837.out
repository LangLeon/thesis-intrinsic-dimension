Namespace(batch_size=64, chunked=False, ddim_vs_acc=True, dense=False, device=device(type='cuda'), lr=1.0, model='reg_lenet_3', n_epochs=50, non_wrapped=False, optimizer='SGD', parameter_correction=False, print_freq=20, print_prec=2, schedule=True, schedule_freq=10, schedule_gamma=0.4, seed=1, subspace_training=True, timestamp='2020-01-19 20:22:47')
nonzero elements in E: 10499
elements in E: 2249500
fraction nonzero: 0.004667259390975773
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.31; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.11
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.17
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.28; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.12
Batch: 440; loss: 2.29; acc: 0.09
Batch: 460; loss: 2.29; acc: 0.12
Batch: 480; loss: 2.29; acc: 0.16
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.29; acc: 0.06
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.29; acc: 0.12
Batch: 580; loss: 2.28; acc: 0.16
Batch: 600; loss: 2.28; acc: 0.17
Batch: 620; loss: 2.28; acc: 0.19
Batch: 640; loss: 2.28; acc: 0.12
Batch: 660; loss: 2.27; acc: 0.22
Batch: 680; loss: 2.28; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.08
Batch: 720; loss: 2.26; acc: 0.17
Batch: 740; loss: 2.29; acc: 0.09
Batch: 760; loss: 2.29; acc: 0.11
Batch: 780; loss: 2.27; acc: 0.09
Train Epoch over. train_loss: 2.29; train_accuracy: 0.11 

Batch: 0; loss: 2.27; acc: 0.16
Batch: 20; loss: 2.28; acc: 0.17
Batch: 40; loss: 2.27; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.28; acc: 0.09
Batch: 100; loss: 2.28; acc: 0.17
Batch: 120; loss: 2.27; acc: 0.14
Batch: 140; loss: 2.28; acc: 0.12
Val Epoch over. val_loss: 2.2789832892691253; val_accuracy: 0.12838375796178345 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.17
Batch: 20; loss: 2.28; acc: 0.12
Batch: 40; loss: 2.27; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.16
Batch: 80; loss: 2.26; acc: 0.16
Batch: 100; loss: 2.27; acc: 0.2
Batch: 120; loss: 2.28; acc: 0.22
Batch: 140; loss: 2.28; acc: 0.22
Batch: 160; loss: 2.27; acc: 0.2
Batch: 180; loss: 2.27; acc: 0.22
Batch: 200; loss: 2.26; acc: 0.19
Batch: 220; loss: 2.28; acc: 0.14
Batch: 240; loss: 2.27; acc: 0.2
Batch: 260; loss: 2.27; acc: 0.25
Batch: 280; loss: 2.25; acc: 0.31
Batch: 300; loss: 2.27; acc: 0.19
Batch: 320; loss: 2.28; acc: 0.11
Batch: 340; loss: 2.27; acc: 0.28
Batch: 360; loss: 2.25; acc: 0.2
Batch: 380; loss: 2.26; acc: 0.22
Batch: 400; loss: 2.27; acc: 0.25
Batch: 420; loss: 2.24; acc: 0.23
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.25; acc: 0.19
Batch: 480; loss: 2.24; acc: 0.33
Batch: 500; loss: 2.24; acc: 0.27
Batch: 520; loss: 2.25; acc: 0.31
Batch: 540; loss: 2.24; acc: 0.34
Batch: 560; loss: 2.25; acc: 0.25
Batch: 580; loss: 2.23; acc: 0.3
Batch: 600; loss: 2.25; acc: 0.25
Batch: 620; loss: 2.25; acc: 0.28
Batch: 640; loss: 2.23; acc: 0.31
Batch: 660; loss: 2.2; acc: 0.39
Batch: 680; loss: 2.23; acc: 0.39
Batch: 700; loss: 2.23; acc: 0.3
Batch: 720; loss: 2.21; acc: 0.27
Batch: 740; loss: 2.22; acc: 0.3
Batch: 760; loss: 2.19; acc: 0.33
Batch: 780; loss: 2.22; acc: 0.25
Train Epoch over. train_loss: 2.25; train_accuracy: 0.25 

Batch: 0; loss: 2.2; acc: 0.36
Batch: 20; loss: 2.22; acc: 0.3
Batch: 40; loss: 2.18; acc: 0.38
Batch: 60; loss: 2.19; acc: 0.34
Batch: 80; loss: 2.19; acc: 0.36
Batch: 100; loss: 2.23; acc: 0.33
Batch: 120; loss: 2.19; acc: 0.3
Batch: 140; loss: 2.23; acc: 0.27
Val Epoch over. val_loss: 2.21025561982659; val_accuracy: 0.3028463375796178 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.24; acc: 0.27
Batch: 20; loss: 2.24; acc: 0.23
Batch: 40; loss: 2.23; acc: 0.25
Batch: 60; loss: 2.19; acc: 0.34
Batch: 80; loss: 2.16; acc: 0.41
Batch: 100; loss: 2.17; acc: 0.31
Batch: 120; loss: 2.19; acc: 0.27
Batch: 140; loss: 2.17; acc: 0.28
Batch: 160; loss: 2.18; acc: 0.25
Batch: 180; loss: 2.13; acc: 0.34
Batch: 200; loss: 2.16; acc: 0.27
Batch: 220; loss: 2.15; acc: 0.23
Batch: 240; loss: 2.13; acc: 0.27
Batch: 260; loss: 2.11; acc: 0.33
Batch: 280; loss: 2.15; acc: 0.28
Batch: 300; loss: 2.04; acc: 0.38
Batch: 320; loss: 2.02; acc: 0.36
Batch: 340; loss: 2.04; acc: 0.34
Batch: 360; loss: 1.89; acc: 0.42
Batch: 380; loss: 1.99; acc: 0.27
Batch: 400; loss: 1.82; acc: 0.39
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.97; acc: 0.34
Batch: 460; loss: 1.89; acc: 0.3
Batch: 480; loss: 1.82; acc: 0.42
Batch: 500; loss: 1.74; acc: 0.38
Batch: 520; loss: 1.8; acc: 0.38
Batch: 540; loss: 1.61; acc: 0.39
Batch: 560; loss: 1.55; acc: 0.45
Batch: 580; loss: 1.68; acc: 0.41
Batch: 600; loss: 1.56; acc: 0.45
Batch: 620; loss: 1.52; acc: 0.52
Batch: 640; loss: 1.71; acc: 0.39
Batch: 660; loss: 1.58; acc: 0.52
Batch: 680; loss: 1.81; acc: 0.36
Batch: 700; loss: 1.51; acc: 0.55
Batch: 720; loss: 1.61; acc: 0.5
Batch: 740; loss: 1.64; acc: 0.45
Batch: 760; loss: 1.48; acc: 0.52
Batch: 780; loss: 1.35; acc: 0.55
Train Epoch over. train_loss: 1.88; train_accuracy: 0.38 

Batch: 0; loss: 1.51; acc: 0.45
Batch: 20; loss: 1.42; acc: 0.52
Batch: 40; loss: 1.21; acc: 0.62
Batch: 60; loss: 1.52; acc: 0.44
Batch: 80; loss: 1.37; acc: 0.47
Batch: 100; loss: 1.37; acc: 0.48
Batch: 120; loss: 1.69; acc: 0.42
Batch: 140; loss: 1.35; acc: 0.5
Val Epoch over. val_loss: 1.4463551203916027; val_accuracy: 0.4938296178343949 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.39
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.33; acc: 0.61
Batch: 60; loss: 1.36; acc: 0.56
Batch: 80; loss: 1.38; acc: 0.55
Batch: 100; loss: 1.43; acc: 0.5
Batch: 120; loss: 1.38; acc: 0.53
Batch: 140; loss: 1.23; acc: 0.53
Batch: 160; loss: 1.48; acc: 0.48
Batch: 180; loss: 1.42; acc: 0.45
Batch: 200; loss: 1.49; acc: 0.45
Batch: 220; loss: 1.26; acc: 0.61
Batch: 240; loss: 1.57; acc: 0.47
Batch: 260; loss: 1.35; acc: 0.48
Batch: 280; loss: 1.27; acc: 0.55
Batch: 300; loss: 1.39; acc: 0.55
Batch: 320; loss: 1.43; acc: 0.52
Batch: 340; loss: 1.28; acc: 0.56
Batch: 360; loss: 1.34; acc: 0.55
Batch: 380; loss: 1.41; acc: 0.56
Batch: 400; loss: 1.36; acc: 0.53
Batch: 420; loss: 1.31; acc: 0.52
Batch: 440; loss: 1.23; acc: 0.62
Batch: 460; loss: 1.16; acc: 0.59
Batch: 480; loss: 1.19; acc: 0.55
Batch: 500; loss: 1.1; acc: 0.66
Batch: 520; loss: 1.4; acc: 0.53
Batch: 540; loss: 1.41; acc: 0.52
Batch: 560; loss: 1.29; acc: 0.61
Batch: 580; loss: 1.14; acc: 0.56
Batch: 600; loss: 1.09; acc: 0.66
Batch: 620; loss: 1.11; acc: 0.61
Batch: 640; loss: 1.23; acc: 0.56
Batch: 660; loss: 1.22; acc: 0.61
Batch: 680; loss: 1.37; acc: 0.52
Batch: 700; loss: 1.21; acc: 0.59
Batch: 720; loss: 1.43; acc: 0.58
Batch: 740; loss: 1.15; acc: 0.61
Batch: 760; loss: 1.12; acc: 0.61
Batch: 780; loss: 1.18; acc: 0.53
Train Epoch over. train_loss: 1.32; train_accuracy: 0.54 

Batch: 0; loss: 1.34; acc: 0.59
Batch: 20; loss: 1.26; acc: 0.53
Batch: 40; loss: 0.8; acc: 0.73
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.11; acc: 0.58
Batch: 100; loss: 1.14; acc: 0.62
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 0.92; acc: 0.73
Val Epoch over. val_loss: 1.1599416732788086; val_accuracy: 0.6175358280254777 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.33; acc: 0.55
Batch: 20; loss: 1.22; acc: 0.67
Batch: 40; loss: 1.13; acc: 0.64
Batch: 60; loss: 1.36; acc: 0.53
Batch: 80; loss: 1.12; acc: 0.67
Batch: 100; loss: 1.35; acc: 0.52
Batch: 120; loss: 1.41; acc: 0.48
Batch: 140; loss: 1.24; acc: 0.61
Batch: 160; loss: 1.17; acc: 0.59
Batch: 180; loss: 1.28; acc: 0.59
Batch: 200; loss: 1.31; acc: 0.59
Batch: 220; loss: 1.14; acc: 0.67
Batch: 240; loss: 1.15; acc: 0.56
Batch: 260; loss: 1.18; acc: 0.59
Batch: 280; loss: 1.24; acc: 0.56
Batch: 300; loss: 1.19; acc: 0.58
Batch: 320; loss: 1.07; acc: 0.66
Batch: 340; loss: 1.06; acc: 0.66
Batch: 360; loss: 1.47; acc: 0.5
Batch: 380; loss: 1.02; acc: 0.72
Batch: 400; loss: 1.1; acc: 0.69
Batch: 420; loss: 1.46; acc: 0.53
Batch: 440; loss: 1.37; acc: 0.42
Batch: 460; loss: 1.43; acc: 0.52
Batch: 480; loss: 1.2; acc: 0.53
Batch: 500; loss: 1.27; acc: 0.53
Batch: 520; loss: 1.38; acc: 0.64
Batch: 540; loss: 1.27; acc: 0.61
Batch: 560; loss: 0.97; acc: 0.73
Batch: 580; loss: 1.43; acc: 0.47
Batch: 600; loss: 0.96; acc: 0.7
Batch: 620; loss: 1.35; acc: 0.56
Batch: 640; loss: 1.17; acc: 0.55
Batch: 660; loss: 1.21; acc: 0.56
Batch: 680; loss: 1.21; acc: 0.53
Batch: 700; loss: 1.21; acc: 0.55
Batch: 720; loss: 1.31; acc: 0.61
Batch: 740; loss: 1.36; acc: 0.62
Batch: 760; loss: 0.97; acc: 0.73
Batch: 780; loss: 1.18; acc: 0.53
Train Epoch over. train_loss: 1.21; train_accuracy: 0.59 

Batch: 0; loss: 1.32; acc: 0.58
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.77
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.07; acc: 0.62
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 1.35; acc: 0.61
Batch: 140; loss: 0.85; acc: 0.77
Val Epoch over. val_loss: 1.1307843828656872; val_accuracy: 0.6271894904458599 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.32; acc: 0.55
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 1.01; acc: 0.69
Batch: 60; loss: 1.13; acc: 0.59
Batch: 80; loss: 1.33; acc: 0.5
Batch: 100; loss: 1.35; acc: 0.59
Batch: 120; loss: 1.18; acc: 0.58
Batch: 140; loss: 1.19; acc: 0.61
Batch: 160; loss: 1.04; acc: 0.64
Batch: 180; loss: 1.54; acc: 0.56
Batch: 200; loss: 1.06; acc: 0.55
Batch: 220; loss: 0.89; acc: 0.77
Batch: 240; loss: 1.25; acc: 0.55
Batch: 260; loss: 1.28; acc: 0.59
Batch: 280; loss: 0.91; acc: 0.67
Batch: 300; loss: 1.13; acc: 0.58
Batch: 320; loss: 1.41; acc: 0.56
Batch: 340; loss: 0.83; acc: 0.75
Batch: 360; loss: 1.16; acc: 0.59
Batch: 380; loss: 1.36; acc: 0.52
Batch: 400; loss: 1.27; acc: 0.58
Batch: 420; loss: 0.99; acc: 0.67
Batch: 440; loss: 1.2; acc: 0.59
Batch: 460; loss: 1.13; acc: 0.64
Batch: 480; loss: 1.19; acc: 0.56
Batch: 500; loss: 1.06; acc: 0.64
Batch: 520; loss: 1.23; acc: 0.61
Batch: 540; loss: 1.11; acc: 0.64
Batch: 560; loss: 1.2; acc: 0.62
Batch: 580; loss: 0.87; acc: 0.73
Batch: 600; loss: 1.23; acc: 0.55
Batch: 620; loss: 1.48; acc: 0.58
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 0.96; acc: 0.66
Batch: 680; loss: 1.42; acc: 0.48
Batch: 700; loss: 1.03; acc: 0.67
Batch: 720; loss: 1.25; acc: 0.53
Batch: 740; loss: 1.11; acc: 0.64
Batch: 760; loss: 1.09; acc: 0.66
Batch: 780; loss: 1.06; acc: 0.62
Train Epoch over. train_loss: 1.21; train_accuracy: 0.59 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.22; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.77
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.05; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.62
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 0.89; acc: 0.75
Val Epoch over. val_loss: 1.1332876799972194; val_accuracy: 0.6271894904458599 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.58
Batch: 20; loss: 1.34; acc: 0.59
Batch: 40; loss: 1.05; acc: 0.66
Batch: 60; loss: 1.47; acc: 0.44
Batch: 80; loss: 1.27; acc: 0.52
Batch: 100; loss: 1.11; acc: 0.58
Batch: 120; loss: 1.09; acc: 0.67
Batch: 140; loss: 1.31; acc: 0.58
Batch: 160; loss: 1.31; acc: 0.5
Batch: 180; loss: 1.26; acc: 0.56
Batch: 200; loss: 1.18; acc: 0.64
Batch: 220; loss: 1.14; acc: 0.62
Batch: 240; loss: 1.07; acc: 0.64
Batch: 260; loss: 0.99; acc: 0.66
Batch: 280; loss: 1.05; acc: 0.66
Batch: 300; loss: 1.22; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.56
Batch: 340; loss: 1.1; acc: 0.62
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.27; acc: 0.55
Batch: 400; loss: 1.09; acc: 0.61
Batch: 420; loss: 1.31; acc: 0.58
Batch: 440; loss: 1.29; acc: 0.59
Batch: 460; loss: 1.36; acc: 0.5
Batch: 480; loss: 1.36; acc: 0.52
Batch: 500; loss: 1.25; acc: 0.61
Batch: 520; loss: 1.09; acc: 0.61
Batch: 540; loss: 1.26; acc: 0.59
Batch: 560; loss: 1.26; acc: 0.55
Batch: 580; loss: 1.19; acc: 0.58
Batch: 600; loss: 1.3; acc: 0.64
Batch: 620; loss: 1.4; acc: 0.56
Batch: 640; loss: 1.2; acc: 0.64
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 1.44; acc: 0.53
Batch: 700; loss: 1.31; acc: 0.52
Batch: 720; loss: 1.08; acc: 0.66
Batch: 740; loss: 1.14; acc: 0.58
Batch: 760; loss: 1.06; acc: 0.62
Batch: 780; loss: 1.26; acc: 0.56
Train Epoch over. train_loss: 1.21; train_accuracy: 0.59 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.7
Batch: 60; loss: 1.36; acc: 0.58
Batch: 80; loss: 1.05; acc: 0.64
Batch: 100; loss: 1.05; acc: 0.67
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.72
Val Epoch over. val_loss: 1.1388295915476077; val_accuracy: 0.6184315286624203 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.28; acc: 0.55
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 1.11; acc: 0.67
Batch: 60; loss: 1.34; acc: 0.59
Batch: 80; loss: 1.08; acc: 0.66
Batch: 100; loss: 1.23; acc: 0.5
Batch: 120; loss: 1.28; acc: 0.56
Batch: 140; loss: 1.25; acc: 0.53
Batch: 160; loss: 1.21; acc: 0.58
Batch: 180; loss: 1.18; acc: 0.58
Batch: 200; loss: 0.94; acc: 0.75
Batch: 220; loss: 1.2; acc: 0.64
Batch: 240; loss: 1.33; acc: 0.53
Batch: 260; loss: 1.24; acc: 0.58
Batch: 280; loss: 1.23; acc: 0.66
Batch: 300; loss: 1.33; acc: 0.55
Batch: 320; loss: 1.25; acc: 0.61
Batch: 340; loss: 1.22; acc: 0.52
Batch: 360; loss: 1.18; acc: 0.58
Batch: 380; loss: 1.17; acc: 0.67
Batch: 400; loss: 1.32; acc: 0.52
Batch: 420; loss: 1.07; acc: 0.64
Batch: 440; loss: 1.18; acc: 0.58
Batch: 460; loss: 1.31; acc: 0.62
Batch: 480; loss: 1.04; acc: 0.64
Batch: 500; loss: 1.21; acc: 0.62
Batch: 520; loss: 0.97; acc: 0.7
Batch: 540; loss: 1.02; acc: 0.67
Batch: 560; loss: 1.2; acc: 0.59
Batch: 580; loss: 1.06; acc: 0.62
Batch: 600; loss: 1.34; acc: 0.53
Batch: 620; loss: 1.18; acc: 0.64
Batch: 640; loss: 1.02; acc: 0.66
Batch: 660; loss: 1.05; acc: 0.56
Batch: 680; loss: 1.35; acc: 0.56
Batch: 700; loss: 1.28; acc: 0.53
Batch: 720; loss: 1.44; acc: 0.53
Batch: 740; loss: 1.29; acc: 0.53
Batch: 760; loss: 1.41; acc: 0.55
Batch: 780; loss: 0.97; acc: 0.69
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.3; acc: 0.56
Batch: 20; loss: 1.16; acc: 0.61
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.06; acc: 0.64
Batch: 100; loss: 1.07; acc: 0.64
Batch: 120; loss: 1.38; acc: 0.61
Batch: 140; loss: 0.9; acc: 0.72
Val Epoch over. val_loss: 1.139421459216221; val_accuracy: 0.6275875796178344 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.5
Batch: 20; loss: 1.13; acc: 0.64
Batch: 40; loss: 1.43; acc: 0.53
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.23; acc: 0.58
Batch: 120; loss: 1.13; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.59
Batch: 160; loss: 1.36; acc: 0.45
Batch: 180; loss: 1.24; acc: 0.52
Batch: 200; loss: 1.04; acc: 0.66
Batch: 220; loss: 1.28; acc: 0.59
Batch: 240; loss: 1.31; acc: 0.59
Batch: 260; loss: 1.41; acc: 0.59
Batch: 280; loss: 1.16; acc: 0.58
Batch: 300; loss: 1.05; acc: 0.59
Batch: 320; loss: 1.08; acc: 0.61
Batch: 340; loss: 1.41; acc: 0.48
Batch: 360; loss: 1.18; acc: 0.64
Batch: 380; loss: 1.38; acc: 0.55
Batch: 400; loss: 1.19; acc: 0.64
Batch: 420; loss: 1.5; acc: 0.5
Batch: 440; loss: 1.12; acc: 0.61
Batch: 460; loss: 1.5; acc: 0.52
Batch: 480; loss: 0.97; acc: 0.73
Batch: 500; loss: 1.45; acc: 0.44
Batch: 520; loss: 1.26; acc: 0.61
Batch: 540; loss: 1.16; acc: 0.62
Batch: 560; loss: 1.14; acc: 0.61
Batch: 580; loss: 1.4; acc: 0.44
Batch: 600; loss: 1.38; acc: 0.56
Batch: 620; loss: 1.09; acc: 0.62
Batch: 640; loss: 1.25; acc: 0.62
Batch: 660; loss: 1.21; acc: 0.58
Batch: 680; loss: 1.13; acc: 0.62
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.27; acc: 0.56
Batch: 740; loss: 1.11; acc: 0.58
Batch: 760; loss: 1.14; acc: 0.62
Batch: 780; loss: 1.19; acc: 0.58
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.19; acc: 0.64
Batch: 40; loss: 0.82; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.07; acc: 0.66
Batch: 100; loss: 1.07; acc: 0.62
Batch: 120; loss: 1.35; acc: 0.61
Batch: 140; loss: 0.88; acc: 0.78
Val Epoch over. val_loss: 1.1357365000020168; val_accuracy: 0.6233081210191083 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.58
Batch: 40; loss: 0.97; acc: 0.7
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.4; acc: 0.52
Batch: 100; loss: 1.39; acc: 0.53
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 1.32; acc: 0.59
Batch: 160; loss: 1.01; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.62
Batch: 200; loss: 1.21; acc: 0.62
Batch: 220; loss: 1.22; acc: 0.64
Batch: 240; loss: 1.06; acc: 0.67
Batch: 260; loss: 1.34; acc: 0.55
Batch: 280; loss: 1.28; acc: 0.56
Batch: 300; loss: 1.24; acc: 0.56
Batch: 320; loss: 1.14; acc: 0.59
Batch: 340; loss: 1.4; acc: 0.52
Batch: 360; loss: 0.95; acc: 0.7
Batch: 380; loss: 1.1; acc: 0.64
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 1.03; acc: 0.61
Batch: 440; loss: 1.21; acc: 0.58
Batch: 460; loss: 1.15; acc: 0.66
Batch: 480; loss: 1.12; acc: 0.59
Batch: 500; loss: 1.11; acc: 0.64
Batch: 520; loss: 1.2; acc: 0.56
Batch: 540; loss: 1.06; acc: 0.66
Batch: 560; loss: 1.26; acc: 0.59
Batch: 580; loss: 1.12; acc: 0.55
Batch: 600; loss: 1.29; acc: 0.58
Batch: 620; loss: 1.2; acc: 0.56
Batch: 640; loss: 1.15; acc: 0.59
Batch: 660; loss: 1.2; acc: 0.56
Batch: 680; loss: 1.64; acc: 0.41
Batch: 700; loss: 1.36; acc: 0.48
Batch: 720; loss: 1.1; acc: 0.64
Batch: 740; loss: 1.23; acc: 0.59
Batch: 760; loss: 1.32; acc: 0.53
Batch: 780; loss: 1.39; acc: 0.58
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.33; acc: 0.58
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.58
Batch: 80; loss: 1.06; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 0.9; acc: 0.73
Val Epoch over. val_loss: 1.1418963074684143; val_accuracy: 0.6236066878980892 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.3; acc: 0.52
Batch: 20; loss: 1.25; acc: 0.55
Batch: 40; loss: 1.14; acc: 0.66
Batch: 60; loss: 1.06; acc: 0.58
Batch: 80; loss: 1.3; acc: 0.58
Batch: 100; loss: 1.16; acc: 0.61
Batch: 120; loss: 1.35; acc: 0.56
Batch: 140; loss: 1.18; acc: 0.56
Batch: 160; loss: 1.1; acc: 0.67
Batch: 180; loss: 1.04; acc: 0.69
Batch: 200; loss: 1.11; acc: 0.61
Batch: 220; loss: 1.33; acc: 0.56
Batch: 240; loss: 1.18; acc: 0.58
Batch: 260; loss: 1.39; acc: 0.58
Batch: 280; loss: 0.97; acc: 0.69
Batch: 300; loss: 1.27; acc: 0.53
Batch: 320; loss: 1.1; acc: 0.69
Batch: 340; loss: 1.09; acc: 0.66
Batch: 360; loss: 1.11; acc: 0.59
Batch: 380; loss: 1.28; acc: 0.53
Batch: 400; loss: 1.24; acc: 0.58
Batch: 420; loss: 1.08; acc: 0.64
Batch: 440; loss: 1.22; acc: 0.67
Batch: 460; loss: 1.25; acc: 0.59
Batch: 480; loss: 1.25; acc: 0.64
Batch: 500; loss: 1.5; acc: 0.48
Batch: 520; loss: 1.13; acc: 0.56
Batch: 540; loss: 1.32; acc: 0.56
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 1.36; acc: 0.47
Batch: 600; loss: 1.03; acc: 0.66
Batch: 620; loss: 1.13; acc: 0.61
Batch: 640; loss: 1.28; acc: 0.58
Batch: 660; loss: 1.01; acc: 0.69
Batch: 680; loss: 1.22; acc: 0.53
Batch: 700; loss: 1.05; acc: 0.61
Batch: 720; loss: 1.18; acc: 0.62
Batch: 740; loss: 1.19; acc: 0.56
Batch: 760; loss: 1.1; acc: 0.61
Batch: 780; loss: 1.52; acc: 0.53
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.31; acc: 0.58
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.62
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1318274151747394; val_accuracy: 0.6266918789808917 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.04; acc: 0.67
Batch: 20; loss: 1.18; acc: 0.61
Batch: 40; loss: 1.34; acc: 0.52
Batch: 60; loss: 0.98; acc: 0.64
Batch: 80; loss: 1.38; acc: 0.45
Batch: 100; loss: 1.26; acc: 0.56
Batch: 120; loss: 1.42; acc: 0.59
Batch: 140; loss: 1.17; acc: 0.66
Batch: 160; loss: 1.25; acc: 0.53
Batch: 180; loss: 1.03; acc: 0.64
Batch: 200; loss: 0.98; acc: 0.7
Batch: 220; loss: 0.82; acc: 0.77
Batch: 240; loss: 1.01; acc: 0.59
Batch: 260; loss: 1.15; acc: 0.58
Batch: 280; loss: 1.25; acc: 0.61
Batch: 300; loss: 1.12; acc: 0.66
Batch: 320; loss: 0.98; acc: 0.64
Batch: 340; loss: 0.86; acc: 0.77
Batch: 360; loss: 1.13; acc: 0.62
Batch: 380; loss: 1.33; acc: 0.56
Batch: 400; loss: 1.2; acc: 0.61
Batch: 420; loss: 1.21; acc: 0.61
Batch: 440; loss: 1.24; acc: 0.62
Batch: 460; loss: 1.18; acc: 0.64
Batch: 480; loss: 1.28; acc: 0.55
Batch: 500; loss: 1.33; acc: 0.55
Batch: 520; loss: 1.0; acc: 0.66
Batch: 540; loss: 1.18; acc: 0.53
Batch: 560; loss: 1.11; acc: 0.58
Batch: 580; loss: 1.35; acc: 0.5
Batch: 600; loss: 1.07; acc: 0.69
Batch: 620; loss: 1.33; acc: 0.56
Batch: 640; loss: 0.97; acc: 0.66
Batch: 660; loss: 1.39; acc: 0.48
Batch: 680; loss: 1.33; acc: 0.58
Batch: 700; loss: 1.32; acc: 0.53
Batch: 720; loss: 1.16; acc: 0.61
Batch: 740; loss: 1.34; acc: 0.62
Batch: 760; loss: 1.28; acc: 0.58
Batch: 780; loss: 1.25; acc: 0.55
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.27; acc: 0.58
Batch: 20; loss: 1.19; acc: 0.59
Batch: 40; loss: 0.82; acc: 0.73
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.62
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1311399196363559; val_accuracy: 0.6274880573248408 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.13; acc: 0.66
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 1.04; acc: 0.66
Batch: 60; loss: 1.18; acc: 0.58
Batch: 80; loss: 1.26; acc: 0.59
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.27; acc: 0.55
Batch: 140; loss: 1.37; acc: 0.5
Batch: 160; loss: 1.13; acc: 0.62
Batch: 180; loss: 1.38; acc: 0.52
Batch: 200; loss: 1.01; acc: 0.67
Batch: 220; loss: 1.24; acc: 0.59
Batch: 240; loss: 0.95; acc: 0.67
Batch: 260; loss: 1.19; acc: 0.64
Batch: 280; loss: 1.27; acc: 0.59
Batch: 300; loss: 1.11; acc: 0.61
Batch: 320; loss: 1.22; acc: 0.58
Batch: 340; loss: 1.01; acc: 0.64
Batch: 360; loss: 1.5; acc: 0.5
Batch: 380; loss: 1.27; acc: 0.61
Batch: 400; loss: 1.19; acc: 0.56
Batch: 420; loss: 1.35; acc: 0.5
Batch: 440; loss: 1.24; acc: 0.62
Batch: 460; loss: 1.23; acc: 0.56
Batch: 480; loss: 1.22; acc: 0.58
Batch: 500; loss: 1.19; acc: 0.55
Batch: 520; loss: 1.39; acc: 0.55
Batch: 540; loss: 1.18; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.58
Batch: 580; loss: 1.31; acc: 0.55
Batch: 600; loss: 1.32; acc: 0.53
Batch: 620; loss: 1.19; acc: 0.5
Batch: 640; loss: 1.19; acc: 0.59
Batch: 660; loss: 1.31; acc: 0.5
Batch: 680; loss: 0.85; acc: 0.77
Batch: 700; loss: 1.31; acc: 0.56
Batch: 720; loss: 1.22; acc: 0.61
Batch: 740; loss: 1.33; acc: 0.52
Batch: 760; loss: 1.09; acc: 0.7
Batch: 780; loss: 0.91; acc: 0.66
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.88; acc: 0.77
Val Epoch over. val_loss: 1.129779480445157; val_accuracy: 0.6307722929936306 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.14; acc: 0.52
Batch: 20; loss: 1.14; acc: 0.56
Batch: 40; loss: 1.03; acc: 0.72
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.24; acc: 0.64
Batch: 100; loss: 1.48; acc: 0.56
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 1.21; acc: 0.52
Batch: 160; loss: 1.12; acc: 0.66
Batch: 180; loss: 1.14; acc: 0.56
Batch: 200; loss: 1.29; acc: 0.53
Batch: 220; loss: 1.16; acc: 0.62
Batch: 240; loss: 1.19; acc: 0.61
Batch: 260; loss: 1.12; acc: 0.7
Batch: 280; loss: 1.44; acc: 0.55
Batch: 300; loss: 1.23; acc: 0.55
Batch: 320; loss: 1.34; acc: 0.52
Batch: 340; loss: 1.09; acc: 0.69
Batch: 360; loss: 1.19; acc: 0.61
Batch: 380; loss: 1.23; acc: 0.59
Batch: 400; loss: 1.12; acc: 0.61
Batch: 420; loss: 1.31; acc: 0.58
Batch: 440; loss: 1.25; acc: 0.59
Batch: 460; loss: 1.49; acc: 0.58
Batch: 480; loss: 1.12; acc: 0.62
Batch: 500; loss: 1.17; acc: 0.62
Batch: 520; loss: 1.18; acc: 0.56
Batch: 540; loss: 1.24; acc: 0.61
Batch: 560; loss: 1.07; acc: 0.64
Batch: 580; loss: 1.19; acc: 0.59
Batch: 600; loss: 1.14; acc: 0.59
Batch: 620; loss: 1.28; acc: 0.64
Batch: 640; loss: 1.2; acc: 0.53
Batch: 660; loss: 1.31; acc: 0.52
Batch: 680; loss: 1.01; acc: 0.62
Batch: 700; loss: 0.97; acc: 0.67
Batch: 720; loss: 1.14; acc: 0.67
Batch: 740; loss: 1.2; acc: 0.58
Batch: 760; loss: 1.25; acc: 0.59
Batch: 780; loss: 1.35; acc: 0.5
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.3; acc: 0.56
Batch: 20; loss: 1.23; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.03; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.62
Batch: 120; loss: 1.37; acc: 0.56
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.137063636521625; val_accuracy: 0.625 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 1.13; acc: 0.69
Batch: 60; loss: 1.29; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.5
Batch: 100; loss: 1.5; acc: 0.52
Batch: 120; loss: 1.13; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.53
Batch: 160; loss: 1.27; acc: 0.59
Batch: 180; loss: 1.31; acc: 0.56
Batch: 200; loss: 1.25; acc: 0.56
Batch: 220; loss: 1.22; acc: 0.53
Batch: 240; loss: 1.15; acc: 0.67
Batch: 260; loss: 1.36; acc: 0.5
Batch: 280; loss: 1.24; acc: 0.56
Batch: 300; loss: 1.29; acc: 0.55
Batch: 320; loss: 1.09; acc: 0.64
Batch: 340; loss: 1.35; acc: 0.55
Batch: 360; loss: 1.13; acc: 0.69
Batch: 380; loss: 1.34; acc: 0.55
Batch: 400; loss: 1.26; acc: 0.5
Batch: 420; loss: 1.19; acc: 0.62
Batch: 440; loss: 1.19; acc: 0.58
Batch: 460; loss: 1.29; acc: 0.52
Batch: 480; loss: 1.25; acc: 0.55
Batch: 500; loss: 1.08; acc: 0.61
Batch: 520; loss: 1.0; acc: 0.69
Batch: 540; loss: 1.19; acc: 0.62
Batch: 560; loss: 1.39; acc: 0.5
Batch: 580; loss: 1.07; acc: 0.67
Batch: 600; loss: 1.15; acc: 0.55
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 1.26; acc: 0.59
Batch: 660; loss: 1.29; acc: 0.56
Batch: 680; loss: 1.18; acc: 0.62
Batch: 700; loss: 1.33; acc: 0.53
Batch: 720; loss: 1.1; acc: 0.69
Batch: 740; loss: 1.26; acc: 0.53
Batch: 760; loss: 1.42; acc: 0.55
Batch: 780; loss: 1.05; acc: 0.61
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.32; acc: 0.58
Batch: 20; loss: 1.23; acc: 0.59
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.56
Batch: 80; loss: 1.04; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.35; acc: 0.61
Batch: 140; loss: 0.89; acc: 0.75
Val Epoch over. val_loss: 1.1319592272400096; val_accuracy: 0.6285828025477707 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.48; acc: 0.52
Batch: 20; loss: 1.04; acc: 0.67
Batch: 40; loss: 0.99; acc: 0.67
Batch: 60; loss: 1.32; acc: 0.53
Batch: 80; loss: 1.14; acc: 0.64
Batch: 100; loss: 1.1; acc: 0.62
Batch: 120; loss: 1.31; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.48
Batch: 160; loss: 1.2; acc: 0.62
Batch: 180; loss: 1.25; acc: 0.55
Batch: 200; loss: 1.1; acc: 0.62
Batch: 220; loss: 1.05; acc: 0.66
Batch: 240; loss: 1.12; acc: 0.66
Batch: 260; loss: 1.41; acc: 0.5
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.27; acc: 0.62
Batch: 320; loss: 1.21; acc: 0.64
Batch: 340; loss: 1.13; acc: 0.55
Batch: 360; loss: 1.44; acc: 0.56
Batch: 380; loss: 1.05; acc: 0.72
Batch: 400; loss: 1.19; acc: 0.59
Batch: 420; loss: 1.06; acc: 0.64
Batch: 440; loss: 1.38; acc: 0.52
Batch: 460; loss: 1.36; acc: 0.59
Batch: 480; loss: 1.39; acc: 0.47
Batch: 500; loss: 1.58; acc: 0.45
Batch: 520; loss: 1.22; acc: 0.52
Batch: 540; loss: 1.52; acc: 0.47
Batch: 560; loss: 1.21; acc: 0.58
Batch: 580; loss: 1.24; acc: 0.53
Batch: 600; loss: 1.03; acc: 0.72
Batch: 620; loss: 1.0; acc: 0.64
Batch: 640; loss: 1.23; acc: 0.55
Batch: 660; loss: 1.15; acc: 0.62
Batch: 680; loss: 1.21; acc: 0.62
Batch: 700; loss: 0.95; acc: 0.7
Batch: 720; loss: 1.31; acc: 0.58
Batch: 740; loss: 1.07; acc: 0.58
Batch: 760; loss: 1.3; acc: 0.55
Batch: 780; loss: 1.09; acc: 0.64
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 0.79; acc: 0.77
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.04; acc: 0.64
Batch: 120; loss: 1.35; acc: 0.59
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1316831453590637; val_accuracy: 0.6277866242038217 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.32; acc: 0.58
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 1.4; acc: 0.55
Batch: 60; loss: 1.33; acc: 0.53
Batch: 80; loss: 1.26; acc: 0.52
Batch: 100; loss: 1.26; acc: 0.56
Batch: 120; loss: 1.23; acc: 0.53
Batch: 140; loss: 1.33; acc: 0.61
Batch: 160; loss: 1.39; acc: 0.52
Batch: 180; loss: 1.21; acc: 0.59
Batch: 200; loss: 1.03; acc: 0.69
Batch: 220; loss: 1.11; acc: 0.62
Batch: 240; loss: 1.1; acc: 0.62
Batch: 260; loss: 0.92; acc: 0.69
Batch: 280; loss: 1.44; acc: 0.5
Batch: 300; loss: 1.19; acc: 0.58
Batch: 320; loss: 0.86; acc: 0.67
Batch: 340; loss: 1.05; acc: 0.62
Batch: 360; loss: 1.12; acc: 0.69
Batch: 380; loss: 1.18; acc: 0.62
Batch: 400; loss: 1.2; acc: 0.59
Batch: 420; loss: 1.3; acc: 0.56
Batch: 440; loss: 1.07; acc: 0.64
Batch: 460; loss: 1.33; acc: 0.56
Batch: 480; loss: 0.86; acc: 0.75
Batch: 500; loss: 1.04; acc: 0.62
Batch: 520; loss: 1.31; acc: 0.58
Batch: 540; loss: 1.44; acc: 0.47
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.28; acc: 0.52
Batch: 600; loss: 1.39; acc: 0.52
Batch: 620; loss: 1.26; acc: 0.62
Batch: 640; loss: 1.11; acc: 0.64
Batch: 660; loss: 1.07; acc: 0.69
Batch: 680; loss: 1.16; acc: 0.62
Batch: 700; loss: 1.4; acc: 0.59
Batch: 720; loss: 1.07; acc: 0.66
Batch: 740; loss: 1.07; acc: 0.64
Batch: 760; loss: 0.99; acc: 0.62
Batch: 780; loss: 1.11; acc: 0.56
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.75
Batch: 60; loss: 1.33; acc: 0.56
Batch: 80; loss: 1.04; acc: 0.64
Batch: 100; loss: 1.05; acc: 0.64
Batch: 120; loss: 1.4; acc: 0.56
Batch: 140; loss: 0.88; acc: 0.77
Val Epoch over. val_loss: 1.1338104002035347; val_accuracy: 0.6271894904458599 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 1.13; acc: 0.64
Batch: 60; loss: 0.96; acc: 0.72
Batch: 80; loss: 1.23; acc: 0.55
Batch: 100; loss: 1.34; acc: 0.58
Batch: 120; loss: 1.37; acc: 0.52
Batch: 140; loss: 0.99; acc: 0.69
Batch: 160; loss: 1.35; acc: 0.5
Batch: 180; loss: 1.28; acc: 0.55
Batch: 200; loss: 1.2; acc: 0.55
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 1.16; acc: 0.66
Batch: 260; loss: 0.89; acc: 0.69
Batch: 280; loss: 1.43; acc: 0.56
Batch: 300; loss: 1.38; acc: 0.5
Batch: 320; loss: 1.25; acc: 0.61
Batch: 340; loss: 1.21; acc: 0.55
Batch: 360; loss: 1.3; acc: 0.58
Batch: 380; loss: 1.41; acc: 0.48
Batch: 400; loss: 1.13; acc: 0.56
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 1.18; acc: 0.58
Batch: 460; loss: 1.29; acc: 0.5
Batch: 480; loss: 1.22; acc: 0.56
Batch: 500; loss: 1.29; acc: 0.58
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.35; acc: 0.58
Batch: 560; loss: 1.25; acc: 0.56
Batch: 580; loss: 1.19; acc: 0.61
Batch: 600; loss: 1.22; acc: 0.55
Batch: 620; loss: 1.41; acc: 0.53
Batch: 640; loss: 1.24; acc: 0.53
Batch: 660; loss: 1.28; acc: 0.59
Batch: 680; loss: 1.05; acc: 0.7
Batch: 700; loss: 1.14; acc: 0.72
Batch: 720; loss: 1.1; acc: 0.64
Batch: 740; loss: 1.47; acc: 0.45
Batch: 760; loss: 1.39; acc: 0.61
Batch: 780; loss: 1.35; acc: 0.56
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.28; acc: 0.58
Batch: 20; loss: 1.18; acc: 0.61
Batch: 40; loss: 0.84; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.06; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.62
Batch: 120; loss: 1.4; acc: 0.56
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1340775224053936; val_accuracy: 0.6251990445859873 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.28; acc: 0.56
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 1.14; acc: 0.66
Batch: 60; loss: 1.26; acc: 0.59
Batch: 80; loss: 1.0; acc: 0.67
Batch: 100; loss: 1.33; acc: 0.55
Batch: 120; loss: 1.1; acc: 0.69
Batch: 140; loss: 1.29; acc: 0.58
Batch: 160; loss: 1.31; acc: 0.58
Batch: 180; loss: 1.19; acc: 0.58
Batch: 200; loss: 1.25; acc: 0.55
Batch: 220; loss: 1.23; acc: 0.61
Batch: 240; loss: 1.2; acc: 0.55
Batch: 260; loss: 1.27; acc: 0.62
Batch: 280; loss: 1.1; acc: 0.61
Batch: 300; loss: 1.02; acc: 0.64
Batch: 320; loss: 1.16; acc: 0.62
Batch: 340; loss: 1.31; acc: 0.55
Batch: 360; loss: 1.09; acc: 0.59
Batch: 380; loss: 1.27; acc: 0.62
Batch: 400; loss: 1.26; acc: 0.58
Batch: 420; loss: 1.14; acc: 0.64
Batch: 440; loss: 1.37; acc: 0.59
Batch: 460; loss: 1.36; acc: 0.5
Batch: 480; loss: 0.97; acc: 0.62
Batch: 500; loss: 1.27; acc: 0.64
Batch: 520; loss: 1.21; acc: 0.61
Batch: 540; loss: 0.96; acc: 0.67
Batch: 560; loss: 1.2; acc: 0.58
Batch: 580; loss: 1.19; acc: 0.59
Batch: 600; loss: 0.95; acc: 0.72
Batch: 620; loss: 1.05; acc: 0.62
Batch: 640; loss: 1.33; acc: 0.61
Batch: 660; loss: 1.24; acc: 0.59
Batch: 680; loss: 1.22; acc: 0.59
Batch: 700; loss: 0.94; acc: 0.67
Batch: 720; loss: 1.25; acc: 0.56
Batch: 740; loss: 1.19; acc: 0.64
Batch: 760; loss: 1.13; acc: 0.53
Batch: 780; loss: 1.29; acc: 0.58
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.22; acc: 0.59
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.05; acc: 0.66
Batch: 100; loss: 1.03; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.61
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1304729041779877; val_accuracy: 0.6271894904458599 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.99; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 1.31; acc: 0.55
Batch: 60; loss: 1.57; acc: 0.45
Batch: 80; loss: 1.23; acc: 0.53
Batch: 100; loss: 1.28; acc: 0.61
Batch: 120; loss: 1.41; acc: 0.5
Batch: 140; loss: 1.33; acc: 0.55
Batch: 160; loss: 1.26; acc: 0.52
Batch: 180; loss: 0.82; acc: 0.72
Batch: 200; loss: 1.29; acc: 0.56
Batch: 220; loss: 1.17; acc: 0.62
Batch: 240; loss: 1.1; acc: 0.61
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.26; acc: 0.56
Batch: 300; loss: 1.35; acc: 0.55
Batch: 320; loss: 1.26; acc: 0.59
Batch: 340; loss: 1.1; acc: 0.64
Batch: 360; loss: 1.17; acc: 0.66
Batch: 380; loss: 1.05; acc: 0.66
Batch: 400; loss: 1.29; acc: 0.55
Batch: 420; loss: 1.11; acc: 0.55
Batch: 440; loss: 1.07; acc: 0.67
Batch: 460; loss: 1.24; acc: 0.58
Batch: 480; loss: 0.91; acc: 0.66
Batch: 500; loss: 1.32; acc: 0.53
Batch: 520; loss: 1.05; acc: 0.66
Batch: 540; loss: 1.41; acc: 0.52
Batch: 560; loss: 1.18; acc: 0.56
Batch: 580; loss: 1.13; acc: 0.56
Batch: 600; loss: 1.21; acc: 0.62
Batch: 620; loss: 1.26; acc: 0.64
Batch: 640; loss: 1.09; acc: 0.56
Batch: 660; loss: 1.29; acc: 0.52
Batch: 680; loss: 1.18; acc: 0.62
Batch: 700; loss: 1.19; acc: 0.53
Batch: 720; loss: 1.35; acc: 0.58
Batch: 740; loss: 1.3; acc: 0.53
Batch: 760; loss: 1.29; acc: 0.56
Batch: 780; loss: 1.23; acc: 0.53
Train Epoch over. train_loss: 1.21; train_accuracy: 0.6 

Batch: 0; loss: 1.28; acc: 0.55
Batch: 20; loss: 1.19; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.75
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.05; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.73
Val Epoch over. val_loss: 1.1303258910300626; val_accuracy: 0.6262937898089171 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.08; acc: 0.62
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.16; acc: 0.61
Batch: 60; loss: 1.38; acc: 0.52
Batch: 80; loss: 1.39; acc: 0.55
Batch: 100; loss: 1.01; acc: 0.67
Batch: 120; loss: 1.09; acc: 0.66
Batch: 140; loss: 1.04; acc: 0.62
Batch: 160; loss: 1.08; acc: 0.62
Batch: 180; loss: 1.16; acc: 0.59
Batch: 200; loss: 1.29; acc: 0.62
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 1.13; acc: 0.62
Batch: 260; loss: 1.28; acc: 0.58
Batch: 280; loss: 1.18; acc: 0.62
Batch: 300; loss: 1.23; acc: 0.64
Batch: 320; loss: 1.03; acc: 0.66
Batch: 340; loss: 1.02; acc: 0.67
Batch: 360; loss: 1.11; acc: 0.66
Batch: 380; loss: 1.19; acc: 0.56
Batch: 400; loss: 1.3; acc: 0.55
Batch: 420; loss: 1.2; acc: 0.59
Batch: 440; loss: 1.13; acc: 0.59
Batch: 460; loss: 1.27; acc: 0.53
Batch: 480; loss: 1.35; acc: 0.58
Batch: 500; loss: 1.15; acc: 0.66
Batch: 520; loss: 1.07; acc: 0.59
Batch: 540; loss: 1.08; acc: 0.62
Batch: 560; loss: 1.47; acc: 0.53
Batch: 580; loss: 1.16; acc: 0.62
Batch: 600; loss: 1.23; acc: 0.53
Batch: 620; loss: 1.21; acc: 0.61
Batch: 640; loss: 1.34; acc: 0.55
Batch: 660; loss: 1.09; acc: 0.64
Batch: 680; loss: 1.25; acc: 0.56
Batch: 700; loss: 1.15; acc: 0.59
Batch: 720; loss: 1.1; acc: 0.64
Batch: 740; loss: 1.06; acc: 0.61
Batch: 760; loss: 1.26; acc: 0.64
Batch: 780; loss: 1.24; acc: 0.58
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.61
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.03; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.128804399329386; val_accuracy: 0.626890923566879 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.35; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 0.99; acc: 0.61
Batch: 60; loss: 1.08; acc: 0.61
Batch: 80; loss: 1.01; acc: 0.7
Batch: 100; loss: 1.23; acc: 0.58
Batch: 120; loss: 1.25; acc: 0.55
Batch: 140; loss: 1.27; acc: 0.55
Batch: 160; loss: 1.32; acc: 0.55
Batch: 180; loss: 1.14; acc: 0.58
Batch: 200; loss: 1.06; acc: 0.64
Batch: 220; loss: 1.32; acc: 0.59
Batch: 240; loss: 1.39; acc: 0.55
Batch: 260; loss: 1.53; acc: 0.48
Batch: 280; loss: 1.4; acc: 0.56
Batch: 300; loss: 1.0; acc: 0.62
Batch: 320; loss: 1.51; acc: 0.45
Batch: 340; loss: 1.21; acc: 0.53
Batch: 360; loss: 1.59; acc: 0.48
Batch: 380; loss: 1.28; acc: 0.58
Batch: 400; loss: 1.19; acc: 0.53
Batch: 420; loss: 1.09; acc: 0.62
Batch: 440; loss: 1.2; acc: 0.5
Batch: 460; loss: 1.37; acc: 0.56
Batch: 480; loss: 1.16; acc: 0.64
Batch: 500; loss: 0.98; acc: 0.72
Batch: 520; loss: 1.35; acc: 0.52
Batch: 540; loss: 0.97; acc: 0.69
Batch: 560; loss: 1.03; acc: 0.72
Batch: 580; loss: 1.09; acc: 0.64
Batch: 600; loss: 1.09; acc: 0.61
Batch: 620; loss: 1.17; acc: 0.61
Batch: 640; loss: 1.21; acc: 0.61
Batch: 660; loss: 1.41; acc: 0.53
Batch: 680; loss: 1.17; acc: 0.56
Batch: 700; loss: 1.4; acc: 0.58
Batch: 720; loss: 1.16; acc: 0.53
Batch: 740; loss: 1.46; acc: 0.52
Batch: 760; loss: 1.09; acc: 0.61
Batch: 780; loss: 1.1; acc: 0.56
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.19; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.05; acc: 0.64
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1293826365167168; val_accuracy: 0.6269904458598726 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 1.07; acc: 0.66
Batch: 60; loss: 1.26; acc: 0.55
Batch: 80; loss: 0.98; acc: 0.62
Batch: 100; loss: 1.25; acc: 0.58
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.21; acc: 0.61
Batch: 180; loss: 1.34; acc: 0.58
Batch: 200; loss: 1.24; acc: 0.58
Batch: 220; loss: 1.29; acc: 0.62
Batch: 240; loss: 1.18; acc: 0.7
Batch: 260; loss: 1.13; acc: 0.62
Batch: 280; loss: 1.08; acc: 0.62
Batch: 300; loss: 0.92; acc: 0.73
Batch: 320; loss: 1.01; acc: 0.62
Batch: 340; loss: 1.12; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.59
Batch: 380; loss: 1.3; acc: 0.56
Batch: 400; loss: 1.11; acc: 0.64
Batch: 420; loss: 1.24; acc: 0.59
Batch: 440; loss: 1.22; acc: 0.56
Batch: 460; loss: 1.14; acc: 0.61
Batch: 480; loss: 1.35; acc: 0.59
Batch: 500; loss: 1.14; acc: 0.62
Batch: 520; loss: 1.35; acc: 0.5
Batch: 540; loss: 1.32; acc: 0.53
Batch: 560; loss: 1.19; acc: 0.58
Batch: 580; loss: 1.03; acc: 0.64
Batch: 600; loss: 1.44; acc: 0.55
Batch: 620; loss: 1.16; acc: 0.59
Batch: 640; loss: 1.17; acc: 0.5
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 1.18; acc: 0.61
Batch: 700; loss: 1.3; acc: 0.55
Batch: 720; loss: 1.22; acc: 0.64
Batch: 740; loss: 1.11; acc: 0.59
Batch: 760; loss: 1.12; acc: 0.61
Batch: 780; loss: 0.96; acc: 0.72
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.28; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.31; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.64
Batch: 120; loss: 1.37; acc: 0.59
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1298174911243901; val_accuracy: 0.6281847133757962 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.19; acc: 0.58
Batch: 40; loss: 1.24; acc: 0.53
Batch: 60; loss: 0.93; acc: 0.7
Batch: 80; loss: 1.18; acc: 0.58
Batch: 100; loss: 1.2; acc: 0.62
Batch: 120; loss: 1.06; acc: 0.64
Batch: 140; loss: 1.06; acc: 0.69
Batch: 160; loss: 1.23; acc: 0.62
Batch: 180; loss: 1.18; acc: 0.62
Batch: 200; loss: 1.35; acc: 0.48
Batch: 220; loss: 1.12; acc: 0.64
Batch: 240; loss: 1.27; acc: 0.62
Batch: 260; loss: 1.08; acc: 0.62
Batch: 280; loss: 1.29; acc: 0.56
Batch: 300; loss: 1.11; acc: 0.58
Batch: 320; loss: 1.18; acc: 0.67
Batch: 340; loss: 1.34; acc: 0.62
Batch: 360; loss: 1.32; acc: 0.58
Batch: 380; loss: 1.08; acc: 0.62
Batch: 400; loss: 1.08; acc: 0.64
Batch: 420; loss: 1.11; acc: 0.61
Batch: 440; loss: 1.18; acc: 0.59
Batch: 460; loss: 0.98; acc: 0.64
Batch: 480; loss: 1.19; acc: 0.58
Batch: 500; loss: 0.93; acc: 0.7
Batch: 520; loss: 1.32; acc: 0.53
Batch: 540; loss: 1.13; acc: 0.66
Batch: 560; loss: 1.24; acc: 0.55
Batch: 580; loss: 1.39; acc: 0.56
Batch: 600; loss: 1.33; acc: 0.53
Batch: 620; loss: 1.37; acc: 0.58
Batch: 640; loss: 1.16; acc: 0.66
Batch: 660; loss: 1.34; acc: 0.56
Batch: 680; loss: 1.3; acc: 0.58
Batch: 700; loss: 1.28; acc: 0.61
Batch: 720; loss: 1.4; acc: 0.48
Batch: 740; loss: 1.32; acc: 0.52
Batch: 760; loss: 1.48; acc: 0.48
Batch: 780; loss: 1.13; acc: 0.61
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.3; acc: 0.59
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.62
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1291394879104226; val_accuracy: 0.6274880573248408 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.4; acc: 0.53
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 1.21; acc: 0.58
Batch: 60; loss: 1.07; acc: 0.58
Batch: 80; loss: 1.14; acc: 0.61
Batch: 100; loss: 1.3; acc: 0.61
Batch: 120; loss: 1.19; acc: 0.59
Batch: 140; loss: 1.42; acc: 0.5
Batch: 160; loss: 1.34; acc: 0.58
Batch: 180; loss: 1.12; acc: 0.64
Batch: 200; loss: 1.13; acc: 0.62
Batch: 220; loss: 1.19; acc: 0.59
Batch: 240; loss: 1.28; acc: 0.58
Batch: 260; loss: 1.34; acc: 0.45
Batch: 280; loss: 1.05; acc: 0.59
Batch: 300; loss: 1.52; acc: 0.53
Batch: 320; loss: 1.13; acc: 0.66
Batch: 340; loss: 1.33; acc: 0.52
Batch: 360; loss: 1.07; acc: 0.69
Batch: 380; loss: 1.38; acc: 0.48
Batch: 400; loss: 1.17; acc: 0.61
Batch: 420; loss: 1.02; acc: 0.7
Batch: 440; loss: 1.08; acc: 0.64
Batch: 460; loss: 1.37; acc: 0.58
Batch: 480; loss: 1.23; acc: 0.5
Batch: 500; loss: 0.99; acc: 0.62
Batch: 520; loss: 1.0; acc: 0.67
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 1.52; acc: 0.5
Batch: 580; loss: 1.17; acc: 0.62
Batch: 600; loss: 1.07; acc: 0.56
Batch: 620; loss: 1.08; acc: 0.66
Batch: 640; loss: 1.31; acc: 0.52
Batch: 660; loss: 0.94; acc: 0.66
Batch: 680; loss: 1.17; acc: 0.67
Batch: 700; loss: 1.18; acc: 0.7
Batch: 720; loss: 1.12; acc: 0.62
Batch: 740; loss: 1.09; acc: 0.67
Batch: 760; loss: 1.26; acc: 0.58
Batch: 780; loss: 1.03; acc: 0.66
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.28; acc: 0.59
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.8; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.03; acc: 0.62
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1294635352055737; val_accuracy: 0.6270899681528662 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.28; acc: 0.5
Batch: 20; loss: 1.3; acc: 0.52
Batch: 40; loss: 1.42; acc: 0.48
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 1.25; acc: 0.59
Batch: 100; loss: 1.23; acc: 0.61
Batch: 120; loss: 1.09; acc: 0.61
Batch: 140; loss: 1.12; acc: 0.59
Batch: 160; loss: 1.44; acc: 0.52
Batch: 180; loss: 1.15; acc: 0.69
Batch: 200; loss: 1.18; acc: 0.59
Batch: 220; loss: 1.4; acc: 0.64
Batch: 240; loss: 1.26; acc: 0.59
Batch: 260; loss: 1.11; acc: 0.62
Batch: 280; loss: 1.05; acc: 0.66
Batch: 300; loss: 1.32; acc: 0.52
Batch: 320; loss: 0.97; acc: 0.73
Batch: 340; loss: 1.18; acc: 0.67
Batch: 360; loss: 1.46; acc: 0.52
Batch: 380; loss: 1.2; acc: 0.53
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 0.88; acc: 0.77
Batch: 440; loss: 1.17; acc: 0.62
Batch: 460; loss: 1.1; acc: 0.61
Batch: 480; loss: 1.16; acc: 0.62
Batch: 500; loss: 1.37; acc: 0.53
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.26; acc: 0.58
Batch: 560; loss: 1.51; acc: 0.52
Batch: 580; loss: 1.28; acc: 0.47
Batch: 600; loss: 1.48; acc: 0.44
Batch: 620; loss: 1.15; acc: 0.62
Batch: 640; loss: 1.45; acc: 0.47
Batch: 660; loss: 1.27; acc: 0.58
Batch: 680; loss: 1.01; acc: 0.69
Batch: 700; loss: 1.23; acc: 0.61
Batch: 720; loss: 1.07; acc: 0.61
Batch: 740; loss: 1.23; acc: 0.52
Batch: 760; loss: 1.19; acc: 0.59
Batch: 780; loss: 1.26; acc: 0.56
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.55
Batch: 20; loss: 1.21; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.04; acc: 0.64
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.128634893970125; val_accuracy: 0.6282842356687898 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 1.44; acc: 0.53
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.34; acc: 0.52
Batch: 100; loss: 1.37; acc: 0.64
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 1.2; acc: 0.61
Batch: 160; loss: 1.01; acc: 0.64
Batch: 180; loss: 1.45; acc: 0.47
Batch: 200; loss: 1.1; acc: 0.69
Batch: 220; loss: 0.98; acc: 0.67
Batch: 240; loss: 1.29; acc: 0.45
Batch: 260; loss: 1.19; acc: 0.66
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.17; acc: 0.58
Batch: 320; loss: 1.24; acc: 0.56
Batch: 340; loss: 1.2; acc: 0.59
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.1; acc: 0.59
Batch: 400; loss: 1.06; acc: 0.67
Batch: 420; loss: 1.41; acc: 0.52
Batch: 440; loss: 1.51; acc: 0.5
Batch: 460; loss: 1.16; acc: 0.58
Batch: 480; loss: 1.06; acc: 0.61
Batch: 500; loss: 1.1; acc: 0.58
Batch: 520; loss: 1.17; acc: 0.62
Batch: 540; loss: 1.23; acc: 0.58
Batch: 560; loss: 1.39; acc: 0.53
Batch: 580; loss: 1.11; acc: 0.59
Batch: 600; loss: 0.93; acc: 0.64
Batch: 620; loss: 1.33; acc: 0.56
Batch: 640; loss: 0.9; acc: 0.7
Batch: 660; loss: 1.48; acc: 0.47
Batch: 680; loss: 1.37; acc: 0.53
Batch: 700; loss: 1.11; acc: 0.55
Batch: 720; loss: 1.2; acc: 0.64
Batch: 740; loss: 1.16; acc: 0.67
Batch: 760; loss: 1.58; acc: 0.61
Batch: 780; loss: 0.95; acc: 0.66
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.03; acc: 0.62
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1293522392868236; val_accuracy: 0.6272890127388535 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.37; acc: 0.47
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.02; acc: 0.62
Batch: 80; loss: 1.17; acc: 0.61
Batch: 100; loss: 1.07; acc: 0.66
Batch: 120; loss: 1.18; acc: 0.62
Batch: 140; loss: 1.18; acc: 0.67
Batch: 160; loss: 1.22; acc: 0.61
Batch: 180; loss: 1.17; acc: 0.56
Batch: 200; loss: 1.47; acc: 0.5
Batch: 220; loss: 1.22; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.62
Batch: 260; loss: 0.96; acc: 0.67
Batch: 280; loss: 0.99; acc: 0.75
Batch: 300; loss: 1.12; acc: 0.66
Batch: 320; loss: 1.14; acc: 0.58
Batch: 340; loss: 1.27; acc: 0.53
Batch: 360; loss: 1.17; acc: 0.5
Batch: 380; loss: 1.26; acc: 0.59
Batch: 400; loss: 1.12; acc: 0.66
Batch: 420; loss: 1.01; acc: 0.7
Batch: 440; loss: 1.25; acc: 0.48
Batch: 460; loss: 1.29; acc: 0.52
Batch: 480; loss: 1.58; acc: 0.5
Batch: 500; loss: 1.31; acc: 0.53
Batch: 520; loss: 1.16; acc: 0.59
Batch: 540; loss: 1.24; acc: 0.64
Batch: 560; loss: 1.09; acc: 0.62
Batch: 580; loss: 1.29; acc: 0.64
Batch: 600; loss: 1.17; acc: 0.56
Batch: 620; loss: 1.25; acc: 0.55
Batch: 640; loss: 1.26; acc: 0.62
Batch: 660; loss: 1.02; acc: 0.64
Batch: 680; loss: 1.11; acc: 0.64
Batch: 700; loss: 1.35; acc: 0.47
Batch: 720; loss: 1.19; acc: 0.55
Batch: 740; loss: 1.26; acc: 0.5
Batch: 760; loss: 1.16; acc: 0.61
Batch: 780; loss: 1.38; acc: 0.52
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.28; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.75
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.04; acc: 0.62
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1287018869333207; val_accuracy: 0.6274880573248408 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 1.57; acc: 0.42
Batch: 60; loss: 1.15; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.56
Batch: 100; loss: 1.18; acc: 0.59
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 1.27; acc: 0.53
Batch: 160; loss: 1.23; acc: 0.5
Batch: 180; loss: 0.91; acc: 0.69
Batch: 200; loss: 1.15; acc: 0.55
Batch: 220; loss: 1.11; acc: 0.56
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 1.16; acc: 0.62
Batch: 280; loss: 1.14; acc: 0.64
Batch: 300; loss: 1.14; acc: 0.64
Batch: 320; loss: 1.34; acc: 0.53
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 1.16; acc: 0.61
Batch: 380; loss: 1.12; acc: 0.69
Batch: 400; loss: 1.22; acc: 0.58
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.25; acc: 0.59
Batch: 460; loss: 0.98; acc: 0.61
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 1.38; acc: 0.59
Batch: 520; loss: 1.35; acc: 0.55
Batch: 540; loss: 1.17; acc: 0.58
Batch: 560; loss: 1.16; acc: 0.56
Batch: 580; loss: 1.41; acc: 0.5
Batch: 600; loss: 1.25; acc: 0.56
Batch: 620; loss: 1.26; acc: 0.58
Batch: 640; loss: 1.21; acc: 0.55
Batch: 660; loss: 1.34; acc: 0.59
Batch: 680; loss: 1.42; acc: 0.5
Batch: 700; loss: 1.27; acc: 0.55
Batch: 720; loss: 1.24; acc: 0.58
Batch: 740; loss: 0.94; acc: 0.7
Batch: 760; loss: 1.11; acc: 0.62
Batch: 780; loss: 1.16; acc: 0.59
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.28; acc: 0.59
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.56
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.03; acc: 0.64
Batch: 120; loss: 1.39; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1281642010257502; val_accuracy: 0.6263933121019108 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.41; acc: 0.47
Batch: 20; loss: 1.34; acc: 0.58
Batch: 40; loss: 1.33; acc: 0.56
Batch: 60; loss: 1.28; acc: 0.55
Batch: 80; loss: 1.24; acc: 0.62
Batch: 100; loss: 1.1; acc: 0.64
Batch: 120; loss: 1.14; acc: 0.55
Batch: 140; loss: 1.34; acc: 0.56
Batch: 160; loss: 1.33; acc: 0.56
Batch: 180; loss: 1.09; acc: 0.59
Batch: 200; loss: 1.24; acc: 0.58
Batch: 220; loss: 1.18; acc: 0.59
Batch: 240; loss: 1.08; acc: 0.62
Batch: 260; loss: 1.33; acc: 0.55
Batch: 280; loss: 1.18; acc: 0.55
Batch: 300; loss: 1.1; acc: 0.61
Batch: 320; loss: 1.35; acc: 0.53
Batch: 340; loss: 1.34; acc: 0.62
Batch: 360; loss: 1.05; acc: 0.67
Batch: 380; loss: 1.46; acc: 0.56
Batch: 400; loss: 1.15; acc: 0.56
Batch: 420; loss: 1.2; acc: 0.61
Batch: 440; loss: 1.11; acc: 0.66
Batch: 460; loss: 1.41; acc: 0.55
Batch: 480; loss: 1.38; acc: 0.52
Batch: 500; loss: 1.29; acc: 0.58
Batch: 520; loss: 1.12; acc: 0.64
Batch: 540; loss: 1.26; acc: 0.56
Batch: 560; loss: 1.26; acc: 0.61
Batch: 580; loss: 1.26; acc: 0.58
Batch: 600; loss: 1.17; acc: 0.61
Batch: 620; loss: 1.26; acc: 0.62
Batch: 640; loss: 1.17; acc: 0.61
Batch: 660; loss: 1.39; acc: 0.48
Batch: 680; loss: 1.25; acc: 0.59
Batch: 700; loss: 1.32; acc: 0.55
Batch: 720; loss: 1.35; acc: 0.58
Batch: 740; loss: 1.05; acc: 0.67
Batch: 760; loss: 1.21; acc: 0.59
Batch: 780; loss: 1.32; acc: 0.58
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.28; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.129043565054608; val_accuracy: 0.6272890127388535 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.49; acc: 0.53
Batch: 40; loss: 1.24; acc: 0.55
Batch: 60; loss: 0.93; acc: 0.69
Batch: 80; loss: 1.29; acc: 0.58
Batch: 100; loss: 1.48; acc: 0.56
Batch: 120; loss: 1.05; acc: 0.66
Batch: 140; loss: 1.52; acc: 0.45
Batch: 160; loss: 1.21; acc: 0.58
Batch: 180; loss: 1.11; acc: 0.64
Batch: 200; loss: 1.2; acc: 0.56
Batch: 220; loss: 0.88; acc: 0.67
Batch: 240; loss: 1.19; acc: 0.56
Batch: 260; loss: 1.32; acc: 0.58
Batch: 280; loss: 1.2; acc: 0.61
Batch: 300; loss: 1.17; acc: 0.58
Batch: 320; loss: 1.17; acc: 0.61
Batch: 340; loss: 1.29; acc: 0.58
Batch: 360; loss: 1.28; acc: 0.52
Batch: 380; loss: 0.89; acc: 0.69
Batch: 400; loss: 1.29; acc: 0.61
Batch: 420; loss: 1.22; acc: 0.59
Batch: 440; loss: 1.06; acc: 0.66
Batch: 460; loss: 1.27; acc: 0.61
Batch: 480; loss: 1.27; acc: 0.64
Batch: 500; loss: 1.3; acc: 0.56
Batch: 520; loss: 1.27; acc: 0.53
Batch: 540; loss: 1.17; acc: 0.61
Batch: 560; loss: 1.14; acc: 0.58
Batch: 580; loss: 1.33; acc: 0.53
Batch: 600; loss: 1.53; acc: 0.47
Batch: 620; loss: 1.31; acc: 0.44
Batch: 640; loss: 1.18; acc: 0.69
Batch: 660; loss: 1.03; acc: 0.66
Batch: 680; loss: 1.16; acc: 0.59
Batch: 700; loss: 1.17; acc: 0.59
Batch: 720; loss: 1.19; acc: 0.58
Batch: 740; loss: 1.24; acc: 0.64
Batch: 760; loss: 1.15; acc: 0.64
Batch: 780; loss: 1.22; acc: 0.52
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1287410304804517; val_accuracy: 0.6278861464968153 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.1; acc: 0.62
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 1.25; acc: 0.66
Batch: 60; loss: 1.06; acc: 0.64
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.44; acc: 0.48
Batch: 140; loss: 1.08; acc: 0.61
Batch: 160; loss: 1.15; acc: 0.62
Batch: 180; loss: 1.13; acc: 0.59
Batch: 200; loss: 1.05; acc: 0.55
Batch: 220; loss: 1.28; acc: 0.55
Batch: 240; loss: 1.45; acc: 0.61
Batch: 260; loss: 1.29; acc: 0.55
Batch: 280; loss: 1.26; acc: 0.67
Batch: 300; loss: 1.24; acc: 0.53
Batch: 320; loss: 1.08; acc: 0.69
Batch: 340; loss: 0.97; acc: 0.67
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.48; acc: 0.5
Batch: 400; loss: 1.25; acc: 0.59
Batch: 420; loss: 1.33; acc: 0.58
Batch: 440; loss: 1.3; acc: 0.56
Batch: 460; loss: 1.35; acc: 0.47
Batch: 480; loss: 0.91; acc: 0.73
Batch: 500; loss: 0.93; acc: 0.73
Batch: 520; loss: 1.03; acc: 0.69
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.53; acc: 0.5
Batch: 580; loss: 1.24; acc: 0.52
Batch: 600; loss: 1.31; acc: 0.59
Batch: 620; loss: 1.24; acc: 0.56
Batch: 640; loss: 1.14; acc: 0.61
Batch: 660; loss: 1.0; acc: 0.67
Batch: 680; loss: 1.01; acc: 0.67
Batch: 700; loss: 1.2; acc: 0.66
Batch: 720; loss: 1.1; acc: 0.59
Batch: 740; loss: 1.19; acc: 0.62
Batch: 760; loss: 1.12; acc: 0.61
Batch: 780; loss: 1.27; acc: 0.53
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.56
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1284086294234938; val_accuracy: 0.6277866242038217 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.42; acc: 0.58
Batch: 20; loss: 1.16; acc: 0.55
Batch: 40; loss: 1.12; acc: 0.61
Batch: 60; loss: 1.22; acc: 0.62
Batch: 80; loss: 1.29; acc: 0.52
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 1.07; acc: 0.62
Batch: 140; loss: 1.17; acc: 0.61
Batch: 160; loss: 1.2; acc: 0.61
Batch: 180; loss: 1.37; acc: 0.59
Batch: 200; loss: 1.14; acc: 0.66
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 1.36; acc: 0.58
Batch: 260; loss: 1.15; acc: 0.61
Batch: 280; loss: 0.9; acc: 0.69
Batch: 300; loss: 1.3; acc: 0.52
Batch: 320; loss: 1.2; acc: 0.59
Batch: 340; loss: 1.05; acc: 0.58
Batch: 360; loss: 1.1; acc: 0.67
Batch: 380; loss: 1.27; acc: 0.53
Batch: 400; loss: 1.06; acc: 0.59
Batch: 420; loss: 1.29; acc: 0.55
Batch: 440; loss: 1.22; acc: 0.62
Batch: 460; loss: 1.09; acc: 0.58
Batch: 480; loss: 0.99; acc: 0.67
Batch: 500; loss: 1.16; acc: 0.64
Batch: 520; loss: 1.2; acc: 0.48
Batch: 540; loss: 1.32; acc: 0.56
Batch: 560; loss: 1.27; acc: 0.56
Batch: 580; loss: 1.23; acc: 0.61
Batch: 600; loss: 1.28; acc: 0.58
Batch: 620; loss: 1.2; acc: 0.58
Batch: 640; loss: 1.55; acc: 0.48
Batch: 660; loss: 1.32; acc: 0.61
Batch: 680; loss: 1.18; acc: 0.62
Batch: 700; loss: 1.2; acc: 0.59
Batch: 720; loss: 1.36; acc: 0.53
Batch: 740; loss: 1.38; acc: 0.52
Batch: 760; loss: 1.28; acc: 0.59
Batch: 780; loss: 1.14; acc: 0.61
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.21; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1287351182311962; val_accuracy: 0.627687101910828 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.97; acc: 0.7
Batch: 20; loss: 1.53; acc: 0.39
Batch: 40; loss: 1.27; acc: 0.64
Batch: 60; loss: 1.41; acc: 0.44
Batch: 80; loss: 0.98; acc: 0.67
Batch: 100; loss: 1.15; acc: 0.62
Batch: 120; loss: 1.15; acc: 0.61
Batch: 140; loss: 1.12; acc: 0.58
Batch: 160; loss: 0.98; acc: 0.67
Batch: 180; loss: 1.06; acc: 0.7
Batch: 200; loss: 0.89; acc: 0.69
Batch: 220; loss: 1.03; acc: 0.67
Batch: 240; loss: 1.0; acc: 0.7
Batch: 260; loss: 1.03; acc: 0.66
Batch: 280; loss: 0.92; acc: 0.67
Batch: 300; loss: 1.09; acc: 0.64
Batch: 320; loss: 1.02; acc: 0.56
Batch: 340; loss: 1.22; acc: 0.62
Batch: 360; loss: 1.08; acc: 0.69
Batch: 380; loss: 1.22; acc: 0.53
Batch: 400; loss: 1.28; acc: 0.55
Batch: 420; loss: 1.46; acc: 0.48
Batch: 440; loss: 1.58; acc: 0.44
Batch: 460; loss: 1.16; acc: 0.67
Batch: 480; loss: 1.27; acc: 0.59
Batch: 500; loss: 1.33; acc: 0.58
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.69
Batch: 580; loss: 1.16; acc: 0.61
Batch: 600; loss: 1.47; acc: 0.52
Batch: 620; loss: 1.31; acc: 0.58
Batch: 640; loss: 1.18; acc: 0.61
Batch: 660; loss: 1.34; acc: 0.59
Batch: 680; loss: 1.1; acc: 0.64
Batch: 700; loss: 0.94; acc: 0.73
Batch: 720; loss: 1.22; acc: 0.58
Batch: 740; loss: 1.14; acc: 0.64
Batch: 760; loss: 1.28; acc: 0.53
Batch: 780; loss: 1.09; acc: 0.66
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.59
Batch: 20; loss: 1.21; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.72
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.03; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1285354286242442; val_accuracy: 0.6275875796178344 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.36; acc: 0.48
Batch: 20; loss: 1.34; acc: 0.52
Batch: 40; loss: 1.47; acc: 0.52
Batch: 60; loss: 1.35; acc: 0.47
Batch: 80; loss: 1.43; acc: 0.55
Batch: 100; loss: 1.19; acc: 0.61
Batch: 120; loss: 1.27; acc: 0.48
Batch: 140; loss: 1.27; acc: 0.61
Batch: 160; loss: 1.36; acc: 0.56
Batch: 180; loss: 1.11; acc: 0.59
Batch: 200; loss: 1.54; acc: 0.53
Batch: 220; loss: 1.14; acc: 0.61
Batch: 240; loss: 1.14; acc: 0.58
Batch: 260; loss: 1.09; acc: 0.61
Batch: 280; loss: 1.28; acc: 0.56
Batch: 300; loss: 1.26; acc: 0.53
Batch: 320; loss: 1.26; acc: 0.55
Batch: 340; loss: 1.41; acc: 0.45
Batch: 360; loss: 1.08; acc: 0.73
Batch: 380; loss: 1.47; acc: 0.55
Batch: 400; loss: 1.06; acc: 0.64
Batch: 420; loss: 1.07; acc: 0.61
Batch: 440; loss: 1.12; acc: 0.59
Batch: 460; loss: 1.41; acc: 0.44
Batch: 480; loss: 1.27; acc: 0.53
Batch: 500; loss: 1.34; acc: 0.5
Batch: 520; loss: 1.0; acc: 0.62
Batch: 540; loss: 1.21; acc: 0.52
Batch: 560; loss: 1.37; acc: 0.59
Batch: 580; loss: 1.05; acc: 0.67
Batch: 600; loss: 1.0; acc: 0.66
Batch: 620; loss: 1.14; acc: 0.64
Batch: 640; loss: 1.37; acc: 0.55
Batch: 660; loss: 1.04; acc: 0.67
Batch: 680; loss: 1.12; acc: 0.61
Batch: 700; loss: 1.31; acc: 0.59
Batch: 720; loss: 1.22; acc: 0.61
Batch: 740; loss: 1.38; acc: 0.52
Batch: 760; loss: 1.1; acc: 0.7
Batch: 780; loss: 1.2; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.3; acc: 0.58
Batch: 20; loss: 1.21; acc: 0.58
Batch: 40; loss: 0.8; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1285263102525358; val_accuracy: 0.6280851910828026 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.22; acc: 0.55
Batch: 20; loss: 1.39; acc: 0.52
Batch: 40; loss: 1.32; acc: 0.55
Batch: 60; loss: 1.07; acc: 0.62
Batch: 80; loss: 1.07; acc: 0.69
Batch: 100; loss: 0.98; acc: 0.72
Batch: 120; loss: 1.19; acc: 0.53
Batch: 140; loss: 1.35; acc: 0.52
Batch: 160; loss: 1.32; acc: 0.64
Batch: 180; loss: 1.41; acc: 0.53
Batch: 200; loss: 1.2; acc: 0.61
Batch: 220; loss: 1.22; acc: 0.48
Batch: 240; loss: 1.32; acc: 0.55
Batch: 260; loss: 1.31; acc: 0.62
Batch: 280; loss: 1.27; acc: 0.56
Batch: 300; loss: 1.08; acc: 0.61
Batch: 320; loss: 1.14; acc: 0.58
Batch: 340; loss: 1.41; acc: 0.5
Batch: 360; loss: 1.34; acc: 0.48
Batch: 380; loss: 1.06; acc: 0.58
Batch: 400; loss: 1.04; acc: 0.69
Batch: 420; loss: 1.42; acc: 0.55
Batch: 440; loss: 0.92; acc: 0.66
Batch: 460; loss: 1.15; acc: 0.7
Batch: 480; loss: 1.13; acc: 0.62
Batch: 500; loss: 1.26; acc: 0.53
Batch: 520; loss: 1.31; acc: 0.58
Batch: 540; loss: 1.36; acc: 0.5
Batch: 560; loss: 0.99; acc: 0.69
Batch: 580; loss: 1.32; acc: 0.53
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.2; acc: 0.58
Batch: 640; loss: 1.2; acc: 0.67
Batch: 660; loss: 1.39; acc: 0.5
Batch: 680; loss: 1.27; acc: 0.56
Batch: 700; loss: 1.26; acc: 0.56
Batch: 720; loss: 1.09; acc: 0.58
Batch: 740; loss: 1.33; acc: 0.5
Batch: 760; loss: 1.3; acc: 0.55
Batch: 780; loss: 1.14; acc: 0.62
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.64
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1287268764653784; val_accuracy: 0.6270899681528662 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.45; acc: 0.52
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 1.03; acc: 0.61
Batch: 60; loss: 1.24; acc: 0.55
Batch: 80; loss: 0.86; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 1.35; acc: 0.53
Batch: 140; loss: 1.21; acc: 0.55
Batch: 160; loss: 1.13; acc: 0.58
Batch: 180; loss: 1.53; acc: 0.42
Batch: 200; loss: 1.16; acc: 0.62
Batch: 220; loss: 1.1; acc: 0.58
Batch: 240; loss: 1.31; acc: 0.56
Batch: 260; loss: 1.32; acc: 0.53
Batch: 280; loss: 1.05; acc: 0.67
Batch: 300; loss: 0.94; acc: 0.66
Batch: 320; loss: 1.0; acc: 0.7
Batch: 340; loss: 1.38; acc: 0.5
Batch: 360; loss: 1.26; acc: 0.53
Batch: 380; loss: 1.0; acc: 0.67
Batch: 400; loss: 1.21; acc: 0.55
Batch: 420; loss: 1.27; acc: 0.56
Batch: 440; loss: 1.21; acc: 0.61
Batch: 460; loss: 1.03; acc: 0.67
Batch: 480; loss: 1.22; acc: 0.61
Batch: 500; loss: 1.36; acc: 0.56
Batch: 520; loss: 1.4; acc: 0.53
Batch: 540; loss: 1.13; acc: 0.7
Batch: 560; loss: 1.03; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.59
Batch: 600; loss: 1.13; acc: 0.58
Batch: 620; loss: 1.32; acc: 0.52
Batch: 640; loss: 1.04; acc: 0.67
Batch: 660; loss: 1.32; acc: 0.52
Batch: 680; loss: 1.24; acc: 0.56
Batch: 700; loss: 1.19; acc: 0.64
Batch: 720; loss: 1.09; acc: 0.61
Batch: 740; loss: 1.25; acc: 0.62
Batch: 760; loss: 1.29; acc: 0.58
Batch: 780; loss: 1.1; acc: 0.62
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1287378964910082; val_accuracy: 0.6279856687898089 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.32; acc: 0.5
Batch: 20; loss: 1.27; acc: 0.47
Batch: 40; loss: 1.23; acc: 0.61
Batch: 60; loss: 1.41; acc: 0.58
Batch: 80; loss: 1.05; acc: 0.67
Batch: 100; loss: 1.29; acc: 0.59
Batch: 120; loss: 1.07; acc: 0.59
Batch: 140; loss: 1.14; acc: 0.67
Batch: 160; loss: 0.96; acc: 0.72
Batch: 180; loss: 1.03; acc: 0.64
Batch: 200; loss: 1.14; acc: 0.66
Batch: 220; loss: 1.39; acc: 0.55
Batch: 240; loss: 1.52; acc: 0.48
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 1.33; acc: 0.58
Batch: 300; loss: 0.92; acc: 0.67
Batch: 320; loss: 1.35; acc: 0.61
Batch: 340; loss: 1.32; acc: 0.59
Batch: 360; loss: 1.01; acc: 0.7
Batch: 380; loss: 1.23; acc: 0.56
Batch: 400; loss: 1.2; acc: 0.52
Batch: 420; loss: 1.43; acc: 0.5
Batch: 440; loss: 1.35; acc: 0.53
Batch: 460; loss: 1.39; acc: 0.52
Batch: 480; loss: 1.12; acc: 0.64
Batch: 500; loss: 1.48; acc: 0.56
Batch: 520; loss: 1.18; acc: 0.58
Batch: 540; loss: 0.96; acc: 0.69
Batch: 560; loss: 1.28; acc: 0.52
Batch: 580; loss: 1.35; acc: 0.5
Batch: 600; loss: 1.36; acc: 0.58
Batch: 620; loss: 1.34; acc: 0.59
Batch: 640; loss: 1.02; acc: 0.64
Batch: 660; loss: 1.14; acc: 0.61
Batch: 680; loss: 0.9; acc: 0.64
Batch: 700; loss: 1.34; acc: 0.53
Batch: 720; loss: 1.18; acc: 0.61
Batch: 740; loss: 1.31; acc: 0.59
Batch: 760; loss: 1.24; acc: 0.61
Batch: 780; loss: 1.04; acc: 0.64
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.56
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1284419431048593; val_accuracy: 0.627687101910828 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.29; acc: 0.59
Batch: 20; loss: 1.09; acc: 0.62
Batch: 40; loss: 1.16; acc: 0.66
Batch: 60; loss: 1.41; acc: 0.48
Batch: 80; loss: 1.14; acc: 0.58
Batch: 100; loss: 1.06; acc: 0.67
Batch: 120; loss: 1.29; acc: 0.47
Batch: 140; loss: 1.39; acc: 0.52
Batch: 160; loss: 1.2; acc: 0.61
Batch: 180; loss: 1.06; acc: 0.61
Batch: 200; loss: 0.99; acc: 0.7
Batch: 220; loss: 1.36; acc: 0.55
Batch: 240; loss: 1.37; acc: 0.53
Batch: 260; loss: 1.05; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.59
Batch: 300; loss: 1.07; acc: 0.66
Batch: 320; loss: 0.99; acc: 0.69
Batch: 340; loss: 1.18; acc: 0.64
Batch: 360; loss: 1.11; acc: 0.64
Batch: 380; loss: 1.24; acc: 0.59
Batch: 400; loss: 1.46; acc: 0.5
Batch: 420; loss: 1.16; acc: 0.64
Batch: 440; loss: 1.21; acc: 0.66
Batch: 460; loss: 1.11; acc: 0.58
Batch: 480; loss: 1.06; acc: 0.61
Batch: 500; loss: 1.33; acc: 0.61
Batch: 520; loss: 1.22; acc: 0.69
Batch: 540; loss: 1.28; acc: 0.56
Batch: 560; loss: 1.2; acc: 0.53
Batch: 580; loss: 1.35; acc: 0.64
Batch: 600; loss: 1.36; acc: 0.55
Batch: 620; loss: 1.14; acc: 0.58
Batch: 640; loss: 1.26; acc: 0.58
Batch: 660; loss: 1.21; acc: 0.62
Batch: 680; loss: 0.98; acc: 0.69
Batch: 700; loss: 1.35; acc: 0.55
Batch: 720; loss: 0.83; acc: 0.77
Batch: 740; loss: 1.16; acc: 0.59
Batch: 760; loss: 1.33; acc: 0.52
Batch: 780; loss: 1.37; acc: 0.52
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1286920157207805; val_accuracy: 0.6273885350318471 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.32; acc: 0.58
Batch: 20; loss: 1.41; acc: 0.5
Batch: 40; loss: 1.22; acc: 0.59
Batch: 60; loss: 1.04; acc: 0.66
Batch: 80; loss: 1.24; acc: 0.59
Batch: 100; loss: 1.26; acc: 0.58
Batch: 120; loss: 1.04; acc: 0.64
Batch: 140; loss: 1.31; acc: 0.5
Batch: 160; loss: 1.27; acc: 0.59
Batch: 180; loss: 1.25; acc: 0.55
Batch: 200; loss: 1.27; acc: 0.69
Batch: 220; loss: 1.53; acc: 0.47
Batch: 240; loss: 1.0; acc: 0.67
Batch: 260; loss: 1.05; acc: 0.7
Batch: 280; loss: 1.36; acc: 0.58
Batch: 300; loss: 1.39; acc: 0.58
Batch: 320; loss: 1.42; acc: 0.56
Batch: 340; loss: 1.41; acc: 0.53
Batch: 360; loss: 1.17; acc: 0.61
Batch: 380; loss: 1.13; acc: 0.62
Batch: 400; loss: 1.66; acc: 0.45
Batch: 420; loss: 1.22; acc: 0.56
Batch: 440; loss: 1.13; acc: 0.58
Batch: 460; loss: 1.05; acc: 0.7
Batch: 480; loss: 1.4; acc: 0.47
Batch: 500; loss: 1.15; acc: 0.59
Batch: 520; loss: 1.23; acc: 0.59
Batch: 540; loss: 1.04; acc: 0.66
Batch: 560; loss: 1.32; acc: 0.55
Batch: 580; loss: 1.07; acc: 0.67
Batch: 600; loss: 1.25; acc: 0.62
Batch: 620; loss: 1.3; acc: 0.52
Batch: 640; loss: 1.34; acc: 0.61
Batch: 660; loss: 1.29; acc: 0.61
Batch: 680; loss: 0.97; acc: 0.7
Batch: 700; loss: 1.47; acc: 0.5
Batch: 720; loss: 1.14; acc: 0.59
Batch: 740; loss: 1.18; acc: 0.58
Batch: 760; loss: 1.23; acc: 0.61
Batch: 780; loss: 1.24; acc: 0.53
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1285167924917427; val_accuracy: 0.6271894904458599 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.25; acc: 0.58
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.49; acc: 0.48
Batch: 60; loss: 1.25; acc: 0.59
Batch: 80; loss: 1.34; acc: 0.59
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 1.33; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.53
Batch: 160; loss: 1.12; acc: 0.59
Batch: 180; loss: 1.32; acc: 0.53
Batch: 200; loss: 1.24; acc: 0.58
Batch: 220; loss: 1.12; acc: 0.61
Batch: 240; loss: 1.39; acc: 0.58
Batch: 260; loss: 1.31; acc: 0.58
Batch: 280; loss: 1.08; acc: 0.62
Batch: 300; loss: 1.34; acc: 0.58
Batch: 320; loss: 1.09; acc: 0.64
Batch: 340; loss: 1.24; acc: 0.58
Batch: 360; loss: 1.39; acc: 0.5
Batch: 380; loss: 1.06; acc: 0.73
Batch: 400; loss: 1.09; acc: 0.66
Batch: 420; loss: 1.1; acc: 0.67
Batch: 440; loss: 1.13; acc: 0.62
Batch: 460; loss: 1.03; acc: 0.69
Batch: 480; loss: 1.0; acc: 0.66
Batch: 500; loss: 1.65; acc: 0.45
Batch: 520; loss: 1.08; acc: 0.58
Batch: 540; loss: 0.99; acc: 0.72
Batch: 560; loss: 1.42; acc: 0.61
Batch: 580; loss: 1.52; acc: 0.5
Batch: 600; loss: 1.34; acc: 0.59
Batch: 620; loss: 1.24; acc: 0.53
Batch: 640; loss: 1.19; acc: 0.58
Batch: 660; loss: 1.43; acc: 0.5
Batch: 680; loss: 1.25; acc: 0.56
Batch: 700; loss: 1.1; acc: 0.64
Batch: 720; loss: 1.15; acc: 0.67
Batch: 740; loss: 1.36; acc: 0.53
Batch: 760; loss: 0.96; acc: 0.64
Batch: 780; loss: 1.2; acc: 0.62
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.128408315075431; val_accuracy: 0.6277866242038217 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.33; acc: 0.58
Batch: 20; loss: 1.35; acc: 0.55
Batch: 40; loss: 1.04; acc: 0.66
Batch: 60; loss: 1.18; acc: 0.61
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.19; acc: 0.59
Batch: 120; loss: 1.06; acc: 0.66
Batch: 140; loss: 1.2; acc: 0.53
Batch: 160; loss: 1.12; acc: 0.67
Batch: 180; loss: 1.4; acc: 0.53
Batch: 200; loss: 1.41; acc: 0.5
Batch: 220; loss: 1.2; acc: 0.59
Batch: 240; loss: 1.27; acc: 0.61
Batch: 260; loss: 1.36; acc: 0.52
Batch: 280; loss: 1.46; acc: 0.61
Batch: 300; loss: 1.28; acc: 0.59
Batch: 320; loss: 1.24; acc: 0.52
Batch: 340; loss: 1.53; acc: 0.41
Batch: 360; loss: 1.2; acc: 0.56
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 1.41; acc: 0.42
Batch: 440; loss: 1.36; acc: 0.53
Batch: 460; loss: 1.13; acc: 0.61
Batch: 480; loss: 1.37; acc: 0.5
Batch: 500; loss: 1.11; acc: 0.67
Batch: 520; loss: 1.14; acc: 0.56
Batch: 540; loss: 1.1; acc: 0.62
Batch: 560; loss: 1.14; acc: 0.69
Batch: 580; loss: 0.97; acc: 0.69
Batch: 600; loss: 1.14; acc: 0.61
Batch: 620; loss: 1.27; acc: 0.58
Batch: 640; loss: 1.07; acc: 0.78
Batch: 660; loss: 1.14; acc: 0.61
Batch: 680; loss: 1.09; acc: 0.61
Batch: 700; loss: 1.14; acc: 0.62
Batch: 720; loss: 1.04; acc: 0.66
Batch: 740; loss: 1.24; acc: 0.59
Batch: 760; loss: 1.42; acc: 0.48
Batch: 780; loss: 1.17; acc: 0.58
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.31; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1284182963857226; val_accuracy: 0.6274880573248408 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.08; acc: 0.62
Batch: 40; loss: 1.08; acc: 0.61
Batch: 60; loss: 1.29; acc: 0.58
Batch: 80; loss: 1.12; acc: 0.62
Batch: 100; loss: 1.32; acc: 0.53
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 1.19; acc: 0.62
Batch: 160; loss: 1.29; acc: 0.5
Batch: 180; loss: 0.88; acc: 0.69
Batch: 200; loss: 1.26; acc: 0.61
Batch: 220; loss: 1.21; acc: 0.52
Batch: 240; loss: 1.48; acc: 0.5
Batch: 260; loss: 1.11; acc: 0.58
Batch: 280; loss: 1.06; acc: 0.67
Batch: 300; loss: 1.13; acc: 0.64
Batch: 320; loss: 1.11; acc: 0.59
Batch: 340; loss: 1.17; acc: 0.52
Batch: 360; loss: 1.09; acc: 0.67
Batch: 380; loss: 1.2; acc: 0.56
Batch: 400; loss: 1.23; acc: 0.64
Batch: 420; loss: 1.12; acc: 0.66
Batch: 440; loss: 1.37; acc: 0.64
Batch: 460; loss: 1.07; acc: 0.55
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.35; acc: 0.52
Batch: 520; loss: 1.32; acc: 0.56
Batch: 540; loss: 1.11; acc: 0.61
Batch: 560; loss: 1.2; acc: 0.66
Batch: 580; loss: 0.97; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.58
Batch: 620; loss: 1.29; acc: 0.61
Batch: 640; loss: 1.24; acc: 0.53
Batch: 660; loss: 1.38; acc: 0.53
Batch: 680; loss: 1.14; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.58
Batch: 720; loss: 1.05; acc: 0.67
Batch: 740; loss: 1.06; acc: 0.59
Batch: 760; loss: 1.43; acc: 0.48
Batch: 780; loss: 1.19; acc: 0.56
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1285292551775647; val_accuracy: 0.6283837579617835 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.31; acc: 0.52
Batch: 20; loss: 1.16; acc: 0.58
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 1.06; acc: 0.59
Batch: 80; loss: 1.42; acc: 0.58
Batch: 100; loss: 1.29; acc: 0.58
Batch: 120; loss: 1.12; acc: 0.56
Batch: 140; loss: 1.17; acc: 0.7
Batch: 160; loss: 1.41; acc: 0.5
Batch: 180; loss: 1.44; acc: 0.47
Batch: 200; loss: 1.15; acc: 0.62
Batch: 220; loss: 1.29; acc: 0.58
Batch: 240; loss: 1.34; acc: 0.59
Batch: 260; loss: 1.13; acc: 0.61
Batch: 280; loss: 1.2; acc: 0.62
Batch: 300; loss: 1.21; acc: 0.61
Batch: 320; loss: 1.03; acc: 0.7
Batch: 340; loss: 1.04; acc: 0.64
Batch: 360; loss: 1.4; acc: 0.55
Batch: 380; loss: 0.97; acc: 0.73
Batch: 400; loss: 1.04; acc: 0.67
Batch: 420; loss: 1.29; acc: 0.53
Batch: 440; loss: 0.87; acc: 0.7
Batch: 460; loss: 1.26; acc: 0.56
Batch: 480; loss: 1.48; acc: 0.53
Batch: 500; loss: 1.09; acc: 0.73
Batch: 520; loss: 1.15; acc: 0.56
Batch: 540; loss: 0.83; acc: 0.72
Batch: 560; loss: 1.15; acc: 0.66
Batch: 580; loss: 1.25; acc: 0.61
Batch: 600; loss: 1.17; acc: 0.59
Batch: 620; loss: 1.38; acc: 0.52
Batch: 640; loss: 1.17; acc: 0.59
Batch: 660; loss: 1.32; acc: 0.55
Batch: 680; loss: 1.01; acc: 0.64
Batch: 700; loss: 1.13; acc: 0.59
Batch: 720; loss: 1.06; acc: 0.59
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 1.3; acc: 0.62
Batch: 780; loss: 1.25; acc: 0.59
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.59
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1285463548769616; val_accuracy: 0.627687101910828 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.41; acc: 0.52
Batch: 20; loss: 1.2; acc: 0.58
Batch: 40; loss: 1.4; acc: 0.53
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 1.16; acc: 0.66
Batch: 100; loss: 0.89; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.58
Batch: 140; loss: 1.17; acc: 0.58
Batch: 160; loss: 1.23; acc: 0.53
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.36; acc: 0.58
Batch: 220; loss: 1.29; acc: 0.58
Batch: 240; loss: 1.4; acc: 0.58
Batch: 260; loss: 1.2; acc: 0.58
Batch: 280; loss: 1.04; acc: 0.67
Batch: 300; loss: 1.1; acc: 0.61
Batch: 320; loss: 0.98; acc: 0.72
Batch: 340; loss: 1.22; acc: 0.67
Batch: 360; loss: 1.44; acc: 0.53
Batch: 380; loss: 1.32; acc: 0.52
Batch: 400; loss: 1.19; acc: 0.55
Batch: 420; loss: 1.26; acc: 0.61
Batch: 440; loss: 1.15; acc: 0.64
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.14; acc: 0.66
Batch: 500; loss: 1.57; acc: 0.44
Batch: 520; loss: 1.0; acc: 0.69
Batch: 540; loss: 1.23; acc: 0.56
Batch: 560; loss: 1.34; acc: 0.58
Batch: 580; loss: 1.01; acc: 0.62
Batch: 600; loss: 1.05; acc: 0.64
Batch: 620; loss: 1.26; acc: 0.59
Batch: 640; loss: 1.42; acc: 0.55
Batch: 660; loss: 1.32; acc: 0.61
Batch: 680; loss: 1.2; acc: 0.66
Batch: 700; loss: 1.23; acc: 0.61
Batch: 720; loss: 1.28; acc: 0.58
Batch: 740; loss: 1.26; acc: 0.56
Batch: 760; loss: 1.11; acc: 0.66
Batch: 780; loss: 1.21; acc: 0.61
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1285700657565123; val_accuracy: 0.6279856687898089 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.35; acc: 0.52
Batch: 20; loss: 0.97; acc: 0.75
Batch: 40; loss: 1.13; acc: 0.64
Batch: 60; loss: 1.29; acc: 0.59
Batch: 80; loss: 1.25; acc: 0.62
Batch: 100; loss: 1.01; acc: 0.67
Batch: 120; loss: 1.33; acc: 0.48
Batch: 140; loss: 1.39; acc: 0.47
Batch: 160; loss: 1.24; acc: 0.59
Batch: 180; loss: 1.2; acc: 0.61
Batch: 200; loss: 0.99; acc: 0.69
Batch: 220; loss: 1.06; acc: 0.62
Batch: 240; loss: 1.22; acc: 0.59
Batch: 260; loss: 1.3; acc: 0.53
Batch: 280; loss: 1.55; acc: 0.41
Batch: 300; loss: 1.34; acc: 0.55
Batch: 320; loss: 1.0; acc: 0.67
Batch: 340; loss: 1.45; acc: 0.5
Batch: 360; loss: 1.22; acc: 0.62
Batch: 380; loss: 1.41; acc: 0.52
Batch: 400; loss: 1.33; acc: 0.52
Batch: 420; loss: 1.07; acc: 0.67
Batch: 440; loss: 1.18; acc: 0.56
Batch: 460; loss: 1.15; acc: 0.59
Batch: 480; loss: 1.01; acc: 0.67
Batch: 500; loss: 1.4; acc: 0.48
Batch: 520; loss: 0.94; acc: 0.7
Batch: 540; loss: 1.12; acc: 0.55
Batch: 560; loss: 1.13; acc: 0.73
Batch: 580; loss: 1.11; acc: 0.69
Batch: 600; loss: 1.05; acc: 0.66
Batch: 620; loss: 1.05; acc: 0.66
Batch: 640; loss: 1.21; acc: 0.64
Batch: 660; loss: 1.23; acc: 0.62
Batch: 680; loss: 1.07; acc: 0.61
Batch: 700; loss: 1.04; acc: 0.66
Batch: 720; loss: 1.26; acc: 0.59
Batch: 740; loss: 1.3; acc: 0.58
Batch: 760; loss: 1.1; acc: 0.66
Batch: 780; loss: 1.4; acc: 0.56
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1284087380026555; val_accuracy: 0.6279856687898089 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.14; acc: 0.58
Batch: 20; loss: 1.46; acc: 0.45
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.09; acc: 0.67
Batch: 80; loss: 1.16; acc: 0.62
Batch: 100; loss: 1.38; acc: 0.45
Batch: 120; loss: 1.15; acc: 0.61
Batch: 140; loss: 1.06; acc: 0.67
Batch: 160; loss: 1.08; acc: 0.56
Batch: 180; loss: 1.33; acc: 0.53
Batch: 200; loss: 1.13; acc: 0.58
Batch: 220; loss: 1.2; acc: 0.55
Batch: 240; loss: 1.3; acc: 0.56
Batch: 260; loss: 1.19; acc: 0.56
Batch: 280; loss: 1.52; acc: 0.52
Batch: 300; loss: 1.24; acc: 0.56
Batch: 320; loss: 1.15; acc: 0.62
Batch: 340; loss: 0.83; acc: 0.72
Batch: 360; loss: 1.12; acc: 0.64
Batch: 380; loss: 1.39; acc: 0.58
Batch: 400; loss: 1.25; acc: 0.55
Batch: 420; loss: 1.38; acc: 0.58
Batch: 440; loss: 1.24; acc: 0.58
Batch: 460; loss: 1.25; acc: 0.59
Batch: 480; loss: 1.14; acc: 0.64
Batch: 500; loss: 1.36; acc: 0.53
Batch: 520; loss: 1.38; acc: 0.47
Batch: 540; loss: 1.27; acc: 0.52
Batch: 560; loss: 1.2; acc: 0.62
Batch: 580; loss: 1.28; acc: 0.52
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 0.91; acc: 0.69
Batch: 640; loss: 1.17; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 1.01; acc: 0.62
Batch: 700; loss: 0.97; acc: 0.7
Batch: 720; loss: 1.31; acc: 0.56
Batch: 740; loss: 0.92; acc: 0.69
Batch: 760; loss: 1.08; acc: 0.53
Batch: 780; loss: 1.17; acc: 0.61
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.128478772700972; val_accuracy: 0.6275875796178344 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.05; acc: 0.61
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 1.21; acc: 0.62
Batch: 60; loss: 1.49; acc: 0.56
Batch: 80; loss: 1.12; acc: 0.52
Batch: 100; loss: 1.16; acc: 0.66
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 1.1; acc: 0.66
Batch: 160; loss: 1.19; acc: 0.64
Batch: 180; loss: 1.4; acc: 0.53
Batch: 200; loss: 1.31; acc: 0.47
Batch: 220; loss: 1.29; acc: 0.55
Batch: 240; loss: 1.05; acc: 0.61
Batch: 260; loss: 1.15; acc: 0.62
Batch: 280; loss: 1.15; acc: 0.56
Batch: 300; loss: 1.07; acc: 0.56
Batch: 320; loss: 1.22; acc: 0.59
Batch: 340; loss: 1.24; acc: 0.58
Batch: 360; loss: 1.23; acc: 0.55
Batch: 380; loss: 1.2; acc: 0.61
Batch: 400; loss: 1.41; acc: 0.53
Batch: 420; loss: 1.38; acc: 0.5
Batch: 440; loss: 0.97; acc: 0.67
Batch: 460; loss: 1.45; acc: 0.56
Batch: 480; loss: 1.0; acc: 0.64
Batch: 500; loss: 1.31; acc: 0.58
Batch: 520; loss: 1.33; acc: 0.55
Batch: 540; loss: 1.56; acc: 0.48
Batch: 560; loss: 1.11; acc: 0.66
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 1.19; acc: 0.61
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 1.14; acc: 0.67
Batch: 660; loss: 1.43; acc: 0.52
Batch: 680; loss: 1.25; acc: 0.58
Batch: 700; loss: 1.04; acc: 0.62
Batch: 720; loss: 1.02; acc: 0.64
Batch: 740; loss: 1.39; acc: 0.53
Batch: 760; loss: 1.15; acc: 0.59
Batch: 780; loss: 1.53; acc: 0.5
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.28; acc: 0.56
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.64
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1286882993521963; val_accuracy: 0.626890923566879 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 1.31; acc: 0.58
Batch: 60; loss: 1.27; acc: 0.58
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 0.94; acc: 0.7
Batch: 120; loss: 1.01; acc: 0.61
Batch: 140; loss: 1.24; acc: 0.59
Batch: 160; loss: 1.16; acc: 0.61
Batch: 180; loss: 1.12; acc: 0.55
Batch: 200; loss: 1.26; acc: 0.59
Batch: 220; loss: 1.44; acc: 0.5
Batch: 240; loss: 1.25; acc: 0.62
Batch: 260; loss: 1.37; acc: 0.5
Batch: 280; loss: 1.25; acc: 0.66
Batch: 300; loss: 1.57; acc: 0.41
Batch: 320; loss: 1.1; acc: 0.61
Batch: 340; loss: 1.22; acc: 0.52
Batch: 360; loss: 1.23; acc: 0.55
Batch: 380; loss: 1.16; acc: 0.62
Batch: 400; loss: 1.31; acc: 0.58
Batch: 420; loss: 1.27; acc: 0.62
Batch: 440; loss: 1.18; acc: 0.62
Batch: 460; loss: 1.08; acc: 0.62
Batch: 480; loss: 1.22; acc: 0.5
Batch: 500; loss: 1.09; acc: 0.58
Batch: 520; loss: 1.21; acc: 0.58
Batch: 540; loss: 1.34; acc: 0.61
Batch: 560; loss: 1.08; acc: 0.61
Batch: 580; loss: 1.1; acc: 0.62
Batch: 600; loss: 1.04; acc: 0.66
Batch: 620; loss: 1.19; acc: 0.66
Batch: 640; loss: 1.17; acc: 0.56
Batch: 660; loss: 0.94; acc: 0.62
Batch: 680; loss: 1.08; acc: 0.56
Batch: 700; loss: 1.48; acc: 0.48
Batch: 720; loss: 1.22; acc: 0.56
Batch: 740; loss: 1.23; acc: 0.56
Batch: 760; loss: 1.4; acc: 0.52
Batch: 780; loss: 1.15; acc: 0.58
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.56
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.64
Batch: 120; loss: 1.38; acc: 0.58
Batch: 140; loss: 0.87; acc: 0.75
Val Epoch over. val_loss: 1.1285953252178849; val_accuracy: 0.626890923566879 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.56; acc: 0.52
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 1.26; acc: 0.53
Batch: 60; loss: 1.04; acc: 0.64
Batch: 80; loss: 1.2; acc: 0.56
Batch: 100; loss: 1.3; acc: 0.53
Batch: 120; loss: 1.16; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.53
Batch: 160; loss: 1.33; acc: 0.58
Batch: 180; loss: 1.32; acc: 0.52
Batch: 200; loss: 1.07; acc: 0.59
Batch: 220; loss: 1.24; acc: 0.61
Batch: 240; loss: 1.14; acc: 0.66
Batch: 260; loss: 1.27; acc: 0.56
Batch: 280; loss: 1.24; acc: 0.5
Batch: 300; loss: 1.05; acc: 0.64
Batch: 320; loss: 1.02; acc: 0.72
Batch: 340; loss: 1.09; acc: 0.69
Batch: 360; loss: 1.06; acc: 0.67
Batch: 380; loss: 1.49; acc: 0.44
Batch: 400; loss: 1.13; acc: 0.7
Batch: 420; loss: 1.22; acc: 0.59
Batch: 440; loss: 1.42; acc: 0.59
Batch: 460; loss: 1.01; acc: 0.64
Batch: 480; loss: 1.15; acc: 0.58
Batch: 500; loss: 1.23; acc: 0.58
Batch: 520; loss: 1.05; acc: 0.67
Batch: 540; loss: 1.55; acc: 0.5
Batch: 560; loss: 1.44; acc: 0.48
Batch: 580; loss: 1.12; acc: 0.61
Batch: 600; loss: 1.14; acc: 0.64
Batch: 620; loss: 1.19; acc: 0.56
Batch: 640; loss: 1.42; acc: 0.53
Batch: 660; loss: 1.19; acc: 0.55
Batch: 680; loss: 1.0; acc: 0.62
Batch: 700; loss: 1.54; acc: 0.56
Batch: 720; loss: 1.05; acc: 0.66
Batch: 740; loss: 1.23; acc: 0.59
Batch: 760; loss: 1.43; acc: 0.58
Batch: 780; loss: 1.28; acc: 0.52
Train Epoch over. train_loss: 1.2; train_accuracy: 0.6 

Batch: 0; loss: 1.29; acc: 0.58
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.58
Batch: 140; loss: 0.88; acc: 0.75
Val Epoch over. val_loss: 1.1286832268830318; val_accuracy: 0.627687101910828 

plots/subspace_training/reg_lenet_3/2020-01-19 20:22:47/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 21266
elements in E: 4499000
fraction nonzero: 0.004726828184040898
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.32; acc: 0.05
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.09
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.16
Batch: 340; loss: 2.3; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.29; acc: 0.08
Batch: 400; loss: 2.28; acc: 0.11
Batch: 420; loss: 2.28; acc: 0.16
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.29; acc: 0.17
Batch: 480; loss: 2.29; acc: 0.16
Batch: 500; loss: 2.29; acc: 0.12
Batch: 520; loss: 2.28; acc: 0.16
Batch: 540; loss: 2.28; acc: 0.25
Batch: 560; loss: 2.28; acc: 0.23
Batch: 580; loss: 2.28; acc: 0.25
Batch: 600; loss: 2.27; acc: 0.23
Batch: 620; loss: 2.27; acc: 0.2
Batch: 640; loss: 2.26; acc: 0.28
Batch: 660; loss: 2.26; acc: 0.27
Batch: 680; loss: 2.26; acc: 0.23
Batch: 700; loss: 2.26; acc: 0.23
Batch: 720; loss: 2.24; acc: 0.34
Batch: 740; loss: 2.26; acc: 0.22
Batch: 760; loss: 2.24; acc: 0.34
Batch: 780; loss: 2.24; acc: 0.38
Train Epoch over. train_loss: 2.29; train_accuracy: 0.15 

Batch: 0; loss: 2.25; acc: 0.33
Batch: 20; loss: 2.25; acc: 0.28
Batch: 40; loss: 2.22; acc: 0.42
Batch: 60; loss: 2.23; acc: 0.39
Batch: 80; loss: 2.23; acc: 0.36
Batch: 100; loss: 2.24; acc: 0.33
Batch: 120; loss: 2.24; acc: 0.31
Batch: 140; loss: 2.25; acc: 0.22
Val Epoch over. val_loss: 2.2437928861873164; val_accuracy: 0.317078025477707 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.24; acc: 0.28
Batch: 20; loss: 2.24; acc: 0.33
Batch: 40; loss: 2.22; acc: 0.36
Batch: 60; loss: 2.21; acc: 0.36
Batch: 80; loss: 2.19; acc: 0.47
Batch: 100; loss: 2.23; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.31
Batch: 140; loss: 2.19; acc: 0.33
Batch: 160; loss: 2.15; acc: 0.33
Batch: 180; loss: 2.15; acc: 0.36
Batch: 200; loss: 2.14; acc: 0.31
Batch: 220; loss: 2.14; acc: 0.27
Batch: 240; loss: 1.99; acc: 0.45
Batch: 260; loss: 1.98; acc: 0.38
Batch: 280; loss: 1.96; acc: 0.39
Batch: 300; loss: 1.89; acc: 0.36
Batch: 320; loss: 1.93; acc: 0.41
Batch: 340; loss: 1.83; acc: 0.36
Batch: 360; loss: 1.69; acc: 0.48
Batch: 380; loss: 1.59; acc: 0.53
Batch: 400; loss: 1.65; acc: 0.44
Batch: 420; loss: 1.5; acc: 0.55
Batch: 440; loss: 1.24; acc: 0.69
Batch: 460; loss: 1.3; acc: 0.61
Batch: 480; loss: 1.42; acc: 0.52
Batch: 500; loss: 1.35; acc: 0.52
Batch: 520; loss: 1.09; acc: 0.69
Batch: 540; loss: 1.14; acc: 0.62
Batch: 560; loss: 1.35; acc: 0.5
Batch: 580; loss: 1.08; acc: 0.64
Batch: 600; loss: 1.15; acc: 0.62
Batch: 620; loss: 1.33; acc: 0.56
Batch: 640; loss: 1.2; acc: 0.61
Batch: 660; loss: 1.06; acc: 0.67
Batch: 680; loss: 0.83; acc: 0.7
Batch: 700; loss: 1.16; acc: 0.59
Batch: 720; loss: 1.14; acc: 0.64
Batch: 740; loss: 0.82; acc: 0.77
Batch: 760; loss: 1.09; acc: 0.52
Batch: 780; loss: 0.86; acc: 0.73
Train Epoch over. train_loss: 1.63; train_accuracy: 0.5 

Batch: 0; loss: 1.49; acc: 0.52
Batch: 20; loss: 1.18; acc: 0.58
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 1.2; acc: 0.66
Batch: 80; loss: 0.87; acc: 0.69
Batch: 100; loss: 1.04; acc: 0.7
Batch: 120; loss: 1.44; acc: 0.5
Batch: 140; loss: 0.78; acc: 0.75
Val Epoch over. val_loss: 0.9930619730311594; val_accuracy: 0.6690883757961783 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.61
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.86; acc: 0.67
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 0.8; acc: 0.75
Batch: 160; loss: 0.92; acc: 0.77
Batch: 180; loss: 0.83; acc: 0.69
Batch: 200; loss: 0.73; acc: 0.77
Batch: 220; loss: 0.78; acc: 0.75
Batch: 240; loss: 0.76; acc: 0.73
Batch: 260; loss: 0.61; acc: 0.75
Batch: 280; loss: 0.88; acc: 0.73
Batch: 300; loss: 1.02; acc: 0.7
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.77
Batch: 400; loss: 0.81; acc: 0.69
Batch: 420; loss: 0.62; acc: 0.75
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.72
Batch: 480; loss: 0.73; acc: 0.73
Batch: 500; loss: 0.5; acc: 0.8
Batch: 520; loss: 1.07; acc: 0.61
Batch: 540; loss: 0.89; acc: 0.73
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.96; acc: 0.64
Batch: 600; loss: 0.62; acc: 0.77
Batch: 620; loss: 0.58; acc: 0.77
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.67; acc: 0.77
Batch: 680; loss: 1.04; acc: 0.67
Batch: 700; loss: 0.79; acc: 0.72
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.86
Train Epoch over. train_loss: 0.76; train_accuracy: 0.75 

Batch: 0; loss: 0.98; acc: 0.66
Batch: 20; loss: 1.19; acc: 0.64
Batch: 40; loss: 0.43; acc: 0.78
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.87; acc: 0.8
Batch: 120; loss: 0.93; acc: 0.75
Batch: 140; loss: 0.39; acc: 0.91
Val Epoch over. val_loss: 0.7162029254398529; val_accuracy: 0.7736863057324841 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.68; acc: 0.83
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.68; acc: 0.77
Batch: 100; loss: 0.73; acc: 0.73
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.92; acc: 0.72
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.77
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.94; acc: 0.67
Batch: 340; loss: 0.71; acc: 0.77
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.88; acc: 0.75
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.67; acc: 0.77
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.76; acc: 0.7
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.94; acc: 0.67
Batch: 640; loss: 0.69; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.69; acc: 0.83
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 1.05; acc: 0.64
Batch: 740; loss: 0.46; acc: 0.78
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.5821808306085077; val_accuracy: 0.8145899681528662 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.75
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.88; acc: 0.7
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.59; acc: 0.77
Batch: 200; loss: 0.63; acc: 0.73
Batch: 220; loss: 0.85; acc: 0.77
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.84
Batch: 360; loss: 0.8; acc: 0.73
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.47; acc: 0.92
Batch: 500; loss: 1.04; acc: 0.7
Batch: 520; loss: 0.69; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.71; acc: 0.78
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.78
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.69; acc: 0.77
Batch: 720; loss: 1.02; acc: 0.77
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.86; acc: 0.72
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.81; acc: 0.66
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.6265312318399454; val_accuracy: 0.7944864649681529 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.87; acc: 0.69
Batch: 100; loss: 0.6; acc: 0.75
Batch: 120; loss: 0.82; acc: 0.7
Batch: 140; loss: 0.6; acc: 0.8
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.72
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.86; acc: 0.75
Batch: 340; loss: 0.48; acc: 0.81
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.94; acc: 0.77
Batch: 400; loss: 0.82; acc: 0.72
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.75
Batch: 560; loss: 0.78; acc: 0.78
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.52; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.75
Batch: 680; loss: 0.73; acc: 0.72
Batch: 700; loss: 0.64; acc: 0.73
Batch: 720; loss: 0.57; acc: 0.8
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.8 

Batch: 0; loss: 0.67; acc: 0.7
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.83; acc: 0.72
Batch: 140; loss: 0.4; acc: 0.89
Val Epoch over. val_loss: 0.6063941657353359; val_accuracy: 0.8080214968152867 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.84; acc: 0.73
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.75
Batch: 160; loss: 0.68; acc: 0.77
Batch: 180; loss: 0.77; acc: 0.77
Batch: 200; loss: 0.43; acc: 0.8
Batch: 220; loss: 0.5; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.81
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.55; acc: 0.78
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.75
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.68; acc: 0.78
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.51; acc: 0.78
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.78
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.93; acc: 0.73
Batch: 620; loss: 0.79; acc: 0.73
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.76; acc: 0.69
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.68; acc: 0.77
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.61; train_accuracy: 0.8 

Batch: 0; loss: 0.54; acc: 0.78
Batch: 20; loss: 0.92; acc: 0.7
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.84
Val Epoch over. val_loss: 0.6117525622723209; val_accuracy: 0.8034434713375797 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.67; acc: 0.72
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.78
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.78
Batch: 360; loss: 0.88; acc: 0.77
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.49; acc: 0.81
Batch: 460; loss: 0.64; acc: 0.75
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.47; acc: 0.81
Batch: 520; loss: 0.58; acc: 0.81
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.75; acc: 0.69
Batch: 580; loss: 0.51; acc: 0.77
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.78
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.83
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.73
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.5016013614975723; val_accuracy: 0.8341958598726115 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.62; acc: 0.8
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.66; acc: 0.7
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.78
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.83; acc: 0.7
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.78
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.81; acc: 0.73
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.78
Batch: 420; loss: 0.88; acc: 0.7
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.95; acc: 0.69
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.83
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.62; acc: 0.75
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.69; acc: 0.78
Batch: 760; loss: 0.54; acc: 0.8
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.82; acc: 0.73
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.543994362661793; val_accuracy: 0.8274283439490446 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.78
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.75
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.53; acc: 0.8
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.65; acc: 0.81
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.83
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.94; acc: 0.72
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.77
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.77
Train Epoch over. train_loss: 0.54; train_accuracy: 0.82 

Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.91
Val Epoch over. val_loss: 0.5205491565784831; val_accuracy: 0.8318073248407644 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.79; acc: 0.8
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.6; acc: 0.78
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.59; acc: 0.77
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.77
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.81
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.77
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.56; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.5; train_accuracy: 0.84 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.4607508121782048; val_accuracy: 0.852109872611465 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.6; acc: 0.78
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.81
Batch: 500; loss: 0.8; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.77; acc: 0.72
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.37; acc: 0.84
Batch: 660; loss: 0.69; acc: 0.73
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.4607198359859977; val_accuracy: 0.8532046178343949 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.84
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.83
Batch: 200; loss: 0.61; acc: 0.81
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.78
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.51; acc: 0.81
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.4521696583670416; val_accuracy: 0.8549960191082803 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.8
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.37; acc: 0.81
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.55; acc: 0.78
Batch: 480; loss: 0.52; acc: 0.8
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.8
Batch: 580; loss: 0.43; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.81; acc: 0.77
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.65; acc: 0.78
Batch: 760; loss: 0.65; acc: 0.75
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.5; train_accuracy: 0.84 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.88
Val Epoch over. val_loss: 0.49870364282541213; val_accuracy: 0.8382762738853503 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.65; acc: 0.81
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.81
Batch: 220; loss: 0.63; acc: 0.86
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.44; acc: 0.81
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.85; acc: 0.73
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.49; acc: 0.8
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.62; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.8
Batch: 680; loss: 0.8; acc: 0.73
Batch: 700; loss: 0.58; acc: 0.8
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.64; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.75
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4641028629366759; val_accuracy: 0.8515127388535032 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.57; acc: 0.78
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.29; acc: 0.86
Batch: 360; loss: 0.82; acc: 0.75
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.55; acc: 0.8
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.66
Batch: 680; loss: 0.35; acc: 0.83
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.81
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.84 

Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4650874150216959; val_accuracy: 0.8515127388535032 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.78
Batch: 160; loss: 0.67; acc: 0.77
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.74; acc: 0.77
Batch: 300; loss: 0.52; acc: 0.78
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.8
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.8
Batch: 520; loss: 0.7; acc: 0.75
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.66; acc: 0.77
Batch: 600; loss: 0.58; acc: 0.78
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.81
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.8
Batch: 700; loss: 0.58; acc: 0.77
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.84 

Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.4747403749045293; val_accuracy: 0.8471337579617835 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.57; acc: 0.8
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.73; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.76; acc: 0.77
Batch: 380; loss: 0.55; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.62; acc: 0.86
Batch: 500; loss: 0.95; acc: 0.69
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.81
Batch: 560; loss: 0.9; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.65; acc: 0.8
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.83
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.84 

Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.88
Val Epoch over. val_loss: 0.48302464637976544; val_accuracy: 0.8463375796178344 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.75; acc: 0.77
Batch: 180; loss: 0.57; acc: 0.77
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.78
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.32; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.77
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.81
Batch: 640; loss: 0.46; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.63; acc: 0.73
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.84 

Batch: 0; loss: 0.59; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.540812618793196; val_accuracy: 0.8277269108280255 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 0.76; acc: 0.77
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.56; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.66; acc: 0.8
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.67; acc: 0.77
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.68; acc: 0.75
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.84 

Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.4726617869677817; val_accuracy: 0.8481289808917197 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.81
Batch: 180; loss: 0.65; acc: 0.77
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.62; acc: 0.75
Batch: 240; loss: 0.52; acc: 0.78
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.71; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.38; acc: 0.83
Batch: 520; loss: 0.6; acc: 0.77
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.46; acc: 0.81
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.4466719035367677; val_accuracy: 0.8572850318471338 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.81
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.75
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.8
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.8
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.84 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.4541018259278528; val_accuracy: 0.8517117834394905 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.78; acc: 0.8
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.85; acc: 0.75
Batch: 200; loss: 0.51; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.83
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.81
Batch: 280; loss: 0.42; acc: 0.84
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.77; acc: 0.73
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.87; acc: 0.77
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.4440619113623716; val_accuracy: 0.8592754777070064 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.58; acc: 0.78
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.78
Batch: 380; loss: 0.6; acc: 0.78
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.54; acc: 0.92
Batch: 620; loss: 0.72; acc: 0.8
Batch: 640; loss: 0.4; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4482878542677232; val_accuracy: 0.8558917197452229 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.78
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.53; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.8
Batch: 680; loss: 0.71; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.77; acc: 0.73
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.4467935727754976; val_accuracy: 0.857484076433121 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.8
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.81
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.9; acc: 0.77
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.73
Batch: 320; loss: 0.41; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.8; acc: 0.73
Batch: 660; loss: 0.55; acc: 0.75
Batch: 680; loss: 0.72; acc: 0.77
Batch: 700; loss: 0.34; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.4426159968089526; val_accuracy: 0.8591759554140127 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.8
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.77
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.81
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.77
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.8
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.81
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.8
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.59; acc: 0.78
Batch: 660; loss: 0.51; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.79; acc: 0.77
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.445777018999408; val_accuracy: 0.8570859872611465 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.83
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.81
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.81
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.78
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 0.74; acc: 0.75
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.62; acc: 0.8
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.66; acc: 0.77
Batch: 720; loss: 0.67; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.66; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.4558057974857889; val_accuracy: 0.8540007961783439 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.26; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.71; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.73
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.78; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.78
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.83
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.81
Batch: 700; loss: 0.5; acc: 0.83
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.78
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.44220499732312124; val_accuracy: 0.8579816878980892 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.71; acc: 0.73
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.84
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.8
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.68; acc: 0.77
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.77
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.55; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.8
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.84
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.43988747916119114; val_accuracy: 0.8599721337579618 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.68; acc: 0.77
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.81
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.6; acc: 0.78
Batch: 460; loss: 0.51; acc: 0.78
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.78
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 0.6; acc: 0.78
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.77
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4398139191281264; val_accuracy: 0.8578821656050956 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.81
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.72
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.73
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.83
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 0.44; acc: 0.8
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4401985841571905; val_accuracy: 0.8607683121019108 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.8
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.8; acc: 0.77
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.75; acc: 0.77
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.66; acc: 0.78
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.75
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.44081392317156126; val_accuracy: 0.8600716560509554 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.74; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.81
Batch: 300; loss: 0.48; acc: 0.83
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.88
Batch: 380; loss: 0.54; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.78
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.78
Batch: 480; loss: 0.61; acc: 0.73
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.44100403114204195; val_accuracy: 0.8565883757961783 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.75
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.81
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.8
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.81
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.72; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.73
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.41; acc: 0.83
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.439314379006814; val_accuracy: 0.8599721337579618 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.55; acc: 0.81
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.8
Batch: 180; loss: 0.69; acc: 0.77
Batch: 200; loss: 0.61; acc: 0.81
Batch: 220; loss: 0.43; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.73
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.84; acc: 0.8
Batch: 340; loss: 0.69; acc: 0.77
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.8
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.81
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.64; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4413544640039942; val_accuracy: 0.8572850318471338 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.8
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.56; acc: 0.8
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.7; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.78
Batch: 580; loss: 0.62; acc: 0.78
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.88
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.57; acc: 0.89
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4409025307436278; val_accuracy: 0.8580812101910829 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.74; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.8
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.67; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.78
Batch: 540; loss: 0.51; acc: 0.8
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.62; acc: 0.77
Batch: 720; loss: 0.41; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.43878671028621635; val_accuracy: 0.8601711783439491 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.75
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.84
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.74; acc: 0.77
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.68; acc: 0.8
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.74; acc: 0.81
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.59; acc: 0.8
Batch: 760; loss: 0.66; acc: 0.75
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.4380225208914204; val_accuracy: 0.8606687898089171 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.95; acc: 0.78
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.78
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.81
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.81; acc: 0.83
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.65; acc: 0.73
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.4386248573850674; val_accuracy: 0.859375 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.81
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.8
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.6; acc: 0.81
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.77
Batch: 660; loss: 0.84; acc: 0.72
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4393760222395894; val_accuracy: 0.8598726114649682 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.63; acc: 0.77
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.7; acc: 0.7
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.8
Batch: 260; loss: 0.59; acc: 0.8
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.78
Batch: 340; loss: 0.63; acc: 0.72
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.64; acc: 0.78
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.81
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.4378355175114361; val_accuracy: 0.8601711783439491 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.78
Batch: 680; loss: 0.43; acc: 0.81
Batch: 700; loss: 0.49; acc: 0.78
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.6; acc: 0.8
Batch: 780; loss: 0.83; acc: 0.73
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.4374958991909483; val_accuracy: 0.859375 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.88; acc: 0.78
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.8
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.81
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.77
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.57; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.62; acc: 0.78
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.8
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.62; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.4381836564487712; val_accuracy: 0.8605692675159236 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.81
Batch: 340; loss: 0.69; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.63; acc: 0.78
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.81
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.78
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.43735556442076995; val_accuracy: 0.859375 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.78
Batch: 180; loss: 0.62; acc: 0.72
Batch: 200; loss: 0.61; acc: 0.86
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.7; acc: 0.77
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.51; acc: 0.8
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.84
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.81
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.78
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.64; acc: 0.78
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.43794434477284455; val_accuracy: 0.8600716560509554 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.81
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.59; acc: 0.77
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.8
Batch: 520; loss: 0.61; acc: 0.8
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.69; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.81
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4390122831152503; val_accuracy: 0.859375 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.77
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.78
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.75
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.5; acc: 0.81
Batch: 400; loss: 0.82; acc: 0.83
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.68; acc: 0.89
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.74; acc: 0.8
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.81
Batch: 660; loss: 0.49; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.8
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.43774141454298027; val_accuracy: 0.8600716560509554 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.83; acc: 0.73
Batch: 260; loss: 0.68; acc: 0.75
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.75
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.67; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.81
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.87; acc: 0.77
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.43853668685836394; val_accuracy: 0.8588773885350318 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.83; acc: 0.78
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.59; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.66; acc: 0.78
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.84; acc: 0.77
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4381672347996645; val_accuracy: 0.8590764331210191 

plots/subspace_training/reg_lenet_3/2020-01-19 20:22:47/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 42326
elements in E: 8998000
fraction nonzero: 0.004703934207601689
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.11
Batch: 140; loss: 2.3; acc: 0.19
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.3; acc: 0.05
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.11
Batch: 260; loss: 2.27; acc: 0.12
Batch: 280; loss: 2.27; acc: 0.14
Batch: 300; loss: 2.26; acc: 0.28
Batch: 320; loss: 2.26; acc: 0.34
Batch: 340; loss: 2.26; acc: 0.36
Batch: 360; loss: 2.25; acc: 0.42
Batch: 380; loss: 2.23; acc: 0.41
Batch: 400; loss: 2.22; acc: 0.36
Batch: 420; loss: 2.2; acc: 0.42
Batch: 440; loss: 2.14; acc: 0.56
Batch: 460; loss: 2.14; acc: 0.45
Batch: 480; loss: 2.12; acc: 0.39
Batch: 500; loss: 1.97; acc: 0.55
Batch: 520; loss: 1.85; acc: 0.48
Batch: 540; loss: 1.74; acc: 0.56
Batch: 560; loss: 1.32; acc: 0.66
Batch: 580; loss: 1.37; acc: 0.58
Batch: 600; loss: 0.98; acc: 0.72
Batch: 620; loss: 0.91; acc: 0.75
Batch: 640; loss: 0.99; acc: 0.75
Batch: 660; loss: 0.84; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.78
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.87; acc: 0.75
Batch: 780; loss: 0.67; acc: 0.78
Train Epoch over. train_loss: 1.82; train_accuracy: 0.41 

Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.7
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 1.08; acc: 0.78
Batch: 140; loss: 0.4; acc: 0.91
Val Epoch over. val_loss: 0.6018144340272162; val_accuracy: 0.8168789808917197 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.75
Batch: 20; loss: 0.79; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.9; acc: 0.77
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.68; acc: 0.83
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.8
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3605548650216145; val_accuracy: 0.8915207006369427 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.31; acc: 0.86
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.8
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.78
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3413187080792561; val_accuracy: 0.8946058917197452 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.34; acc: 0.84
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.58; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2805915356251844; val_accuracy: 0.9125199044585988 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.81
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2606114596128464; val_accuracy: 0.9201831210191083 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.57; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.09; acc: 1.0
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.276464614139241; val_accuracy: 0.9107285031847133 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.19; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.78
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.40088205082211525; val_accuracy: 0.8779856687898089 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 1.27; acc: 0.69
Batch: 140; loss: 0.3; acc: 0.88
Val Epoch over. val_loss: 0.5113865446038307; val_accuracy: 0.8333996815286624 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.92; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.4941764027829383; val_accuracy: 0.8544984076433121 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.2841394827908771; val_accuracy: 0.9113256369426752 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.22840071156336245; val_accuracy: 0.9306329617834395 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.22940911756009813; val_accuracy: 0.930234872611465 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.97
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26028950782907995; val_accuracy: 0.9244625796178344 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.24490569394295383; val_accuracy: 0.9242635350318471 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.1; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2263517468028767; val_accuracy: 0.9322253184713376 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.6; acc: 0.8
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.23; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.21891448959992948; val_accuracy: 0.9333200636942676 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22168429314520707; val_accuracy: 0.9330214968152867 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.52; acc: 0.89
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.22413643824446733; val_accuracy: 0.9330214968152867 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23466601550199423; val_accuracy: 0.927547770700637 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2308634014645959; val_accuracy: 0.9331210191082803 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22199335604146786; val_accuracy: 0.9338176751592356 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.54; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22089832503894333; val_accuracy: 0.932921974522293 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.54; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.26; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.224645595309461; val_accuracy: 0.9335191082802548 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22818191406453492; val_accuracy: 0.9320262738853503 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.86
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.68; acc: 0.84
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.224406435421318; val_accuracy: 0.9330214968152867 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.08; acc: 1.0
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21546102917877732; val_accuracy: 0.9350119426751592 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21937643114928226; val_accuracy: 0.9322253184713376 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.88
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2203137972362482; val_accuracy: 0.9339171974522293 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2178874379320509; val_accuracy: 0.9346138535031847 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21877862043251659; val_accuracy: 0.9350119426751592 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.29; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21553948194168177; val_accuracy: 0.9352109872611465 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21700112811129563; val_accuracy: 0.9357085987261147 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2201367379848365; val_accuracy: 0.9353105095541401 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.09; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.21748766032563652; val_accuracy: 0.9357085987261147 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21977181841803203; val_accuracy: 0.934812898089172 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21917974317719222; val_accuracy: 0.933718152866242 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2160897709571632; val_accuracy: 0.9346138535031847 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.51; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.15; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2167562597021935; val_accuracy: 0.9353105095541401 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.4; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2187272898235898; val_accuracy: 0.9352109872611465 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2158739623275532; val_accuracy: 0.934812898089172 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.88
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.22; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.83
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21539810926291594; val_accuracy: 0.9351114649681529 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.81
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.15; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2153298804524598; val_accuracy: 0.935609076433121 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.86
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21571414936689814; val_accuracy: 0.9350119426751592 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21561872982864927; val_accuracy: 0.9347133757961783 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21501030118032627; val_accuracy: 0.934812898089172 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.35; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21613882277991361; val_accuracy: 0.9351114649681529 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21547309964135952; val_accuracy: 0.9350119426751592 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21475638131237335; val_accuracy: 0.9359076433121019 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.11; acc: 1.0
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21619003546086085; val_accuracy: 0.9357085987261147 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.88
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21590790119330594; val_accuracy: 0.9353105095541401 

plots/subspace_training/reg_lenet_3/2020-01-19 20:22:47/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 63176
elements in E: 13497000
fraction nonzero: 0.0046807438690079275
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.17
Batch: 80; loss: 2.29; acc: 0.17
Batch: 100; loss: 2.29; acc: 0.09
Batch: 120; loss: 2.29; acc: 0.16
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.27; acc: 0.22
Batch: 180; loss: 2.27; acc: 0.38
Batch: 200; loss: 2.24; acc: 0.47
Batch: 220; loss: 2.22; acc: 0.52
Batch: 240; loss: 2.21; acc: 0.36
Batch: 260; loss: 2.18; acc: 0.41
Batch: 280; loss: 2.12; acc: 0.47
Batch: 300; loss: 1.9; acc: 0.5
Batch: 320; loss: 1.68; acc: 0.56
Batch: 340; loss: 1.31; acc: 0.62
Batch: 360; loss: 1.11; acc: 0.64
Batch: 380; loss: 0.95; acc: 0.62
Batch: 400; loss: 0.97; acc: 0.67
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.81; acc: 0.73
Batch: 460; loss: 0.85; acc: 0.7
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.54; acc: 0.78
Batch: 520; loss: 0.76; acc: 0.78
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.87; acc: 0.77
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.73
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 1.3; train_accuracy: 0.58 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.78
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.36015585999769767; val_accuracy: 0.8916202229299363 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.79; acc: 0.83
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 1.19; acc: 0.77
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.86
Val Epoch over. val_loss: 0.5219261528580053; val_accuracy: 0.8357882165605095 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.30955153437936384; val_accuracy: 0.8998805732484076 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.11; acc: 1.0
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.86
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.18278762793085376; val_accuracy: 0.9441679936305732 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.86
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.95
Val Epoch over. val_loss: 0.17237567275193086; val_accuracy: 0.9483479299363057 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.16045472614324777; val_accuracy: 0.9509355095541401 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1713448379924343; val_accuracy: 0.9471536624203821 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.21881797188406538; val_accuracy: 0.9307324840764332 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.33488084836180804; val_accuracy: 0.9028662420382165 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17754862533443294; val_accuracy: 0.9481488853503185 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.18; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13649163632446035; val_accuracy: 0.9590963375796179 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14463034620994974; val_accuracy: 0.9571058917197452 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.14268404088772027; val_accuracy: 0.9578025477707006 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.16755796731657283; val_accuracy: 0.9503383757961783 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13596013715122915; val_accuracy: 0.960390127388535 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.89
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1473297375212809; val_accuracy: 0.9557125796178344 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.91
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.134005126584867; val_accuracy: 0.9599920382165605 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1311310099283601; val_accuracy: 0.9594944267515924 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1328040186766606; val_accuracy: 0.960390127388535 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.88
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15492049784986836; val_accuracy: 0.953125 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.91
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.1309814723149227; val_accuracy: 0.9609872611464968 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.89
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13035023893425418; val_accuracy: 0.9609872611464968 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.91
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.98
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12693950676234664; val_accuracy: 0.9617834394904459 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.88
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.03; acc: 0.97
Val Epoch over. val_loss: 0.13623546861159574; val_accuracy: 0.9597929936305732 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13194275628419438; val_accuracy: 0.9613853503184714 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12521141762756238; val_accuracy: 0.9627786624203821 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1253824043235961; val_accuracy: 0.9623805732484076 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.13128272448755374; val_accuracy: 0.9624800955414012 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.92
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12468413804556913; val_accuracy: 0.9628781847133758 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12625356122946282; val_accuracy: 0.963077229299363 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1267525721697291; val_accuracy: 0.9617834394904459 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12630220671083517; val_accuracy: 0.9606886942675159 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12278087861883412; val_accuracy: 0.9634753184713376 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1256389253932959; val_accuracy: 0.9631767515923567 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12375012913327309; val_accuracy: 0.9631767515923567 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12760919873501844; val_accuracy: 0.9637738853503185 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12571896717047235; val_accuracy: 0.9621815286624203 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.92
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12381551538113576; val_accuracy: 0.9632762738853503 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.43; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12627809637101592; val_accuracy: 0.9634753184713376 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.98
Batch: 280; loss: 0.38; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12304636451658929; val_accuracy: 0.9636743630573248 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12244005097894911; val_accuracy: 0.9643710191082803 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12279556933671805; val_accuracy: 0.9648686305732485 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12370042146960641; val_accuracy: 0.9635748407643312 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12280370553682564; val_accuracy: 0.9636743630573248 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12191208105558043; val_accuracy: 0.964171974522293 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1231393949336307; val_accuracy: 0.9634753184713376 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12230444474110178; val_accuracy: 0.9643710191082803 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.32; acc: 0.97
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.91
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1223924558633452; val_accuracy: 0.9650676751592356 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.21; acc: 0.98
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.89
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.3; acc: 0.86
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12265998943691041; val_accuracy: 0.9640724522292994 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.94
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1227122147087079; val_accuracy: 0.9638734076433121 

plots/subspace_training/reg_lenet_3/2020-01-19 20:22:47/d_dim_300_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 84360
elements in E: 17996000
fraction nonzero: 0.00468770837963992
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.17
Batch: 60; loss: 2.3; acc: 0.2
Batch: 80; loss: 2.28; acc: 0.17
Batch: 100; loss: 2.28; acc: 0.06
Batch: 120; loss: 2.27; acc: 0.2
Batch: 140; loss: 2.25; acc: 0.33
Batch: 160; loss: 2.21; acc: 0.52
Batch: 180; loss: 2.14; acc: 0.39
Batch: 200; loss: 1.82; acc: 0.58
Batch: 220; loss: 1.29; acc: 0.69
Batch: 240; loss: 1.16; acc: 0.53
Batch: 260; loss: 0.99; acc: 0.64
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.71; acc: 0.77
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.95
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.78
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.76; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.95; train_accuracy: 0.69 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.24966015652486473; val_accuracy: 0.9210788216560509 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.94; acc: 0.72
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.4327967759150608; val_accuracy: 0.8565883757961783 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.20220453847365774; val_accuracy: 0.9355095541401274 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14995835541160243; val_accuracy: 0.9517316878980892 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.14559163385705584; val_accuracy: 0.955812101910828 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.51; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.94
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.13702556189552995; val_accuracy: 0.9583996815286624 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.86
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.37906060310875533; val_accuracy: 0.894406847133758 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.98
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14332669090693165; val_accuracy: 0.9554140127388535 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.83
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.94
Val Epoch over. val_loss: 0.2961541916344576; val_accuracy: 0.9093351910828026 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11380471018659082; val_accuracy: 0.9673566878980892 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10390556444692764; val_accuracy: 0.9689490445859873 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11444547402251298; val_accuracy: 0.9653662420382165 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10106668476561073; val_accuracy: 0.9697452229299363 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.13612770910855312; val_accuracy: 0.9596934713375797 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10023522737679208; val_accuracy: 0.971437101910828 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.91
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10072506546594534; val_accuracy: 0.9703423566878981 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09807355030421998; val_accuracy: 0.9710390127388535 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09738771600802992; val_accuracy: 0.9701433121019108 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10016057372188113; val_accuracy: 0.9694466560509554 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09950352792337441; val_accuracy: 0.9705414012738853 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.92
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09505037666790804; val_accuracy: 0.9712380573248408 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09684556907719108; val_accuracy: 0.9700437898089171 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.94
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.92
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09854686962571113; val_accuracy: 0.9694466560509554 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09829193946852047; val_accuracy: 0.9699442675159236 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09431421652341344; val_accuracy: 0.9715366242038217 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09187771835524565; val_accuracy: 0.9727308917197452 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0906734896740716; val_accuracy: 0.9734275477707006 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09242188486798554; val_accuracy: 0.9726313694267515 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.94
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09187565482915587; val_accuracy: 0.9726313694267515 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09253200468648771; val_accuracy: 0.9719347133757962 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09018242219166392; val_accuracy: 0.9736265923566879 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09027894287352349; val_accuracy: 0.9732285031847133 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.88
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09076006701038142; val_accuracy: 0.9722332802547771 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08968942909009138; val_accuracy: 0.9743232484076433 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.94
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09170737269387882; val_accuracy: 0.9722332802547771 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.89
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09046962150153082; val_accuracy: 0.9727308917197452 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0905881658386273; val_accuracy: 0.9740246815286624 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.94
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08969730416395862; val_accuracy: 0.9724323248407644 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.27; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.05; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.89
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09070843254115171; val_accuracy: 0.9728304140127388 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09023936574531209; val_accuracy: 0.973328025477707 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08981608817721629; val_accuracy: 0.9731289808917197 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08934501081610181; val_accuracy: 0.9741242038216561 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08951623277489547; val_accuracy: 0.9738256369426752 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08974891915253014; val_accuracy: 0.973328025477707 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08973224819370895; val_accuracy: 0.9734275477707006 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08965097728428567; val_accuracy: 0.9736265923566879 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08914188746434108; val_accuracy: 0.9732285031847133 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08895999589448522; val_accuracy: 0.9735270700636943 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0894248725692178; val_accuracy: 0.9734275477707006 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08918349141148245; val_accuracy: 0.974422770700637 

plots/subspace_training/reg_lenet_3/2020-01-19 20:22:47/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 105958
elements in E: 22495000
fraction nonzero: 0.004710291175816848
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.19
Batch: 80; loss: 2.28; acc: 0.12
Batch: 100; loss: 2.28; acc: 0.06
Batch: 120; loss: 2.27; acc: 0.16
Batch: 140; loss: 2.25; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.42
Batch: 180; loss: 2.19; acc: 0.27
Batch: 200; loss: 2.0; acc: 0.58
Batch: 220; loss: 1.7; acc: 0.66
Batch: 240; loss: 1.43; acc: 0.58
Batch: 260; loss: 1.23; acc: 0.56
Batch: 280; loss: 0.94; acc: 0.67
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.91; acc: 0.66
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.77; acc: 0.7
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 1.01; train_accuracy: 0.66 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.27897920991015285; val_accuracy: 0.916202229299363 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18857113718037394; val_accuracy: 0.9397890127388535 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.92
Val Epoch over. val_loss: 0.1985382378623364; val_accuracy: 0.9394904458598726 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11334354358066799; val_accuracy: 0.9670581210191083 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1067007917459983; val_accuracy: 0.9678542993630573 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.27; acc: 0.88
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.37; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11321613346790052; val_accuracy: 0.9650676751592356 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16944443773786733; val_accuracy: 0.951234076433121 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.095229563819375; val_accuracy: 0.970640923566879 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1357085909101234; val_accuracy: 0.9605891719745223 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09203149748455947; val_accuracy: 0.974422770700637 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07958013213174359; val_accuracy: 0.9746218152866242 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.91
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07650730284346137; val_accuracy: 0.9763136942675159 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.94
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07753256150776414; val_accuracy: 0.9766122611464968 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09481966343654949; val_accuracy: 0.9711385350318471 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08080106344857034; val_accuracy: 0.9759156050955414 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.94
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08039005667825413; val_accuracy: 0.976015127388535 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08442864750933116; val_accuracy: 0.9763136942675159 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08060394016325853; val_accuracy: 0.9749203821656051 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07777056304417598; val_accuracy: 0.9767117834394905 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07987273365828641; val_accuracy: 0.9758160828025477 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.3; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07816251141914896; val_accuracy: 0.9761146496815286 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07656938414190226; val_accuracy: 0.9772093949044586 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07682581655206575; val_accuracy: 0.9777070063694268 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07637700755267766; val_accuracy: 0.9772093949044586 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07494953784878088; val_accuracy: 0.9780055732484076 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.95
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07500422168167153; val_accuracy: 0.977906050955414 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08029691581940575; val_accuracy: 0.9765127388535032 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07584300721479449; val_accuracy: 0.9770103503184714 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07540472477057557; val_accuracy: 0.9775079617834395 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07654811185636338; val_accuracy: 0.977109872611465 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.95
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07409380795137518; val_accuracy: 0.977906050955414 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07454626405742137; val_accuracy: 0.9782046178343949 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07462707724256121; val_accuracy: 0.9782046178343949 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07561832484878173; val_accuracy: 0.9773089171974523 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0747689574268783; val_accuracy: 0.9776074840764332 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0743423212604348; val_accuracy: 0.9781050955414012 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07402309972294577; val_accuracy: 0.9782046178343949 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07354037486206574; val_accuracy: 0.977906050955414 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.94
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07541062293728445; val_accuracy: 0.9776074840764332 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07446055357480884; val_accuracy: 0.9777070063694268 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.94
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07447212837209367; val_accuracy: 0.9778065286624203 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07436250387483341; val_accuracy: 0.9778065286624203 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07365232598107711; val_accuracy: 0.9784036624203821 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0736093714501068; val_accuracy: 0.977906050955414 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07387697275514436; val_accuracy: 0.9784036624203821 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07420103644631851; val_accuracy: 0.9778065286624203 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07401555273563239; val_accuracy: 0.9783041401273885 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07405813126142617; val_accuracy: 0.9782046178343949 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07395704797688563; val_accuracy: 0.9777070063694268 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07401379413760392; val_accuracy: 0.9780055732484076 

plots/subspace_training/reg_lenet_3/2020-01-19 20:22:47/d_dim_500_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/reg_lenet_3/2020-01-19 20:22:47/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
/var/spool/slurm-llnl/slurmd/job4385837/slurm_script: line 25: --print_freq=20: command not found
