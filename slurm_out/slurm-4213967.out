Epoch 1 start
Batch: 0; loss: 16.79; acc: 0.06
Batch: 20; loss: 5.92; acc: 0.17
Batch: 40; loss: 5.23; acc: 0.22
Batch: 60; loss: 5.34; acc: 0.22
Batch: 80; loss: 4.47; acc: 0.27
Batch: 100; loss: 4.89; acc: 0.27
Batch: 120; loss: 4.97; acc: 0.28
Batch: 140; loss: 4.18; acc: 0.28
Batch: 160; loss: 2.95; acc: 0.41
Batch: 180; loss: 4.4; acc: 0.36
Batch: 200; loss: 4.49; acc: 0.36
Batch: 220; loss: 4.39; acc: 0.33
Batch: 240; loss: 4.45; acc: 0.38
Batch: 260; loss: 4.17; acc: 0.45
Batch: 280; loss: 5.03; acc: 0.27
Batch: 300; loss: 3.78; acc: 0.36
Batch: 320; loss: 4.18; acc: 0.33
Batch: 340; loss: 4.31; acc: 0.34
Batch: 360; loss: 4.92; acc: 0.42
Batch: 380; loss: 4.56; acc: 0.27
Batch: 400; loss: 4.78; acc: 0.31
Batch: 420; loss: 3.97; acc: 0.41
Batch: 440; loss: 3.28; acc: 0.41
Batch: 460; loss: 4.46; acc: 0.3
Batch: 480; loss: 3.53; acc: 0.48
Batch: 500; loss: 4.5; acc: 0.38
Batch: 520; loss: 4.34; acc: 0.38
Batch: 540; loss: 3.39; acc: 0.42
Batch: 560; loss: 4.88; acc: 0.3
Batch: 580; loss: 3.92; acc: 0.42
Batch: 600; loss: 5.88; acc: 0.38
Batch: 620; loss: 4.77; acc: 0.36
Train Epoch over. train_loss: 4.47; train_accuracy: 0.33 

Batch: 0; loss: 4.23; acc: 0.38
Batch: 20; loss: 5.77; acc: 0.27
Batch: 40; loss: 3.29; acc: 0.52
Batch: 60; loss: 3.97; acc: 0.3
Batch: 80; loss: 5.7; acc: 0.22
Batch: 100; loss: 4.19; acc: 0.41
Batch: 120; loss: 4.47; acc: 0.47
Batch: 140; loss: 6.13; acc: 0.3
Val Epoch over. val_loss: 4.408539026406161; val_accuracy: 0.3708200636942675 

Epoch 2 start
Batch: 0; loss: 3.02; acc: 0.5
Batch: 20; loss: 3.08; acc: 0.38
Batch: 40; loss: 4.68; acc: 0.36
Batch: 60; loss: 3.91; acc: 0.45
Batch: 80; loss: 3.1; acc: 0.47
Batch: 100; loss: 4.55; acc: 0.44
Batch: 120; loss: 4.78; acc: 0.36
Batch: 140; loss: 3.91; acc: 0.39
Batch: 160; loss: 3.51; acc: 0.44
Batch: 180; loss: 3.37; acc: 0.45
Batch: 200; loss: 4.42; acc: 0.39
Batch: 220; loss: 2.81; acc: 0.55
Batch: 240; loss: 3.86; acc: 0.47
Batch: 260; loss: 4.24; acc: 0.47
Batch: 280; loss: 3.68; acc: 0.44
Batch: 300; loss: 3.53; acc: 0.45
Batch: 320; loss: 3.76; acc: 0.38
Batch: 340; loss: 3.77; acc: 0.3
Batch: 360; loss: 4.45; acc: 0.36
Batch: 380; loss: 3.71; acc: 0.45
Batch: 400; loss: 3.25; acc: 0.45
Batch: 420; loss: 2.85; acc: 0.52
Batch: 440; loss: 4.75; acc: 0.41
Batch: 460; loss: 5.19; acc: 0.22
Batch: 480; loss: 3.61; acc: 0.42
Batch: 500; loss: 3.62; acc: 0.42
Batch: 520; loss: 4.33; acc: 0.41
Batch: 540; loss: 3.74; acc: 0.41
Batch: 560; loss: 4.87; acc: 0.36
Batch: 580; loss: 4.02; acc: 0.38
Batch: 600; loss: 4.3; acc: 0.34
Batch: 620; loss: 4.47; acc: 0.39
Train Epoch over. train_loss: 4.01; train_accuracy: 0.41 

Batch: 0; loss: 3.62; acc: 0.34
Batch: 20; loss: 5.79; acc: 0.33
Batch: 40; loss: 2.63; acc: 0.48
Batch: 60; loss: 3.77; acc: 0.3
Batch: 80; loss: 4.89; acc: 0.34
Batch: 100; loss: 4.07; acc: 0.39
Batch: 120; loss: 4.3; acc: 0.45
Batch: 140; loss: 5.53; acc: 0.3
Val Epoch over. val_loss: 3.9730068361683255; val_accuracy: 0.414609872611465 

Epoch 3 start
Batch: 0; loss: 3.63; acc: 0.42
Batch: 20; loss: 4.54; acc: 0.3
Batch: 40; loss: 2.83; acc: 0.52
Batch: 60; loss: 3.7; acc: 0.5
Batch: 80; loss: 4.07; acc: 0.39
Batch: 100; loss: 3.62; acc: 0.42
Batch: 120; loss: 3.45; acc: 0.45
Batch: 140; loss: 4.14; acc: 0.42
Batch: 160; loss: 3.98; acc: 0.41
Batch: 180; loss: 3.56; acc: 0.44
Batch: 200; loss: 3.38; acc: 0.5
Batch: 220; loss: 4.66; acc: 0.38
Batch: 240; loss: 5.0; acc: 0.31
Batch: 260; loss: 3.64; acc: 0.52
Batch: 280; loss: 3.47; acc: 0.41
Batch: 300; loss: 3.48; acc: 0.36
Batch: 320; loss: 3.41; acc: 0.42
Batch: 340; loss: 2.85; acc: 0.48
Batch: 360; loss: 3.85; acc: 0.52
Batch: 380; loss: 3.07; acc: 0.47
Batch: 400; loss: 4.85; acc: 0.38
Batch: 420; loss: 3.33; acc: 0.42
Batch: 440; loss: 3.68; acc: 0.45
Batch: 460; loss: 3.69; acc: 0.48
Batch: 480; loss: 4.41; acc: 0.48
Batch: 500; loss: 4.25; acc: 0.48
Batch: 520; loss: 4.54; acc: 0.3
Batch: 540; loss: 4.34; acc: 0.36
Batch: 560; loss: 3.3; acc: 0.52
Batch: 580; loss: 3.53; acc: 0.41
Batch: 600; loss: 4.52; acc: 0.42
Batch: 620; loss: 3.43; acc: 0.45
Train Epoch over. train_loss: 4.0; train_accuracy: 0.41 

Batch: 0; loss: 4.46; acc: 0.23
Batch: 20; loss: 6.01; acc: 0.27
Batch: 40; loss: 3.38; acc: 0.44
Batch: 60; loss: 4.35; acc: 0.31
Batch: 80; loss: 5.42; acc: 0.3
Batch: 100; loss: 4.05; acc: 0.44
Batch: 120; loss: 4.08; acc: 0.45
Batch: 140; loss: 5.23; acc: 0.31
Val Epoch over. val_loss: 4.281536220744917; val_accuracy: 0.37460191082802546 

Epoch 4 start
Batch: 0; loss: 4.42; acc: 0.38
Batch: 20; loss: 5.04; acc: 0.34
Batch: 40; loss: 4.43; acc: 0.47
Batch: 60; loss: 3.59; acc: 0.42
Batch: 80; loss: 3.92; acc: 0.34
Batch: 100; loss: 3.63; acc: 0.48
Batch: 120; loss: 4.8; acc: 0.36
Batch: 140; loss: 4.28; acc: 0.28
Batch: 160; loss: 3.51; acc: 0.41
Batch: 180; loss: 3.69; acc: 0.56
Batch: 200; loss: 3.83; acc: 0.34
Batch: 220; loss: 4.23; acc: 0.34
Batch: 240; loss: 4.05; acc: 0.44
Batch: 260; loss: 4.21; acc: 0.39
Batch: 280; loss: 3.41; acc: 0.39
Batch: 300; loss: 4.35; acc: 0.41
Batch: 320; loss: 3.25; acc: 0.5
Batch: 340; loss: 4.75; acc: 0.3
Batch: 360; loss: 3.19; acc: 0.45
Batch: 380; loss: 3.14; acc: 0.33
Batch: 400; loss: 4.07; acc: 0.5
Batch: 420; loss: 4.37; acc: 0.36
Batch: 440; loss: 3.97; acc: 0.47
Batch: 460; loss: 3.99; acc: 0.33
Batch: 480; loss: 4.76; acc: 0.34
Batch: 500; loss: 5.29; acc: 0.27
Batch: 520; loss: 5.27; acc: 0.36
Batch: 540; loss: 3.38; acc: 0.45
Batch: 560; loss: 3.9; acc: 0.45
Batch: 580; loss: 3.64; acc: 0.42
Batch: 600; loss: 4.2; acc: 0.42
Batch: 620; loss: 4.76; acc: 0.33
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 4.72; acc: 0.28
Batch: 20; loss: 5.65; acc: 0.3
Batch: 40; loss: 2.61; acc: 0.48
Batch: 60; loss: 4.35; acc: 0.36
Batch: 80; loss: 5.55; acc: 0.39
Batch: 100; loss: 4.41; acc: 0.47
Batch: 120; loss: 4.06; acc: 0.42
Batch: 140; loss: 6.42; acc: 0.3
Val Epoch over. val_loss: 4.2757338247481425; val_accuracy: 0.4015724522292994 

Epoch 5 start
Batch: 0; loss: 3.92; acc: 0.48
Batch: 20; loss: 3.37; acc: 0.47
Batch: 40; loss: 4.34; acc: 0.38
Batch: 60; loss: 4.64; acc: 0.36
Batch: 80; loss: 4.46; acc: 0.38
Batch: 100; loss: 4.03; acc: 0.48
Batch: 120; loss: 4.21; acc: 0.42
Batch: 140; loss: 3.91; acc: 0.33
Batch: 160; loss: 5.01; acc: 0.31
Batch: 180; loss: 5.04; acc: 0.27
Batch: 200; loss: 4.25; acc: 0.38
Batch: 220; loss: 4.36; acc: 0.45
Batch: 240; loss: 4.34; acc: 0.25
Batch: 260; loss: 3.69; acc: 0.39
Batch: 280; loss: 4.8; acc: 0.27
Batch: 300; loss: 3.48; acc: 0.38
Batch: 320; loss: 4.55; acc: 0.3
Batch: 340; loss: 4.88; acc: 0.38
Batch: 360; loss: 4.31; acc: 0.41
Batch: 380; loss: 3.87; acc: 0.36
Batch: 400; loss: 3.88; acc: 0.33
Batch: 420; loss: 3.79; acc: 0.47
Batch: 440; loss: 4.64; acc: 0.3
Batch: 460; loss: 3.58; acc: 0.55
Batch: 480; loss: 3.92; acc: 0.44
Batch: 500; loss: 4.17; acc: 0.36
Batch: 520; loss: 4.04; acc: 0.39
Batch: 540; loss: 3.69; acc: 0.48
Batch: 560; loss: 3.76; acc: 0.38
Batch: 580; loss: 5.29; acc: 0.36
Batch: 600; loss: 3.5; acc: 0.41
Batch: 620; loss: 4.08; acc: 0.34
Train Epoch over. train_loss: 3.97; train_accuracy: 0.41 

Batch: 0; loss: 3.89; acc: 0.27
Batch: 20; loss: 5.63; acc: 0.31
Batch: 40; loss: 2.4; acc: 0.45
Batch: 60; loss: 3.81; acc: 0.31
Batch: 80; loss: 4.93; acc: 0.31
Batch: 100; loss: 3.87; acc: 0.39
Batch: 120; loss: 3.99; acc: 0.42
Batch: 140; loss: 5.31; acc: 0.34
Val Epoch over. val_loss: 3.9595293649442636; val_accuracy: 0.39798964968152867 

Epoch 6 start
Batch: 0; loss: 3.91; acc: 0.44
Batch: 20; loss: 3.43; acc: 0.39
Batch: 40; loss: 3.73; acc: 0.41
Batch: 60; loss: 3.98; acc: 0.47
Batch: 80; loss: 3.62; acc: 0.44
Batch: 100; loss: 4.62; acc: 0.3
Batch: 120; loss: 3.67; acc: 0.47
Batch: 140; loss: 4.41; acc: 0.41
Batch: 160; loss: 3.74; acc: 0.45
Batch: 180; loss: 4.37; acc: 0.34
Batch: 200; loss: 4.23; acc: 0.45
Batch: 220; loss: 5.07; acc: 0.34
Batch: 240; loss: 3.68; acc: 0.44
Batch: 260; loss: 4.6; acc: 0.33
Batch: 280; loss: 4.09; acc: 0.34
Batch: 300; loss: 2.68; acc: 0.42
Batch: 320; loss: 3.96; acc: 0.45
Batch: 340; loss: 4.52; acc: 0.39
Batch: 360; loss: 4.61; acc: 0.38
Batch: 380; loss: 4.23; acc: 0.44
Batch: 400; loss: 4.4; acc: 0.41
Batch: 420; loss: 2.9; acc: 0.55
Batch: 440; loss: 3.18; acc: 0.44
Batch: 460; loss: 3.87; acc: 0.39
Batch: 480; loss: 3.57; acc: 0.42
Batch: 500; loss: 3.38; acc: 0.41
Batch: 520; loss: 3.37; acc: 0.45
Batch: 540; loss: 3.58; acc: 0.52
Batch: 560; loss: 4.36; acc: 0.41
Batch: 580; loss: 4.33; acc: 0.36
Batch: 600; loss: 3.23; acc: 0.44
Batch: 620; loss: 3.74; acc: 0.5
Train Epoch over. train_loss: 4.0; train_accuracy: 0.41 

Batch: 0; loss: 4.23; acc: 0.27
Batch: 20; loss: 5.79; acc: 0.27
Batch: 40; loss: 2.65; acc: 0.48
Batch: 60; loss: 3.83; acc: 0.34
Batch: 80; loss: 4.94; acc: 0.31
Batch: 100; loss: 4.16; acc: 0.44
Batch: 120; loss: 4.19; acc: 0.38
Batch: 140; loss: 5.4; acc: 0.25
Val Epoch over. val_loss: 4.033034095338955; val_accuracy: 0.376890923566879 

Epoch 7 start
Batch: 0; loss: 4.78; acc: 0.38
Batch: 20; loss: 4.41; acc: 0.33
Batch: 40; loss: 4.92; acc: 0.41
Batch: 60; loss: 3.79; acc: 0.42
Batch: 80; loss: 4.16; acc: 0.39
Batch: 100; loss: 3.68; acc: 0.5
Batch: 120; loss: 4.74; acc: 0.39
Batch: 140; loss: 3.04; acc: 0.59
Batch: 160; loss: 3.64; acc: 0.33
Batch: 180; loss: 3.24; acc: 0.44
Batch: 200; loss: 4.4; acc: 0.34
Batch: 220; loss: 4.39; acc: 0.44
Batch: 240; loss: 3.78; acc: 0.39
Batch: 260; loss: 4.44; acc: 0.39
Batch: 280; loss: 4.42; acc: 0.39
Batch: 300; loss: 4.1; acc: 0.44
Batch: 320; loss: 2.9; acc: 0.52
Batch: 340; loss: 3.98; acc: 0.45
Batch: 360; loss: 4.59; acc: 0.38
Batch: 380; loss: 5.24; acc: 0.34
Batch: 400; loss: 4.2; acc: 0.44
Batch: 420; loss: 4.46; acc: 0.38
Batch: 440; loss: 4.22; acc: 0.31
Batch: 460; loss: 4.31; acc: 0.39
Batch: 480; loss: 5.41; acc: 0.38
Batch: 500; loss: 3.22; acc: 0.45
Batch: 520; loss: 3.8; acc: 0.3
Batch: 540; loss: 3.84; acc: 0.36
Batch: 560; loss: 3.1; acc: 0.48
Batch: 580; loss: 3.7; acc: 0.42
Batch: 600; loss: 4.42; acc: 0.38
Batch: 620; loss: 4.5; acc: 0.33
Train Epoch over. train_loss: 4.0; train_accuracy: 0.41 

Batch: 0; loss: 4.32; acc: 0.25
Batch: 20; loss: 6.03; acc: 0.23
Batch: 40; loss: 3.18; acc: 0.36
Batch: 60; loss: 3.58; acc: 0.39
Batch: 80; loss: 5.05; acc: 0.33
Batch: 100; loss: 4.1; acc: 0.36
Batch: 120; loss: 4.12; acc: 0.38
Batch: 140; loss: 5.02; acc: 0.34
Val Epoch over. val_loss: 4.187977298809464; val_accuracy: 0.3714171974522293 

Epoch 8 start
Batch: 0; loss: 3.3; acc: 0.47
Batch: 20; loss: 4.01; acc: 0.34
Batch: 40; loss: 4.97; acc: 0.36
Batch: 60; loss: 4.14; acc: 0.27
Batch: 80; loss: 3.8; acc: 0.39
Batch: 100; loss: 3.55; acc: 0.33
Batch: 120; loss: 3.93; acc: 0.45
Batch: 140; loss: 4.2; acc: 0.39
Batch: 160; loss: 3.86; acc: 0.38
Batch: 180; loss: 5.09; acc: 0.31
Batch: 200; loss: 3.61; acc: 0.31
Batch: 220; loss: 3.77; acc: 0.39
Batch: 240; loss: 4.01; acc: 0.44
Batch: 260; loss: 4.64; acc: 0.38
Batch: 280; loss: 3.56; acc: 0.45
Batch: 300; loss: 3.94; acc: 0.38
Batch: 320; loss: 4.73; acc: 0.38
Batch: 340; loss: 4.45; acc: 0.39
Batch: 360; loss: 3.73; acc: 0.41
Batch: 380; loss: 4.54; acc: 0.38
Batch: 400; loss: 3.15; acc: 0.45
Batch: 420; loss: 4.32; acc: 0.5
Batch: 440; loss: 3.93; acc: 0.42
Batch: 460; loss: 3.25; acc: 0.38
Batch: 480; loss: 3.72; acc: 0.5
Batch: 500; loss: 4.35; acc: 0.34
Batch: 520; loss: 2.96; acc: 0.53
Batch: 540; loss: 3.78; acc: 0.44
Batch: 560; loss: 4.0; acc: 0.39
Batch: 580; loss: 4.87; acc: 0.34
Batch: 600; loss: 2.97; acc: 0.41
Batch: 620; loss: 3.59; acc: 0.36
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 4.15; acc: 0.33
Batch: 20; loss: 5.67; acc: 0.25
Batch: 40; loss: 2.85; acc: 0.5
Batch: 60; loss: 4.06; acc: 0.36
Batch: 80; loss: 4.9; acc: 0.33
Batch: 100; loss: 4.05; acc: 0.41
Batch: 120; loss: 4.2; acc: 0.41
Batch: 140; loss: 5.38; acc: 0.33
Val Epoch over. val_loss: 4.153364526238411; val_accuracy: 0.40515525477707004 

Epoch 9 start
Batch: 0; loss: 3.25; acc: 0.39
Batch: 20; loss: 3.85; acc: 0.38
Batch: 40; loss: 4.22; acc: 0.36
Batch: 60; loss: 3.61; acc: 0.41
Batch: 80; loss: 2.91; acc: 0.59
Batch: 100; loss: 3.23; acc: 0.5
Batch: 120; loss: 4.73; acc: 0.44
Batch: 140; loss: 5.01; acc: 0.36
Batch: 160; loss: 4.39; acc: 0.34
Batch: 180; loss: 4.27; acc: 0.39
Batch: 200; loss: 3.98; acc: 0.39
Batch: 220; loss: 4.06; acc: 0.33
Batch: 240; loss: 5.74; acc: 0.31
Batch: 260; loss: 4.46; acc: 0.33
Batch: 280; loss: 4.22; acc: 0.33
Batch: 300; loss: 4.69; acc: 0.34
Batch: 320; loss: 4.83; acc: 0.25
Batch: 340; loss: 4.29; acc: 0.42
Batch: 360; loss: 5.48; acc: 0.34
Batch: 380; loss: 3.85; acc: 0.41
Batch: 400; loss: 3.21; acc: 0.45
Batch: 420; loss: 4.03; acc: 0.45
Batch: 440; loss: 2.98; acc: 0.5
Batch: 460; loss: 3.73; acc: 0.44
Batch: 480; loss: 2.55; acc: 0.56
Batch: 500; loss: 4.16; acc: 0.42
Batch: 520; loss: 3.95; acc: 0.42
Batch: 540; loss: 4.15; acc: 0.39
Batch: 560; loss: 4.0; acc: 0.44
Batch: 580; loss: 4.4; acc: 0.38
Batch: 600; loss: 4.57; acc: 0.45
Batch: 620; loss: 4.14; acc: 0.45
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 3.6; acc: 0.39
Batch: 20; loss: 5.56; acc: 0.33
Batch: 40; loss: 2.93; acc: 0.58
Batch: 60; loss: 4.01; acc: 0.36
Batch: 80; loss: 4.8; acc: 0.33
Batch: 100; loss: 4.21; acc: 0.47
Batch: 120; loss: 3.85; acc: 0.41
Batch: 140; loss: 5.59; acc: 0.27
Val Epoch over. val_loss: 4.024722365057393; val_accuracy: 0.4171974522292994 

Epoch 10 start
Batch: 0; loss: 4.18; acc: 0.39
Batch: 20; loss: 3.4; acc: 0.45
Batch: 40; loss: 3.92; acc: 0.42
Batch: 60; loss: 4.4; acc: 0.33
Batch: 80; loss: 4.06; acc: 0.48
Batch: 100; loss: 4.0; acc: 0.34
Batch: 120; loss: 3.97; acc: 0.36
Batch: 140; loss: 4.12; acc: 0.5
Batch: 160; loss: 4.11; acc: 0.41
Batch: 180; loss: 2.86; acc: 0.47
Batch: 200; loss: 4.36; acc: 0.33
Batch: 220; loss: 3.01; acc: 0.41
Batch: 240; loss: 3.47; acc: 0.55
Batch: 260; loss: 4.53; acc: 0.41
Batch: 280; loss: 3.38; acc: 0.48
Batch: 300; loss: 3.44; acc: 0.52
Batch: 320; loss: 3.09; acc: 0.41
Batch: 340; loss: 3.64; acc: 0.42
Batch: 360; loss: 4.75; acc: 0.42
Batch: 380; loss: 3.63; acc: 0.39
Batch: 400; loss: 4.02; acc: 0.48
Batch: 420; loss: 2.86; acc: 0.55
Batch: 440; loss: 3.72; acc: 0.42
Batch: 460; loss: 3.55; acc: 0.53
Batch: 480; loss: 3.78; acc: 0.39
Batch: 500; loss: 3.57; acc: 0.38
Batch: 520; loss: 3.1; acc: 0.53
Batch: 540; loss: 3.4; acc: 0.5
Batch: 560; loss: 3.08; acc: 0.5
Batch: 580; loss: 5.03; acc: 0.38
Batch: 600; loss: 3.43; acc: 0.44
Batch: 620; loss: 4.68; acc: 0.34
Train Epoch over. train_loss: 4.0; train_accuracy: 0.41 

Batch: 0; loss: 3.83; acc: 0.28
Batch: 20; loss: 5.12; acc: 0.3
Batch: 40; loss: 3.39; acc: 0.45
Batch: 60; loss: 3.85; acc: 0.44
Batch: 80; loss: 4.76; acc: 0.33
Batch: 100; loss: 4.06; acc: 0.42
Batch: 120; loss: 4.02; acc: 0.42
Batch: 140; loss: 5.05; acc: 0.33
Val Epoch over. val_loss: 4.005641624426386; val_accuracy: 0.40226910828025475 

Epoch 11 start
Batch: 0; loss: 3.58; acc: 0.48
Batch: 20; loss: 4.46; acc: 0.39
Batch: 40; loss: 3.69; acc: 0.44
Batch: 60; loss: 3.87; acc: 0.42
Batch: 80; loss: 3.62; acc: 0.38
Batch: 100; loss: 4.31; acc: 0.3
Batch: 120; loss: 4.68; acc: 0.41
Batch: 140; loss: 3.61; acc: 0.45
Batch: 160; loss: 3.1; acc: 0.47
Batch: 180; loss: 4.77; acc: 0.33
Batch: 200; loss: 3.44; acc: 0.58
Batch: 220; loss: 4.81; acc: 0.34
Batch: 240; loss: 4.05; acc: 0.45
Batch: 260; loss: 3.4; acc: 0.41
Batch: 280; loss: 3.38; acc: 0.47
Batch: 300; loss: 2.57; acc: 0.47
Batch: 320; loss: 4.16; acc: 0.36
Batch: 340; loss: 3.23; acc: 0.53
Batch: 360; loss: 4.13; acc: 0.47
Batch: 380; loss: 3.43; acc: 0.48
Batch: 400; loss: 4.66; acc: 0.36
Batch: 420; loss: 3.0; acc: 0.48
Batch: 440; loss: 2.99; acc: 0.42
Batch: 460; loss: 3.43; acc: 0.41
Batch: 480; loss: 4.79; acc: 0.34
Batch: 500; loss: 3.96; acc: 0.39
Batch: 520; loss: 3.84; acc: 0.44
Batch: 540; loss: 3.17; acc: 0.38
Batch: 560; loss: 3.68; acc: 0.42
Batch: 580; loss: 4.79; acc: 0.34
Batch: 600; loss: 4.78; acc: 0.31
Batch: 620; loss: 3.33; acc: 0.47
Train Epoch over. train_loss: 4.01; train_accuracy: 0.41 

Batch: 0; loss: 3.66; acc: 0.3
Batch: 20; loss: 5.53; acc: 0.33
Batch: 40; loss: 2.61; acc: 0.5
Batch: 60; loss: 3.7; acc: 0.36
Batch: 80; loss: 4.91; acc: 0.36
Batch: 100; loss: 4.03; acc: 0.38
Batch: 120; loss: 4.16; acc: 0.39
Batch: 140; loss: 6.15; acc: 0.28
Val Epoch over. val_loss: 4.012268415681875; val_accuracy: 0.404359076433121 

Epoch 12 start
Batch: 0; loss: 4.22; acc: 0.44
Batch: 20; loss: 3.74; acc: 0.47
Batch: 40; loss: 3.59; acc: 0.47
Batch: 60; loss: 3.8; acc: 0.42
Batch: 80; loss: 4.45; acc: 0.28
Batch: 100; loss: 4.49; acc: 0.38
Batch: 120; loss: 4.18; acc: 0.38
Batch: 140; loss: 4.72; acc: 0.31
Batch: 160; loss: 3.2; acc: 0.44
Batch: 180; loss: 3.96; acc: 0.44
Batch: 200; loss: 4.4; acc: 0.41
Batch: 220; loss: 5.11; acc: 0.38
Batch: 240; loss: 4.61; acc: 0.38
Batch: 260; loss: 5.0; acc: 0.34
Batch: 280; loss: 4.18; acc: 0.45
Batch: 300; loss: 3.43; acc: 0.44
Batch: 320; loss: 3.83; acc: 0.34
Batch: 340; loss: 3.99; acc: 0.41
Batch: 360; loss: 4.62; acc: 0.34
Batch: 380; loss: 3.88; acc: 0.36
Batch: 400; loss: 4.18; acc: 0.38
Batch: 420; loss: 5.08; acc: 0.41
Batch: 440; loss: 3.63; acc: 0.41
Batch: 460; loss: 3.12; acc: 0.45
Batch: 480; loss: 3.87; acc: 0.41
Batch: 500; loss: 3.37; acc: 0.45
Batch: 520; loss: 3.24; acc: 0.5
Batch: 540; loss: 3.42; acc: 0.44
Batch: 560; loss: 4.83; acc: 0.3
Batch: 580; loss: 4.18; acc: 0.45
Batch: 600; loss: 3.3; acc: 0.48
Batch: 620; loss: 4.68; acc: 0.31
Train Epoch over. train_loss: 4.0; train_accuracy: 0.41 

Batch: 0; loss: 3.81; acc: 0.34
Batch: 20; loss: 5.57; acc: 0.34
Batch: 40; loss: 2.9; acc: 0.48
Batch: 60; loss: 3.94; acc: 0.34
Batch: 80; loss: 4.8; acc: 0.36
Batch: 100; loss: 4.12; acc: 0.44
Batch: 120; loss: 4.22; acc: 0.45
Batch: 140; loss: 5.64; acc: 0.33
Val Epoch over. val_loss: 4.04513537959688; val_accuracy: 0.4148089171974522 

Epoch 13 start
Batch: 0; loss: 2.9; acc: 0.5
Batch: 20; loss: 2.97; acc: 0.47
Batch: 40; loss: 4.0; acc: 0.52
Batch: 60; loss: 4.39; acc: 0.41
Batch: 80; loss: 3.38; acc: 0.42
Batch: 100; loss: 4.46; acc: 0.3
Batch: 120; loss: 3.86; acc: 0.38
Batch: 140; loss: 4.14; acc: 0.38
Batch: 160; loss: 3.59; acc: 0.38
Batch: 180; loss: 4.43; acc: 0.36
Batch: 200; loss: 3.92; acc: 0.45
Batch: 220; loss: 4.15; acc: 0.41
Batch: 240; loss: 3.87; acc: 0.5
Batch: 260; loss: 3.63; acc: 0.38
Batch: 280; loss: 3.0; acc: 0.53
Batch: 300; loss: 4.37; acc: 0.27
Batch: 320; loss: 2.51; acc: 0.42
Batch: 340; loss: 3.44; acc: 0.38
Batch: 360; loss: 4.46; acc: 0.38
Batch: 380; loss: 4.55; acc: 0.38
Batch: 400; loss: 3.84; acc: 0.52
Batch: 420; loss: 3.92; acc: 0.39
Batch: 440; loss: 4.74; acc: 0.28
Batch: 460; loss: 3.86; acc: 0.41
Batch: 480; loss: 3.38; acc: 0.44
Batch: 500; loss: 4.35; acc: 0.39
Batch: 520; loss: 3.28; acc: 0.44
Batch: 540; loss: 3.94; acc: 0.47
Batch: 560; loss: 3.72; acc: 0.42
Batch: 580; loss: 2.88; acc: 0.52
Batch: 600; loss: 3.84; acc: 0.45
Batch: 620; loss: 3.27; acc: 0.44
Train Epoch over. train_loss: 4.0; train_accuracy: 0.4 

Batch: 0; loss: 4.18; acc: 0.3
Batch: 20; loss: 6.02; acc: 0.23
Batch: 40; loss: 2.98; acc: 0.45
Batch: 60; loss: 4.11; acc: 0.33
Batch: 80; loss: 5.14; acc: 0.28
Batch: 100; loss: 4.24; acc: 0.41
Batch: 120; loss: 4.2; acc: 0.42
Batch: 140; loss: 5.2; acc: 0.33
Val Epoch over. val_loss: 4.191475474910372; val_accuracy: 0.3869426751592357 

Epoch 14 start
Batch: 0; loss: 4.84; acc: 0.42
Batch: 20; loss: 3.75; acc: 0.31
Batch: 40; loss: 5.21; acc: 0.33
Batch: 60; loss: 4.99; acc: 0.31
Batch: 80; loss: 3.82; acc: 0.44
Batch: 100; loss: 3.19; acc: 0.45
Batch: 120; loss: 4.71; acc: 0.41
Batch: 140; loss: 3.55; acc: 0.42
Batch: 160; loss: 3.72; acc: 0.42
Batch: 180; loss: 3.49; acc: 0.47
Batch: 200; loss: 4.3; acc: 0.44
Batch: 220; loss: 4.14; acc: 0.42
Batch: 240; loss: 3.49; acc: 0.42
Batch: 260; loss: 4.54; acc: 0.27
Batch: 280; loss: 4.17; acc: 0.41
Batch: 300; loss: 4.46; acc: 0.31
Batch: 320; loss: 3.3; acc: 0.47
Batch: 340; loss: 3.45; acc: 0.41
Batch: 360; loss: 4.32; acc: 0.45
Batch: 380; loss: 3.5; acc: 0.48
Batch: 400; loss: 4.43; acc: 0.3
Batch: 420; loss: 3.28; acc: 0.47
Batch: 440; loss: 4.34; acc: 0.39
Batch: 460; loss: 3.4; acc: 0.44
Batch: 480; loss: 3.77; acc: 0.42
Batch: 500; loss: 4.74; acc: 0.41
Batch: 520; loss: 4.03; acc: 0.44
Batch: 540; loss: 3.73; acc: 0.36
Batch: 560; loss: 3.74; acc: 0.44
Batch: 580; loss: 3.53; acc: 0.44
Batch: 600; loss: 3.56; acc: 0.45
Batch: 620; loss: 4.51; acc: 0.41
Train Epoch over. train_loss: 3.99; train_accuracy: 0.4 

Batch: 0; loss: 3.87; acc: 0.3
Batch: 20; loss: 5.45; acc: 0.25
Batch: 40; loss: 2.57; acc: 0.53
Batch: 60; loss: 4.19; acc: 0.33
Batch: 80; loss: 4.54; acc: 0.36
Batch: 100; loss: 4.4; acc: 0.47
Batch: 120; loss: 3.92; acc: 0.41
Batch: 140; loss: 5.65; acc: 0.28
Val Epoch over. val_loss: 4.185334158551162; val_accuracy: 0.37460191082802546 

Epoch 15 start
Batch: 0; loss: 4.53; acc: 0.36
Batch: 20; loss: 4.31; acc: 0.33
Batch: 40; loss: 4.18; acc: 0.44
Batch: 60; loss: 3.08; acc: 0.45
Batch: 80; loss: 4.43; acc: 0.41
Batch: 100; loss: 4.05; acc: 0.38
Batch: 120; loss: 4.71; acc: 0.42
Batch: 140; loss: 3.96; acc: 0.48
Batch: 160; loss: 2.74; acc: 0.47
Batch: 180; loss: 3.61; acc: 0.45
Batch: 200; loss: 3.78; acc: 0.39
Batch: 220; loss: 3.32; acc: 0.53
Batch: 240; loss: 3.49; acc: 0.5
Batch: 260; loss: 3.69; acc: 0.47
Batch: 280; loss: 3.8; acc: 0.44
Batch: 300; loss: 3.98; acc: 0.33
Batch: 320; loss: 5.44; acc: 0.3
Batch: 340; loss: 3.92; acc: 0.58
Batch: 360; loss: 3.94; acc: 0.39
Batch: 380; loss: 4.38; acc: 0.34
Batch: 400; loss: 3.69; acc: 0.53
Batch: 420; loss: 4.07; acc: 0.42
Batch: 440; loss: 3.25; acc: 0.55
Batch: 460; loss: 3.7; acc: 0.44
Batch: 480; loss: 4.62; acc: 0.39
Batch: 500; loss: 4.27; acc: 0.42
Batch: 520; loss: 4.57; acc: 0.39
Batch: 540; loss: 3.85; acc: 0.44
Batch: 560; loss: 3.16; acc: 0.55
Batch: 580; loss: 2.91; acc: 0.44
Batch: 600; loss: 4.31; acc: 0.36
Batch: 620; loss: 3.64; acc: 0.38
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 4.02; acc: 0.33
Batch: 20; loss: 6.28; acc: 0.25
Batch: 40; loss: 2.64; acc: 0.48
Batch: 60; loss: 3.93; acc: 0.34
Batch: 80; loss: 4.96; acc: 0.33
Batch: 100; loss: 4.45; acc: 0.41
Batch: 120; loss: 4.19; acc: 0.44
Batch: 140; loss: 5.77; acc: 0.28
Val Epoch over. val_loss: 4.0775829257479135; val_accuracy: 0.3945063694267516 

Epoch 16 start
Batch: 0; loss: 2.96; acc: 0.5
Batch: 20; loss: 4.5; acc: 0.39
Batch: 40; loss: 4.3; acc: 0.39
Batch: 60; loss: 3.16; acc: 0.48
Batch: 80; loss: 3.02; acc: 0.44
Batch: 100; loss: 3.21; acc: 0.42
Batch: 120; loss: 3.2; acc: 0.44
Batch: 140; loss: 3.64; acc: 0.52
Batch: 160; loss: 4.37; acc: 0.3
Batch: 180; loss: 4.68; acc: 0.38
Batch: 200; loss: 3.41; acc: 0.41
Batch: 220; loss: 3.19; acc: 0.48
Batch: 240; loss: 4.39; acc: 0.3
Batch: 260; loss: 4.36; acc: 0.33
Batch: 280; loss: 4.73; acc: 0.39
Batch: 300; loss: 3.82; acc: 0.42
Batch: 320; loss: 4.03; acc: 0.41
Batch: 340; loss: 4.11; acc: 0.41
Batch: 360; loss: 3.21; acc: 0.45
Batch: 380; loss: 4.39; acc: 0.38
Batch: 400; loss: 2.86; acc: 0.48
Batch: 420; loss: 4.37; acc: 0.44
Batch: 440; loss: 3.77; acc: 0.48
Batch: 460; loss: 4.36; acc: 0.42
Batch: 480; loss: 3.1; acc: 0.39
Batch: 500; loss: 4.06; acc: 0.28
Batch: 520; loss: 3.43; acc: 0.48
Batch: 540; loss: 3.98; acc: 0.41
Batch: 560; loss: 3.28; acc: 0.48
Batch: 580; loss: 3.99; acc: 0.39
Batch: 600; loss: 3.77; acc: 0.38
Batch: 620; loss: 3.93; acc: 0.5
Train Epoch over. train_loss: 4.0; train_accuracy: 0.41 

Batch: 0; loss: 3.95; acc: 0.31
Batch: 20; loss: 5.2; acc: 0.3
Batch: 40; loss: 2.8; acc: 0.5
Batch: 60; loss: 4.15; acc: 0.36
Batch: 80; loss: 4.67; acc: 0.33
Batch: 100; loss: 4.01; acc: 0.52
Batch: 120; loss: 4.27; acc: 0.39
Batch: 140; loss: 5.26; acc: 0.34
Val Epoch over. val_loss: 3.9739712423579707; val_accuracy: 0.4143113057324841 

Epoch 17 start
Batch: 0; loss: 4.96; acc: 0.34
Batch: 20; loss: 3.55; acc: 0.42
Batch: 40; loss: 4.52; acc: 0.38
Batch: 60; loss: 5.24; acc: 0.42
Batch: 80; loss: 3.94; acc: 0.41
Batch: 100; loss: 3.66; acc: 0.38
Batch: 120; loss: 2.9; acc: 0.56
Batch: 140; loss: 4.4; acc: 0.39
Batch: 160; loss: 3.9; acc: 0.45
Batch: 180; loss: 4.29; acc: 0.39
Batch: 200; loss: 3.6; acc: 0.41
Batch: 220; loss: 4.69; acc: 0.34
Batch: 240; loss: 3.78; acc: 0.41
Batch: 260; loss: 3.8; acc: 0.52
Batch: 280; loss: 4.73; acc: 0.36
Batch: 300; loss: 3.95; acc: 0.41
Batch: 320; loss: 5.14; acc: 0.38
Batch: 340; loss: 4.53; acc: 0.38
Batch: 360; loss: 3.9; acc: 0.39
Batch: 380; loss: 3.18; acc: 0.45
Batch: 400; loss: 3.03; acc: 0.48
Batch: 420; loss: 4.11; acc: 0.33
Batch: 440; loss: 4.65; acc: 0.36
Batch: 460; loss: 3.47; acc: 0.48
Batch: 480; loss: 4.43; acc: 0.36
Batch: 500; loss: 3.14; acc: 0.38
Batch: 520; loss: 4.14; acc: 0.52
Batch: 540; loss: 4.05; acc: 0.38
Batch: 560; loss: 3.39; acc: 0.38
Batch: 580; loss: 5.14; acc: 0.33
Batch: 600; loss: 4.52; acc: 0.45
Batch: 620; loss: 4.57; acc: 0.39
Train Epoch over. train_loss: 4.01; train_accuracy: 0.41 

Batch: 0; loss: 3.94; acc: 0.31
Batch: 20; loss: 5.38; acc: 0.33
Batch: 40; loss: 2.64; acc: 0.44
Batch: 60; loss: 3.7; acc: 0.36
Batch: 80; loss: 4.96; acc: 0.38
Batch: 100; loss: 3.85; acc: 0.42
Batch: 120; loss: 4.13; acc: 0.38
Batch: 140; loss: 5.95; acc: 0.34
Val Epoch over. val_loss: 3.959486967439105; val_accuracy: 0.4121218152866242 

Epoch 18 start
Batch: 0; loss: 3.97; acc: 0.5
Batch: 20; loss: 4.86; acc: 0.28
Batch: 40; loss: 3.9; acc: 0.42
Batch: 60; loss: 4.75; acc: 0.34
Batch: 80; loss: 4.14; acc: 0.34
Batch: 100; loss: 3.32; acc: 0.42
Batch: 120; loss: 3.4; acc: 0.42
Batch: 140; loss: 4.2; acc: 0.41
Batch: 160; loss: 4.53; acc: 0.39
Batch: 180; loss: 4.06; acc: 0.44
Batch: 200; loss: 3.25; acc: 0.48
Batch: 220; loss: 3.19; acc: 0.45
Batch: 240; loss: 3.25; acc: 0.44
Batch: 260; loss: 3.59; acc: 0.45
Batch: 280; loss: 3.18; acc: 0.42
Batch: 300; loss: 4.52; acc: 0.39
Batch: 320; loss: 3.51; acc: 0.42
Batch: 340; loss: 4.11; acc: 0.41
Batch: 360; loss: 6.12; acc: 0.22
Batch: 380; loss: 3.46; acc: 0.41
Batch: 400; loss: 2.99; acc: 0.44
Batch: 420; loss: 3.32; acc: 0.55
Batch: 440; loss: 3.76; acc: 0.38
Batch: 460; loss: 3.13; acc: 0.5
Batch: 480; loss: 3.55; acc: 0.42
Batch: 500; loss: 3.77; acc: 0.36
Batch: 520; loss: 4.28; acc: 0.38
Batch: 540; loss: 4.07; acc: 0.48
Batch: 560; loss: 5.63; acc: 0.22
Batch: 580; loss: 4.64; acc: 0.39
Batch: 600; loss: 4.73; acc: 0.47
Batch: 620; loss: 3.34; acc: 0.45
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 3.63; acc: 0.33
Batch: 20; loss: 5.29; acc: 0.31
Batch: 40; loss: 2.57; acc: 0.52
Batch: 60; loss: 3.65; acc: 0.33
Batch: 80; loss: 5.1; acc: 0.36
Batch: 100; loss: 4.13; acc: 0.41
Batch: 120; loss: 4.42; acc: 0.39
Batch: 140; loss: 6.15; acc: 0.33
Val Epoch over. val_loss: 4.124182242496758; val_accuracy: 0.4091361464968153 

Epoch 19 start
Batch: 0; loss: 4.08; acc: 0.44
Batch: 20; loss: 3.32; acc: 0.48
Batch: 40; loss: 4.69; acc: 0.39
Batch: 60; loss: 4.63; acc: 0.39
Batch: 80; loss: 4.62; acc: 0.45
Batch: 100; loss: 4.23; acc: 0.45
Batch: 120; loss: 4.98; acc: 0.38
Batch: 140; loss: 3.58; acc: 0.42
Batch: 160; loss: 3.69; acc: 0.42
Batch: 180; loss: 3.42; acc: 0.44
Batch: 200; loss: 4.8; acc: 0.36
Batch: 220; loss: 4.65; acc: 0.42
Batch: 240; loss: 3.66; acc: 0.39
Batch: 260; loss: 4.39; acc: 0.33
Batch: 280; loss: 4.52; acc: 0.33
Batch: 300; loss: 4.42; acc: 0.44
Batch: 320; loss: 4.72; acc: 0.41
Batch: 340; loss: 3.99; acc: 0.5
Batch: 360; loss: 3.02; acc: 0.45
Batch: 380; loss: 4.89; acc: 0.31
Batch: 400; loss: 3.94; acc: 0.34
Batch: 420; loss: 4.33; acc: 0.3
Batch: 440; loss: 4.07; acc: 0.41
Batch: 460; loss: 4.45; acc: 0.38
Batch: 480; loss: 4.63; acc: 0.28
Batch: 500; loss: 5.04; acc: 0.3
Batch: 520; loss: 4.27; acc: 0.38
Batch: 540; loss: 4.74; acc: 0.34
Batch: 560; loss: 3.82; acc: 0.36
Batch: 580; loss: 4.18; acc: 0.38
Batch: 600; loss: 4.32; acc: 0.38
Batch: 620; loss: 3.14; acc: 0.5
Train Epoch over. train_loss: 4.0; train_accuracy: 0.41 

Batch: 0; loss: 3.8; acc: 0.28
Batch: 20; loss: 5.24; acc: 0.33
Batch: 40; loss: 2.72; acc: 0.44
Batch: 60; loss: 3.88; acc: 0.39
Batch: 80; loss: 5.05; acc: 0.33
Batch: 100; loss: 3.84; acc: 0.41
Batch: 120; loss: 4.06; acc: 0.41
Batch: 140; loss: 5.52; acc: 0.33
Val Epoch over. val_loss: 3.9309971165505186; val_accuracy: 0.39988057324840764 

Epoch 20 start
Batch: 0; loss: 2.59; acc: 0.58
Batch: 20; loss: 2.36; acc: 0.61
Batch: 40; loss: 4.26; acc: 0.38
Batch: 60; loss: 4.16; acc: 0.38
Batch: 80; loss: 3.53; acc: 0.47
Batch: 100; loss: 2.97; acc: 0.53
Batch: 120; loss: 4.96; acc: 0.3
Batch: 140; loss: 4.29; acc: 0.39
Batch: 160; loss: 3.66; acc: 0.42
Batch: 180; loss: 3.57; acc: 0.41
Batch: 200; loss: 2.89; acc: 0.45
Batch: 220; loss: 3.53; acc: 0.42
Batch: 240; loss: 3.56; acc: 0.39
Batch: 260; loss: 3.85; acc: 0.42
Batch: 280; loss: 3.4; acc: 0.42
Batch: 300; loss: 4.31; acc: 0.34
Batch: 320; loss: 2.84; acc: 0.42
Batch: 340; loss: 3.95; acc: 0.42
Batch: 360; loss: 3.66; acc: 0.41
Batch: 380; loss: 4.08; acc: 0.36
Batch: 400; loss: 4.42; acc: 0.44
Batch: 420; loss: 3.65; acc: 0.42
Batch: 440; loss: 4.02; acc: 0.48
Batch: 460; loss: 3.47; acc: 0.44
Batch: 480; loss: 4.61; acc: 0.42
Batch: 500; loss: 4.17; acc: 0.39
Batch: 520; loss: 3.93; acc: 0.39
Batch: 540; loss: 3.49; acc: 0.33
Batch: 560; loss: 3.38; acc: 0.41
Batch: 580; loss: 4.62; acc: 0.41
Batch: 600; loss: 4.21; acc: 0.38
Batch: 620; loss: 3.37; acc: 0.44
Train Epoch over. train_loss: 4.01; train_accuracy: 0.4 

Batch: 0; loss: 3.87; acc: 0.28
Batch: 20; loss: 5.2; acc: 0.27
Batch: 40; loss: 2.93; acc: 0.44
Batch: 60; loss: 3.68; acc: 0.39
Batch: 80; loss: 4.87; acc: 0.38
Batch: 100; loss: 3.78; acc: 0.48
Batch: 120; loss: 4.31; acc: 0.41
Batch: 140; loss: 5.59; acc: 0.28
Val Epoch over. val_loss: 3.9221526953824766; val_accuracy: 0.390625 

Epoch 21 start
Batch: 0; loss: 4.25; acc: 0.38
Batch: 20; loss: 4.29; acc: 0.36
Batch: 40; loss: 4.22; acc: 0.33
Batch: 60; loss: 3.41; acc: 0.42
Batch: 80; loss: 3.49; acc: 0.45
Batch: 100; loss: 4.93; acc: 0.27
Batch: 120; loss: 3.66; acc: 0.44
Batch: 140; loss: 3.26; acc: 0.48
Batch: 160; loss: 4.82; acc: 0.33
Batch: 180; loss: 3.68; acc: 0.42
Batch: 200; loss: 3.27; acc: 0.47
Batch: 220; loss: 4.14; acc: 0.34
Batch: 240; loss: 3.81; acc: 0.5
Batch: 260; loss: 3.66; acc: 0.33
Batch: 280; loss: 3.98; acc: 0.42
Batch: 300; loss: 4.68; acc: 0.34
Batch: 320; loss: 5.06; acc: 0.33
Batch: 340; loss: 3.14; acc: 0.45
Batch: 360; loss: 3.56; acc: 0.45
Batch: 380; loss: 3.84; acc: 0.52
Batch: 400; loss: 3.64; acc: 0.47
Batch: 420; loss: 4.13; acc: 0.38
Batch: 440; loss: 4.38; acc: 0.41
Batch: 460; loss: 3.98; acc: 0.45
Batch: 480; loss: 3.72; acc: 0.38
Batch: 500; loss: 3.5; acc: 0.45
Batch: 520; loss: 5.21; acc: 0.33
Batch: 540; loss: 3.28; acc: 0.41
Batch: 560; loss: 4.58; acc: 0.34
Batch: 580; loss: 5.55; acc: 0.3
Batch: 600; loss: 3.49; acc: 0.38
Batch: 620; loss: 3.5; acc: 0.45
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 3.62; acc: 0.3
Batch: 20; loss: 5.09; acc: 0.3
Batch: 40; loss: 2.59; acc: 0.5
Batch: 60; loss: 3.74; acc: 0.31
Batch: 80; loss: 5.02; acc: 0.31
Batch: 100; loss: 3.9; acc: 0.41
Batch: 120; loss: 4.33; acc: 0.44
Batch: 140; loss: 5.66; acc: 0.31
Val Epoch over. val_loss: 4.001403312014926; val_accuracy: 0.40625 

Epoch 22 start
Batch: 0; loss: 4.07; acc: 0.34
Batch: 20; loss: 4.43; acc: 0.34
Batch: 40; loss: 4.55; acc: 0.36
Batch: 60; loss: 4.76; acc: 0.39
Batch: 80; loss: 4.05; acc: 0.28
Batch: 100; loss: 3.81; acc: 0.47
Batch: 120; loss: 4.33; acc: 0.41
Batch: 140; loss: 3.28; acc: 0.5
Batch: 160; loss: 4.14; acc: 0.36
Batch: 180; loss: 3.44; acc: 0.59
Batch: 200; loss: 2.85; acc: 0.45
Batch: 220; loss: 4.86; acc: 0.33
Batch: 240; loss: 3.95; acc: 0.44
Batch: 260; loss: 4.28; acc: 0.45
Batch: 280; loss: 4.36; acc: 0.42
Batch: 300; loss: 3.1; acc: 0.47
Batch: 320; loss: 5.49; acc: 0.3
Batch: 340; loss: 4.36; acc: 0.41
Batch: 360; loss: 3.86; acc: 0.36
Batch: 380; loss: 3.99; acc: 0.41
Batch: 400; loss: 4.19; acc: 0.45
Batch: 420; loss: 3.01; acc: 0.45
Batch: 440; loss: 4.01; acc: 0.39
Batch: 460; loss: 3.73; acc: 0.47
Batch: 480; loss: 4.21; acc: 0.39
Batch: 500; loss: 4.5; acc: 0.44
Batch: 520; loss: 3.64; acc: 0.39
Batch: 540; loss: 3.1; acc: 0.47
Batch: 560; loss: 2.85; acc: 0.44
Batch: 580; loss: 3.94; acc: 0.42
Batch: 600; loss: 3.26; acc: 0.45
Batch: 620; loss: 4.31; acc: 0.33
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 4.0; acc: 0.33
Batch: 20; loss: 5.31; acc: 0.33
Batch: 40; loss: 2.84; acc: 0.48
Batch: 60; loss: 3.76; acc: 0.34
Batch: 80; loss: 5.1; acc: 0.34
Batch: 100; loss: 3.96; acc: 0.47
Batch: 120; loss: 4.32; acc: 0.41
Batch: 140; loss: 6.08; acc: 0.33
Val Epoch over. val_loss: 4.0049121379852295; val_accuracy: 0.41033041401273884 

Epoch 23 start
Batch: 0; loss: 3.38; acc: 0.47
Batch: 20; loss: 3.66; acc: 0.47
Batch: 40; loss: 4.9; acc: 0.42
Batch: 60; loss: 3.71; acc: 0.36
Batch: 80; loss: 3.18; acc: 0.5
Batch: 100; loss: 3.82; acc: 0.44
Batch: 120; loss: 3.63; acc: 0.41
Batch: 140; loss: 3.05; acc: 0.45
Batch: 160; loss: 4.01; acc: 0.42
Batch: 180; loss: 3.98; acc: 0.38
Batch: 200; loss: 3.55; acc: 0.41
Batch: 220; loss: 4.79; acc: 0.33
Batch: 240; loss: 4.16; acc: 0.3
Batch: 260; loss: 3.35; acc: 0.47
Batch: 280; loss: 4.66; acc: 0.28
Batch: 300; loss: 4.4; acc: 0.36
Batch: 320; loss: 4.17; acc: 0.44
Batch: 340; loss: 4.42; acc: 0.39
Batch: 360; loss: 4.15; acc: 0.45
Batch: 380; loss: 5.44; acc: 0.38
Batch: 400; loss: 3.65; acc: 0.45
Batch: 420; loss: 4.2; acc: 0.38
Batch: 440; loss: 4.23; acc: 0.41
Batch: 460; loss: 5.52; acc: 0.22
Batch: 480; loss: 3.97; acc: 0.34
Batch: 500; loss: 3.8; acc: 0.42
Batch: 520; loss: 3.42; acc: 0.42
Batch: 540; loss: 4.43; acc: 0.44
Batch: 560; loss: 3.34; acc: 0.47
Batch: 580; loss: 3.87; acc: 0.41
Batch: 600; loss: 4.84; acc: 0.33
Batch: 620; loss: 3.51; acc: 0.42
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 3.66; acc: 0.31
Batch: 20; loss: 4.92; acc: 0.33
Batch: 40; loss: 2.64; acc: 0.44
Batch: 60; loss: 3.74; acc: 0.28
Batch: 80; loss: 4.99; acc: 0.34
Batch: 100; loss: 4.29; acc: 0.39
Batch: 120; loss: 3.99; acc: 0.42
Batch: 140; loss: 6.04; acc: 0.28
Val Epoch over. val_loss: 4.02263149030649; val_accuracy: 0.40376194267515925 

Epoch 24 start
Batch: 0; loss: 4.53; acc: 0.28
Batch: 20; loss: 4.25; acc: 0.44
Batch: 40; loss: 3.7; acc: 0.36
Batch: 60; loss: 2.78; acc: 0.47
Batch: 80; loss: 4.46; acc: 0.39
Batch: 100; loss: 4.91; acc: 0.36
Batch: 120; loss: 3.93; acc: 0.45
Batch: 140; loss: 4.58; acc: 0.41
Batch: 160; loss: 4.4; acc: 0.27
Batch: 180; loss: 2.91; acc: 0.52
Batch: 200; loss: 4.03; acc: 0.41
Batch: 220; loss: 3.57; acc: 0.42
Batch: 240; loss: 4.75; acc: 0.33
Batch: 260; loss: 3.64; acc: 0.41
Batch: 280; loss: 3.52; acc: 0.47
Batch: 300; loss: 4.69; acc: 0.33
Batch: 320; loss: 4.12; acc: 0.41
Batch: 340; loss: 3.32; acc: 0.36
Batch: 360; loss: 3.99; acc: 0.44
Batch: 380; loss: 3.88; acc: 0.45
Batch: 400; loss: 3.3; acc: 0.34
Batch: 420; loss: 4.49; acc: 0.36
Batch: 440; loss: 4.0; acc: 0.41
Batch: 460; loss: 3.3; acc: 0.42
Batch: 480; loss: 3.67; acc: 0.41
Batch: 500; loss: 4.21; acc: 0.39
Batch: 520; loss: 3.88; acc: 0.47
Batch: 540; loss: 3.56; acc: 0.38
Batch: 560; loss: 3.69; acc: 0.41
Batch: 580; loss: 4.54; acc: 0.44
Batch: 600; loss: 4.14; acc: 0.33
Batch: 620; loss: 6.05; acc: 0.34
Train Epoch over. train_loss: 3.98; train_accuracy: 0.41 

Batch: 0; loss: 3.84; acc: 0.33
Batch: 20; loss: 5.66; acc: 0.33
Batch: 40; loss: 2.8; acc: 0.48
Batch: 60; loss: 3.81; acc: 0.34
Batch: 80; loss: 5.66; acc: 0.31
Batch: 100; loss: 3.95; acc: 0.42
Batch: 120; loss: 4.38; acc: 0.42
Batch: 140; loss: 6.02; acc: 0.28
Val Epoch over. val_loss: 4.123715007381075; val_accuracy: 0.40605095541401276 

Epoch 25 start
Batch: 0; loss: 3.17; acc: 0.41
Batch: 20; loss: 3.46; acc: 0.41
Batch: 40; loss: 3.99; acc: 0.39
Batch: 60; loss: 3.85; acc: 0.5
Batch: 80; loss: 4.53; acc: 0.28
Batch: 100; loss: 3.29; acc: 0.44
Batch: 120; loss: 3.54; acc: 0.42
Batch: 140; loss: 2.66; acc: 0.5
Batch: 160; loss: 3.65; acc: 0.41
Batch: 180; loss: 3.13; acc: 0.41
Batch: 200; loss: 4.75; acc: 0.44
Batch: 220; loss: 3.3; acc: 0.42
Batch: 240; loss: 3.79; acc: 0.42
Batch: 260; loss: 4.37; acc: 0.38
Batch: 280; loss: 4.13; acc: 0.39
Batch: 300; loss: 4.47; acc: 0.34
Batch: 320; loss: 4.39; acc: 0.45
Batch: 340; loss: 2.78; acc: 0.52
Batch: 360; loss: 3.36; acc: 0.41
Batch: 380; loss: 3.49; acc: 0.38
Batch: 400; loss: 4.46; acc: 0.5
Batch: 420; loss: 3.36; acc: 0.44
Batch: 440; loss: 4.35; acc: 0.33
Batch: 460; loss: 3.13; acc: 0.41
Batch: 480; loss: 3.09; acc: 0.45
Batch: 500; loss: 4.57; acc: 0.34
Batch: 520; loss: 4.36; acc: 0.38
Batch: 540; loss: 4.36; acc: 0.36
Batch: 560; loss: 5.96; acc: 0.27
Batch: 580; loss: 3.08; acc: 0.52
Batch: 600; loss: 3.72; acc: 0.45
Batch: 620; loss: 3.97; acc: 0.44
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 3.96; acc: 0.33
Batch: 20; loss: 5.28; acc: 0.31
Batch: 40; loss: 2.56; acc: 0.47
Batch: 60; loss: 3.66; acc: 0.41
Batch: 80; loss: 4.79; acc: 0.38
Batch: 100; loss: 4.27; acc: 0.42
Batch: 120; loss: 4.31; acc: 0.39
Batch: 140; loss: 5.07; acc: 0.3
Val Epoch over. val_loss: 4.071179221390159; val_accuracy: 0.40216958598726116 

Epoch 26 start
Batch: 0; loss: 4.06; acc: 0.41
Batch: 20; loss: 3.76; acc: 0.48
Batch: 40; loss: 4.84; acc: 0.44
Batch: 60; loss: 4.16; acc: 0.45
Batch: 80; loss: 2.82; acc: 0.42
Batch: 100; loss: 4.78; acc: 0.38
Batch: 120; loss: 3.25; acc: 0.45
Batch: 140; loss: 3.46; acc: 0.45
Batch: 160; loss: 3.56; acc: 0.5
Batch: 180; loss: 4.29; acc: 0.48
Batch: 200; loss: 4.21; acc: 0.31
Batch: 220; loss: 3.46; acc: 0.38
Batch: 240; loss: 4.82; acc: 0.3
Batch: 260; loss: 4.07; acc: 0.31
Batch: 280; loss: 3.71; acc: 0.41
Batch: 300; loss: 3.99; acc: 0.42
Batch: 320; loss: 4.72; acc: 0.28
Batch: 340; loss: 4.52; acc: 0.44
Batch: 360; loss: 3.3; acc: 0.44
Batch: 380; loss: 4.81; acc: 0.41
Batch: 400; loss: 3.75; acc: 0.44
Batch: 420; loss: 4.0; acc: 0.45
Batch: 440; loss: 3.46; acc: 0.44
Batch: 460; loss: 5.15; acc: 0.31
Batch: 480; loss: 3.44; acc: 0.62
Batch: 500; loss: 4.03; acc: 0.45
Batch: 520; loss: 4.82; acc: 0.27
Batch: 540; loss: 3.9; acc: 0.48
Batch: 560; loss: 4.24; acc: 0.42
Batch: 580; loss: 3.62; acc: 0.45
Batch: 600; loss: 4.63; acc: 0.31
Batch: 620; loss: 3.29; acc: 0.45
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 3.75; acc: 0.3
Batch: 20; loss: 5.5; acc: 0.22
Batch: 40; loss: 2.6; acc: 0.52
Batch: 60; loss: 3.8; acc: 0.28
Batch: 80; loss: 4.98; acc: 0.28
Batch: 100; loss: 3.88; acc: 0.45
Batch: 120; loss: 3.89; acc: 0.45
Batch: 140; loss: 5.61; acc: 0.3
Val Epoch over. val_loss: 4.054018755627286; val_accuracy: 0.3926154458598726 

Epoch 27 start
Batch: 0; loss: 3.85; acc: 0.39
Batch: 20; loss: 3.55; acc: 0.42
Batch: 40; loss: 3.35; acc: 0.45
Batch: 60; loss: 4.02; acc: 0.38
Batch: 80; loss: 3.85; acc: 0.42
Batch: 100; loss: 4.11; acc: 0.42
Batch: 120; loss: 4.66; acc: 0.36
Batch: 140; loss: 3.34; acc: 0.5
Batch: 160; loss: 4.64; acc: 0.41
Batch: 180; loss: 4.75; acc: 0.31
Batch: 200; loss: 5.0; acc: 0.23
Batch: 220; loss: 4.94; acc: 0.34
Batch: 240; loss: 4.37; acc: 0.44
Batch: 260; loss: 3.5; acc: 0.5
Batch: 280; loss: 3.99; acc: 0.41
Batch: 300; loss: 3.18; acc: 0.45
Batch: 320; loss: 2.99; acc: 0.44
Batch: 340; loss: 3.13; acc: 0.48
Batch: 360; loss: 5.0; acc: 0.33
Batch: 380; loss: 3.25; acc: 0.36
Batch: 400; loss: 4.69; acc: 0.27
Batch: 420; loss: 3.98; acc: 0.38
Batch: 440; loss: 3.24; acc: 0.47
Batch: 460; loss: 3.79; acc: 0.41
Batch: 480; loss: 3.07; acc: 0.5
Batch: 500; loss: 5.79; acc: 0.28
Batch: 520; loss: 4.2; acc: 0.34
Batch: 540; loss: 3.55; acc: 0.41
Batch: 560; loss: 4.06; acc: 0.45
Batch: 580; loss: 3.1; acc: 0.41
Batch: 600; loss: 3.78; acc: 0.45
Batch: 620; loss: 3.18; acc: 0.52
Train Epoch over. train_loss: 4.01; train_accuracy: 0.4 

Batch: 0; loss: 3.75; acc: 0.34
Batch: 20; loss: 5.47; acc: 0.23
Batch: 40; loss: 2.87; acc: 0.5
Batch: 60; loss: 3.79; acc: 0.36
Batch: 80; loss: 4.95; acc: 0.3
Batch: 100; loss: 4.07; acc: 0.45
Batch: 120; loss: 4.43; acc: 0.41
Batch: 140; loss: 5.39; acc: 0.31
Val Epoch over. val_loss: 4.1509826319992165; val_accuracy: 0.3943073248407643 

Epoch 28 start
Batch: 0; loss: 4.63; acc: 0.31
Batch: 20; loss: 3.62; acc: 0.42
Batch: 40; loss: 4.43; acc: 0.41
Batch: 60; loss: 4.71; acc: 0.34
Batch: 80; loss: 4.36; acc: 0.47
Batch: 100; loss: 3.85; acc: 0.41
Batch: 120; loss: 3.04; acc: 0.47
Batch: 140; loss: 3.9; acc: 0.36
Batch: 160; loss: 4.26; acc: 0.44
Batch: 180; loss: 4.54; acc: 0.34
Batch: 200; loss: 3.54; acc: 0.42
Batch: 220; loss: 3.1; acc: 0.42
Batch: 240; loss: 4.68; acc: 0.33
Batch: 260; loss: 3.43; acc: 0.47
Batch: 280; loss: 4.93; acc: 0.23
Batch: 300; loss: 4.12; acc: 0.39
Batch: 320; loss: 4.08; acc: 0.3
Batch: 340; loss: 2.7; acc: 0.53
Batch: 360; loss: 4.62; acc: 0.27
Batch: 380; loss: 4.1; acc: 0.42
Batch: 400; loss: 4.15; acc: 0.38
Batch: 420; loss: 3.83; acc: 0.44
Batch: 440; loss: 3.25; acc: 0.34
Batch: 460; loss: 4.01; acc: 0.31
Batch: 480; loss: 4.18; acc: 0.39
Batch: 500; loss: 3.64; acc: 0.38
Batch: 520; loss: 4.14; acc: 0.45
Batch: 540; loss: 3.93; acc: 0.48
Batch: 560; loss: 4.91; acc: 0.39
Batch: 580; loss: 3.13; acc: 0.44
Batch: 600; loss: 3.7; acc: 0.36
Batch: 620; loss: 4.0; acc: 0.33
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 3.71; acc: 0.33
Batch: 20; loss: 5.67; acc: 0.3
Batch: 40; loss: 3.22; acc: 0.47
Batch: 60; loss: 4.08; acc: 0.27
Batch: 80; loss: 5.01; acc: 0.31
Batch: 100; loss: 4.06; acc: 0.41
Batch: 120; loss: 4.51; acc: 0.42
Batch: 140; loss: 5.51; acc: 0.36
Val Epoch over. val_loss: 4.174622459776082; val_accuracy: 0.39958200636942676 

Epoch 29 start
Batch: 0; loss: 5.27; acc: 0.28
Batch: 20; loss: 3.14; acc: 0.44
Batch: 40; loss: 4.11; acc: 0.36
Batch: 60; loss: 4.34; acc: 0.34
Batch: 80; loss: 4.86; acc: 0.22
Batch: 100; loss: 4.07; acc: 0.47
Batch: 120; loss: 4.2; acc: 0.36
Batch: 140; loss: 4.19; acc: 0.41
Batch: 160; loss: 4.14; acc: 0.48
Batch: 180; loss: 3.79; acc: 0.39
Batch: 200; loss: 3.96; acc: 0.36
Batch: 220; loss: 3.54; acc: 0.42
Batch: 240; loss: 3.54; acc: 0.45
Batch: 260; loss: 2.99; acc: 0.38
Batch: 280; loss: 3.8; acc: 0.5
Batch: 300; loss: 5.01; acc: 0.42
Batch: 320; loss: 3.97; acc: 0.41
Batch: 340; loss: 4.32; acc: 0.39
Batch: 360; loss: 5.39; acc: 0.31
Batch: 380; loss: 2.98; acc: 0.48
Batch: 400; loss: 3.41; acc: 0.44
Batch: 420; loss: 3.39; acc: 0.42
Batch: 440; loss: 3.54; acc: 0.41
Batch: 460; loss: 4.19; acc: 0.39
Batch: 480; loss: 4.17; acc: 0.31
Batch: 500; loss: 5.38; acc: 0.25
Batch: 520; loss: 3.86; acc: 0.47
Batch: 540; loss: 4.32; acc: 0.39
Batch: 560; loss: 3.99; acc: 0.36
Batch: 580; loss: 4.48; acc: 0.38
Batch: 600; loss: 4.2; acc: 0.36
Batch: 620; loss: 3.04; acc: 0.47
Train Epoch over. train_loss: 3.99; train_accuracy: 0.41 

Batch: 0; loss: 4.01; acc: 0.34
Batch: 20; loss: 5.95; acc: 0.27
Batch: 40; loss: 2.76; acc: 0.48
Batch: 60; loss: 3.95; acc: 0.39
Batch: 80; loss: 5.01; acc: 0.33
Batch: 100; loss: 4.01; acc: 0.45
Batch: 120; loss: 4.35; acc: 0.44
Batch: 140; loss: 5.73; acc: 0.3
Val Epoch over. val_loss: 4.025018581159555; val_accuracy: 0.404359076433121 

Epoch 30 start
Batch: 0; loss: 3.81; acc: 0.5
Batch: 20; loss: 3.74; acc: 0.5
Batch: 40; loss: 3.47; acc: 0.42
Batch: 60; loss: 5.93; acc: 0.34
Batch: 80; loss: 4.25; acc: 0.42
Batch: 100; loss: 3.29; acc: 0.41
Batch: 120; loss: 3.17; acc: 0.47
Batch: 140; loss: 5.65; acc: 0.31
Batch: 160; loss: 4.35; acc: 0.33
Batch: 180; loss: 4.27; acc: 0.34
Batch: 200; loss: 3.99; acc: 0.28
Batch: 220; loss: 3.63; acc: 0.36
Batch: 240; loss: 4.56; acc: 0.39
Batch: 260; loss: 4.29; acc: 0.45
Batch: 280; loss: 3.21; acc: 0.48
Batch: 300; loss: 5.35; acc: 0.36
Batch: 320; loss: 3.95; acc: 0.34
Batch: 340; loss: 3.94; acc: 0.44
Batch: 360; loss: 4.49; acc: 0.41
Batch: 380; loss: 4.45; acc: 0.41
Batch: 400; loss: 4.6; acc: 0.39
Batch: 420; loss: 4.41; acc: 0.5
Batch: 440; loss: 3.64; acc: 0.48
Batch: 460; loss: 3.84; acc: 0.34
Batch: 480; loss: 3.57; acc: 0.33
Batch: 500; loss: 3.09; acc: 0.39
Batch: 520; loss: 3.66; acc: 0.39
Batch: 540; loss: 4.11; acc: 0.44
Batch: 560; loss: 3.91; acc: 0.44
Batch: 580; loss: 2.98; acc: 0.41
Batch: 600; loss: 4.47; acc: 0.3
Batch: 620; loss: 5.29; acc: 0.27
Train Epoch over. train_loss: 4.0; train_accuracy: 0.41 

Batch: 0; loss: 3.69; acc: 0.33
Batch: 20; loss: 5.16; acc: 0.31
Batch: 40; loss: 2.68; acc: 0.52
Batch: 60; loss: 3.82; acc: 0.34
Batch: 80; loss: 4.69; acc: 0.3
Batch: 100; loss: 4.49; acc: 0.42
Batch: 120; loss: 4.37; acc: 0.42
Batch: 140; loss: 5.54; acc: 0.28
Val Epoch over. val_loss: 4.029539310248794; val_accuracy: 0.3996815286624204 

plots/subspace_training/lenet/2020-01-10 04:26:27/d_dim_50_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 15.62; acc: 0.14
Batch: 20; loss: 4.12; acc: 0.33
Batch: 40; loss: 4.17; acc: 0.23
Batch: 60; loss: 3.0; acc: 0.38
Batch: 80; loss: 2.57; acc: 0.5
Batch: 100; loss: 2.47; acc: 0.56
Batch: 120; loss: 1.87; acc: 0.52
Batch: 140; loss: 2.52; acc: 0.52
Batch: 160; loss: 3.06; acc: 0.53
Batch: 180; loss: 2.84; acc: 0.47
Batch: 200; loss: 2.59; acc: 0.52
Batch: 220; loss: 2.07; acc: 0.56
Batch: 240; loss: 2.19; acc: 0.61
Batch: 260; loss: 3.68; acc: 0.47
Batch: 280; loss: 2.87; acc: 0.48
Batch: 300; loss: 3.19; acc: 0.52
Batch: 320; loss: 2.35; acc: 0.61
Batch: 340; loss: 3.23; acc: 0.47
Batch: 360; loss: 2.63; acc: 0.58
Batch: 380; loss: 2.74; acc: 0.55
Batch: 400; loss: 1.98; acc: 0.62
Batch: 420; loss: 2.62; acc: 0.53
Batch: 440; loss: 2.59; acc: 0.53
Batch: 460; loss: 2.8; acc: 0.58
Batch: 480; loss: 2.15; acc: 0.52
Batch: 500; loss: 2.7; acc: 0.56
Batch: 520; loss: 2.3; acc: 0.5
Batch: 540; loss: 2.61; acc: 0.61
Batch: 560; loss: 2.62; acc: 0.58
Batch: 580; loss: 3.78; acc: 0.47
Batch: 600; loss: 2.34; acc: 0.59
Batch: 620; loss: 2.12; acc: 0.64
Train Epoch over. train_loss: 2.74; train_accuracy: 0.51 

Batch: 0; loss: 2.22; acc: 0.48
Batch: 20; loss: 3.0; acc: 0.39
Batch: 40; loss: 1.93; acc: 0.64
Batch: 60; loss: 2.72; acc: 0.52
Batch: 80; loss: 2.03; acc: 0.61
Batch: 100; loss: 2.95; acc: 0.58
Batch: 120; loss: 2.24; acc: 0.58
Batch: 140; loss: 4.05; acc: 0.5
Val Epoch over. val_loss: 2.3616024255752563; val_accuracy: 0.5659832802547771 

Epoch 2 start
Batch: 0; loss: 2.08; acc: 0.62
Batch: 20; loss: 3.89; acc: 0.5
Batch: 40; loss: 1.79; acc: 0.59
Batch: 60; loss: 3.02; acc: 0.52
Batch: 80; loss: 2.2; acc: 0.56
Batch: 100; loss: 2.51; acc: 0.52
Batch: 120; loss: 3.83; acc: 0.48
Batch: 140; loss: 1.65; acc: 0.7
Batch: 160; loss: 2.71; acc: 0.55
Batch: 180; loss: 2.17; acc: 0.58
Batch: 200; loss: 2.5; acc: 0.53
Batch: 220; loss: 2.09; acc: 0.64
Batch: 240; loss: 2.33; acc: 0.61
Batch: 260; loss: 2.47; acc: 0.53
Batch: 280; loss: 2.4; acc: 0.47
Batch: 300; loss: 2.07; acc: 0.59
Batch: 320; loss: 1.4; acc: 0.73
Batch: 340; loss: 1.89; acc: 0.67
Batch: 360; loss: 1.7; acc: 0.73
Batch: 380; loss: 1.5; acc: 0.69
Batch: 400; loss: 2.86; acc: 0.61
Batch: 420; loss: 1.09; acc: 0.69
Batch: 440; loss: 3.01; acc: 0.58
Batch: 460; loss: 2.94; acc: 0.58
Batch: 480; loss: 2.68; acc: 0.53
Batch: 500; loss: 2.6; acc: 0.61
Batch: 520; loss: 3.31; acc: 0.53
Batch: 540; loss: 2.24; acc: 0.56
Batch: 560; loss: 2.18; acc: 0.58
Batch: 580; loss: 2.32; acc: 0.61
Batch: 600; loss: 2.07; acc: 0.56
Batch: 620; loss: 2.14; acc: 0.66
Train Epoch over. train_loss: 2.43; train_accuracy: 0.57 

Batch: 0; loss: 2.18; acc: 0.62
Batch: 20; loss: 3.56; acc: 0.45
Batch: 40; loss: 2.26; acc: 0.58
Batch: 60; loss: 2.81; acc: 0.53
Batch: 80; loss: 2.47; acc: 0.59
Batch: 100; loss: 2.76; acc: 0.55
Batch: 120; loss: 2.45; acc: 0.52
Batch: 140; loss: 5.09; acc: 0.42
Val Epoch over. val_loss: 2.519337638168578; val_accuracy: 0.565187101910828 

Epoch 3 start
Batch: 0; loss: 2.7; acc: 0.61
Batch: 20; loss: 2.85; acc: 0.52
Batch: 40; loss: 1.55; acc: 0.66
Batch: 60; loss: 3.23; acc: 0.48
Batch: 80; loss: 2.48; acc: 0.53
Batch: 100; loss: 2.48; acc: 0.52
Batch: 120; loss: 2.44; acc: 0.59
Batch: 140; loss: 1.89; acc: 0.67
Batch: 160; loss: 3.38; acc: 0.42
Batch: 180; loss: 1.81; acc: 0.62
Batch: 200; loss: 1.93; acc: 0.66
Batch: 220; loss: 2.36; acc: 0.55
Batch: 240; loss: 2.93; acc: 0.56
Batch: 260; loss: 2.06; acc: 0.61
Batch: 280; loss: 2.67; acc: 0.45
Batch: 300; loss: 2.91; acc: 0.53
Batch: 320; loss: 1.67; acc: 0.69
Batch: 340; loss: 3.03; acc: 0.53
Batch: 360; loss: 2.4; acc: 0.48
Batch: 380; loss: 2.29; acc: 0.58
Batch: 400; loss: 2.07; acc: 0.56
Batch: 420; loss: 1.54; acc: 0.61
Batch: 440; loss: 2.66; acc: 0.58
Batch: 460; loss: 2.68; acc: 0.52
Batch: 480; loss: 2.04; acc: 0.55
Batch: 500; loss: 1.99; acc: 0.61
Batch: 520; loss: 2.34; acc: 0.61
Batch: 540; loss: 2.51; acc: 0.56
Batch: 560; loss: 2.53; acc: 0.53
Batch: 580; loss: 2.5; acc: 0.53
Batch: 600; loss: 2.75; acc: 0.55
Batch: 620; loss: 1.84; acc: 0.67
Train Epoch over. train_loss: 2.37; train_accuracy: 0.58 

Batch: 0; loss: 2.23; acc: 0.62
Batch: 20; loss: 3.12; acc: 0.47
Batch: 40; loss: 2.11; acc: 0.58
Batch: 60; loss: 2.67; acc: 0.58
Batch: 80; loss: 1.86; acc: 0.67
Batch: 100; loss: 3.02; acc: 0.52
Batch: 120; loss: 2.4; acc: 0.53
Batch: 140; loss: 4.55; acc: 0.41
Val Epoch over. val_loss: 2.388517648551115; val_accuracy: 0.5782245222929936 

Epoch 4 start
Batch: 0; loss: 3.45; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.64
Batch: 40; loss: 1.35; acc: 0.78
Batch: 60; loss: 1.86; acc: 0.64
Batch: 80; loss: 2.47; acc: 0.47
Batch: 100; loss: 2.21; acc: 0.59
Batch: 120; loss: 3.44; acc: 0.41
Batch: 140; loss: 2.06; acc: 0.59
Batch: 160; loss: 2.04; acc: 0.66
Batch: 180; loss: 1.93; acc: 0.56
Batch: 200; loss: 2.15; acc: 0.7
Batch: 220; loss: 2.44; acc: 0.53
Batch: 240; loss: 1.83; acc: 0.61
Batch: 260; loss: 1.75; acc: 0.66
Batch: 280; loss: 2.1; acc: 0.56
Batch: 300; loss: 3.46; acc: 0.45
Batch: 320; loss: 1.62; acc: 0.52
Batch: 340; loss: 1.97; acc: 0.61
Batch: 360; loss: 2.81; acc: 0.62
Batch: 380; loss: 2.88; acc: 0.55
Batch: 400; loss: 2.25; acc: 0.58
Batch: 420; loss: 2.2; acc: 0.62
Batch: 440; loss: 2.35; acc: 0.5
Batch: 460; loss: 2.06; acc: 0.64
Batch: 480; loss: 1.65; acc: 0.59
Batch: 500; loss: 2.75; acc: 0.59
Batch: 520; loss: 2.46; acc: 0.58
Batch: 540; loss: 2.71; acc: 0.64
Batch: 560; loss: 2.62; acc: 0.61
Batch: 580; loss: 2.02; acc: 0.56
Batch: 600; loss: 1.95; acc: 0.61
Batch: 620; loss: 2.59; acc: 0.55
Train Epoch over. train_loss: 2.4; train_accuracy: 0.58 

Batch: 0; loss: 1.65; acc: 0.62
Batch: 20; loss: 3.3; acc: 0.45
Batch: 40; loss: 1.61; acc: 0.7
Batch: 60; loss: 2.2; acc: 0.61
Batch: 80; loss: 1.6; acc: 0.7
Batch: 100; loss: 2.54; acc: 0.53
Batch: 120; loss: 2.43; acc: 0.56
Batch: 140; loss: 4.52; acc: 0.41
Val Epoch over. val_loss: 2.065856304897624; val_accuracy: 0.6102707006369427 

Epoch 5 start
Batch: 0; loss: 2.29; acc: 0.52
Batch: 20; loss: 2.24; acc: 0.53
Batch: 40; loss: 2.79; acc: 0.55
Batch: 60; loss: 1.79; acc: 0.61
Batch: 80; loss: 2.12; acc: 0.58
Batch: 100; loss: 2.79; acc: 0.53
Batch: 120; loss: 2.37; acc: 0.52
Batch: 140; loss: 1.72; acc: 0.59
Batch: 160; loss: 2.71; acc: 0.62
Batch: 180; loss: 2.45; acc: 0.61
Batch: 200; loss: 2.01; acc: 0.58
Batch: 220; loss: 2.21; acc: 0.58
Batch: 240; loss: 3.06; acc: 0.45
Batch: 260; loss: 2.16; acc: 0.58
Batch: 280; loss: 2.32; acc: 0.59
Batch: 300; loss: 2.04; acc: 0.69
Batch: 320; loss: 1.55; acc: 0.67
Batch: 340; loss: 3.33; acc: 0.41
Batch: 360; loss: 2.64; acc: 0.47
Batch: 380; loss: 1.75; acc: 0.64
Batch: 400; loss: 2.51; acc: 0.62
Batch: 420; loss: 2.35; acc: 0.56
Batch: 440; loss: 1.66; acc: 0.64
Batch: 460; loss: 1.27; acc: 0.66
Batch: 480; loss: 2.18; acc: 0.59
Batch: 500; loss: 3.32; acc: 0.53
Batch: 520; loss: 2.18; acc: 0.56
Batch: 540; loss: 2.25; acc: 0.55
Batch: 560; loss: 2.95; acc: 0.53
Batch: 580; loss: 1.69; acc: 0.58
Batch: 600; loss: 1.97; acc: 0.59
Batch: 620; loss: 2.72; acc: 0.56
Train Epoch over. train_loss: 2.38; train_accuracy: 0.58 

Batch: 0; loss: 1.76; acc: 0.59
Batch: 20; loss: 3.07; acc: 0.48
Batch: 40; loss: 1.86; acc: 0.61
Batch: 60; loss: 2.76; acc: 0.55
Batch: 80; loss: 2.04; acc: 0.62
Batch: 100; loss: 2.55; acc: 0.5
Batch: 120; loss: 2.36; acc: 0.52
Batch: 140; loss: 4.42; acc: 0.45
Val Epoch over. val_loss: 2.2032889682016554; val_accuracy: 0.583797770700637 

Epoch 6 start
Batch: 0; loss: 2.14; acc: 0.7
Batch: 20; loss: 1.77; acc: 0.66
Batch: 40; loss: 2.2; acc: 0.59
Batch: 60; loss: 1.86; acc: 0.62
Batch: 80; loss: 2.01; acc: 0.55
Batch: 100; loss: 2.21; acc: 0.59
Batch: 120; loss: 1.92; acc: 0.62
Batch: 140; loss: 2.38; acc: 0.69
Batch: 160; loss: 1.6; acc: 0.66
Batch: 180; loss: 2.33; acc: 0.58
Batch: 200; loss: 2.14; acc: 0.59
Batch: 220; loss: 2.32; acc: 0.64
Batch: 240; loss: 1.84; acc: 0.61
Batch: 260; loss: 2.15; acc: 0.58
Batch: 280; loss: 2.02; acc: 0.69
Batch: 300; loss: 2.06; acc: 0.59
Batch: 320; loss: 1.89; acc: 0.62
Batch: 340; loss: 2.55; acc: 0.59
Batch: 360; loss: 2.45; acc: 0.61
Batch: 380; loss: 2.29; acc: 0.56
Batch: 400; loss: 2.38; acc: 0.61
Batch: 420; loss: 1.84; acc: 0.67
Batch: 440; loss: 2.46; acc: 0.58
Batch: 460; loss: 1.61; acc: 0.61
Batch: 480; loss: 3.27; acc: 0.58
Batch: 500; loss: 2.33; acc: 0.62
Batch: 520; loss: 2.39; acc: 0.56
Batch: 540; loss: 2.51; acc: 0.56
Batch: 560; loss: 2.69; acc: 0.58
Batch: 580; loss: 3.52; acc: 0.48
Batch: 600; loss: 1.95; acc: 0.61
Batch: 620; loss: 2.25; acc: 0.59
Train Epoch over. train_loss: 2.41; train_accuracy: 0.58 

Batch: 0; loss: 2.71; acc: 0.42
Batch: 20; loss: 3.92; acc: 0.34
Batch: 40; loss: 2.77; acc: 0.55
Batch: 60; loss: 3.01; acc: 0.53
Batch: 80; loss: 2.23; acc: 0.61
Batch: 100; loss: 2.86; acc: 0.47
Batch: 120; loss: 2.89; acc: 0.47
Batch: 140; loss: 4.46; acc: 0.41
Val Epoch over. val_loss: 2.7190564567116415; val_accuracy: 0.5262738853503185 

Epoch 7 start
Batch: 0; loss: 3.67; acc: 0.5
Batch: 20; loss: 2.62; acc: 0.58
Batch: 40; loss: 2.24; acc: 0.58
Batch: 60; loss: 2.02; acc: 0.61
Batch: 80; loss: 2.94; acc: 0.58
Batch: 100; loss: 3.06; acc: 0.55
Batch: 120; loss: 2.6; acc: 0.56
Batch: 140; loss: 2.48; acc: 0.56
Batch: 160; loss: 2.51; acc: 0.59
Batch: 180; loss: 1.86; acc: 0.61
Batch: 200; loss: 2.04; acc: 0.52
Batch: 220; loss: 4.18; acc: 0.41
Batch: 240; loss: 1.72; acc: 0.69
Batch: 260; loss: 2.4; acc: 0.56
Batch: 280; loss: 2.28; acc: 0.53
Batch: 300; loss: 2.71; acc: 0.58
Batch: 320; loss: 3.1; acc: 0.55
Batch: 340; loss: 2.74; acc: 0.56
Batch: 360; loss: 1.71; acc: 0.59
Batch: 380; loss: 1.94; acc: 0.59
Batch: 400; loss: 2.29; acc: 0.64
Batch: 420; loss: 2.36; acc: 0.58
Batch: 440; loss: 1.45; acc: 0.69
Batch: 460; loss: 2.99; acc: 0.56
Batch: 480; loss: 1.96; acc: 0.59
Batch: 500; loss: 2.71; acc: 0.56
Batch: 520; loss: 2.74; acc: 0.53
Batch: 540; loss: 1.86; acc: 0.69
Batch: 560; loss: 2.5; acc: 0.64
Batch: 580; loss: 1.51; acc: 0.64
Batch: 600; loss: 1.71; acc: 0.69
Batch: 620; loss: 2.07; acc: 0.61
Train Epoch over. train_loss: 2.38; train_accuracy: 0.58 

Batch: 0; loss: 1.93; acc: 0.66
Batch: 20; loss: 3.2; acc: 0.44
Batch: 40; loss: 2.11; acc: 0.64
Batch: 60; loss: 2.54; acc: 0.53
Batch: 80; loss: 2.24; acc: 0.55
Batch: 100; loss: 2.74; acc: 0.5
Batch: 120; loss: 2.51; acc: 0.53
Batch: 140; loss: 4.94; acc: 0.39
Val Epoch over. val_loss: 2.4969938788444375; val_accuracy: 0.5748407643312102 

Epoch 8 start
Batch: 0; loss: 2.72; acc: 0.58
Batch: 20; loss: 2.13; acc: 0.66
Batch: 40; loss: 2.41; acc: 0.64
Batch: 60; loss: 3.32; acc: 0.45
Batch: 80; loss: 2.59; acc: 0.52
Batch: 100; loss: 2.43; acc: 0.53
Batch: 120; loss: 2.64; acc: 0.55
Batch: 140; loss: 2.69; acc: 0.5
Batch: 160; loss: 1.37; acc: 0.73
Batch: 180; loss: 3.18; acc: 0.59
Batch: 200; loss: 3.2; acc: 0.42
Batch: 220; loss: 2.45; acc: 0.52
Batch: 240; loss: 1.88; acc: 0.62
Batch: 260; loss: 2.59; acc: 0.62
Batch: 280; loss: 1.88; acc: 0.55
Batch: 300; loss: 2.49; acc: 0.61
Batch: 320; loss: 2.26; acc: 0.58
Batch: 340; loss: 2.44; acc: 0.56
Batch: 360; loss: 2.15; acc: 0.64
Batch: 380; loss: 2.2; acc: 0.58
Batch: 400; loss: 2.29; acc: 0.53
Batch: 420; loss: 2.81; acc: 0.44
Batch: 440; loss: 2.34; acc: 0.56
Batch: 460; loss: 1.71; acc: 0.64
Batch: 480; loss: 2.58; acc: 0.55
Batch: 500; loss: 2.33; acc: 0.58
Batch: 520; loss: 2.41; acc: 0.61
Batch: 540; loss: 2.05; acc: 0.62
Batch: 560; loss: 2.18; acc: 0.56
Batch: 580; loss: 2.01; acc: 0.64
Batch: 600; loss: 3.13; acc: 0.53
Batch: 620; loss: 2.16; acc: 0.59
Train Epoch over. train_loss: 2.41; train_accuracy: 0.58 

Batch: 0; loss: 1.75; acc: 0.64
Batch: 20; loss: 2.81; acc: 0.53
Batch: 40; loss: 2.31; acc: 0.56
Batch: 60; loss: 2.22; acc: 0.58
Batch: 80; loss: 2.26; acc: 0.62
Batch: 100; loss: 2.5; acc: 0.59
Batch: 120; loss: 2.34; acc: 0.55
Batch: 140; loss: 4.75; acc: 0.39
Val Epoch over. val_loss: 2.2229017565964133; val_accuracy: 0.600218949044586 

Epoch 9 start
Batch: 0; loss: 2.47; acc: 0.59
Batch: 20; loss: 3.07; acc: 0.53
Batch: 40; loss: 2.64; acc: 0.52
Batch: 60; loss: 2.67; acc: 0.48
Batch: 80; loss: 2.67; acc: 0.64
Batch: 100; loss: 2.16; acc: 0.62
Batch: 120; loss: 2.68; acc: 0.61
Batch: 140; loss: 2.34; acc: 0.53
Batch: 160; loss: 2.63; acc: 0.52
Batch: 180; loss: 3.59; acc: 0.59
Batch: 200; loss: 2.75; acc: 0.55
Batch: 220; loss: 3.6; acc: 0.47
Batch: 240; loss: 3.32; acc: 0.44
Batch: 260; loss: 1.99; acc: 0.56
Batch: 280; loss: 1.64; acc: 0.61
Batch: 300; loss: 2.24; acc: 0.64
Batch: 320; loss: 1.37; acc: 0.75
Batch: 340; loss: 2.63; acc: 0.56
Batch: 360; loss: 1.94; acc: 0.64
Batch: 380; loss: 1.55; acc: 0.69
Batch: 400; loss: 2.91; acc: 0.53
Batch: 420; loss: 2.12; acc: 0.61
Batch: 440; loss: 1.43; acc: 0.7
Batch: 460; loss: 1.54; acc: 0.66
Batch: 480; loss: 2.4; acc: 0.52
Batch: 500; loss: 1.84; acc: 0.7
Batch: 520; loss: 3.37; acc: 0.58
Batch: 540; loss: 2.92; acc: 0.5
Batch: 560; loss: 2.83; acc: 0.53
Batch: 580; loss: 3.35; acc: 0.5
Batch: 600; loss: 2.8; acc: 0.61
Batch: 620; loss: 2.28; acc: 0.64
Train Epoch over. train_loss: 2.38; train_accuracy: 0.58 

Batch: 0; loss: 1.74; acc: 0.53
Batch: 20; loss: 3.29; acc: 0.47
Batch: 40; loss: 1.84; acc: 0.62
Batch: 60; loss: 2.19; acc: 0.58
Batch: 80; loss: 2.22; acc: 0.58
Batch: 100; loss: 2.68; acc: 0.48
Batch: 120; loss: 2.49; acc: 0.53
Batch: 140; loss: 4.42; acc: 0.41
Val Epoch over. val_loss: 2.171409042777529; val_accuracy: 0.5829020700636943 

Epoch 10 start
Batch: 0; loss: 2.11; acc: 0.62
Batch: 20; loss: 2.53; acc: 0.64
Batch: 40; loss: 2.3; acc: 0.53
Batch: 60; loss: 3.16; acc: 0.55
Batch: 80; loss: 2.52; acc: 0.52
Batch: 100; loss: 2.15; acc: 0.53
Batch: 120; loss: 2.21; acc: 0.56
Batch: 140; loss: 1.38; acc: 0.69
Batch: 160; loss: 1.41; acc: 0.66
Batch: 180; loss: 2.58; acc: 0.53
Batch: 200; loss: 3.16; acc: 0.45
Batch: 220; loss: 1.61; acc: 0.62
Batch: 240; loss: 2.27; acc: 0.59
Batch: 260; loss: 2.37; acc: 0.47
Batch: 280; loss: 2.76; acc: 0.48
Batch: 300; loss: 2.38; acc: 0.59
Batch: 320; loss: 2.53; acc: 0.59
Batch: 340; loss: 4.71; acc: 0.44
Batch: 360; loss: 3.01; acc: 0.53
Batch: 380; loss: 1.81; acc: 0.64
Batch: 400; loss: 2.04; acc: 0.59
Batch: 420; loss: 2.32; acc: 0.59
Batch: 440; loss: 3.17; acc: 0.44
Batch: 460; loss: 2.57; acc: 0.48
Batch: 480; loss: 2.22; acc: 0.55
Batch: 500; loss: 2.88; acc: 0.52
Batch: 520; loss: 2.5; acc: 0.55
Batch: 540; loss: 2.18; acc: 0.48
Batch: 560; loss: 2.01; acc: 0.64
Batch: 580; loss: 2.48; acc: 0.55
Batch: 600; loss: 1.81; acc: 0.62
Batch: 620; loss: 2.54; acc: 0.55
Train Epoch over. train_loss: 2.39; train_accuracy: 0.58 

Batch: 0; loss: 2.51; acc: 0.62
Batch: 20; loss: 3.67; acc: 0.44
Batch: 40; loss: 2.38; acc: 0.66
Batch: 60; loss: 3.04; acc: 0.52
Batch: 80; loss: 2.42; acc: 0.55
Batch: 100; loss: 3.25; acc: 0.53
Batch: 120; loss: 2.48; acc: 0.61
Batch: 140; loss: 5.12; acc: 0.44
Val Epoch over. val_loss: 2.936864283434145; val_accuracy: 0.5475716560509554 

Epoch 11 start
Batch: 0; loss: 2.69; acc: 0.5
Batch: 20; loss: 1.57; acc: 0.64
Batch: 40; loss: 2.76; acc: 0.56
Batch: 60; loss: 2.63; acc: 0.58
Batch: 80; loss: 2.05; acc: 0.66
Batch: 100; loss: 2.34; acc: 0.55
Batch: 120; loss: 3.18; acc: 0.56
Batch: 140; loss: 1.76; acc: 0.67
Batch: 160; loss: 2.3; acc: 0.66
Batch: 180; loss: 1.98; acc: 0.67
Batch: 200; loss: 2.89; acc: 0.52
Batch: 220; loss: 1.96; acc: 0.59
Batch: 240; loss: 2.88; acc: 0.58
Batch: 260; loss: 2.56; acc: 0.52
Batch: 280; loss: 2.96; acc: 0.61
Batch: 300; loss: 2.51; acc: 0.66
Batch: 320; loss: 2.01; acc: 0.61
Batch: 340; loss: 3.1; acc: 0.58
Batch: 360; loss: 1.33; acc: 0.72
Batch: 380; loss: 1.99; acc: 0.61
Batch: 400; loss: 1.96; acc: 0.61
Batch: 420; loss: 1.94; acc: 0.58
Batch: 440; loss: 2.55; acc: 0.47
Batch: 460; loss: 2.23; acc: 0.55
Batch: 480; loss: 2.9; acc: 0.56
Batch: 500; loss: 3.02; acc: 0.61
Batch: 520; loss: 1.9; acc: 0.67
Batch: 540; loss: 3.25; acc: 0.53
Batch: 560; loss: 2.61; acc: 0.62
Batch: 580; loss: 1.56; acc: 0.64
Batch: 600; loss: 1.34; acc: 0.66
Batch: 620; loss: 2.26; acc: 0.61
Train Epoch over. train_loss: 2.4; train_accuracy: 0.58 

Batch: 0; loss: 1.66; acc: 0.58
Batch: 20; loss: 3.38; acc: 0.52
Batch: 40; loss: 1.68; acc: 0.67
Batch: 60; loss: 2.45; acc: 0.59
Batch: 80; loss: 1.66; acc: 0.69
Batch: 100; loss: 2.63; acc: 0.48
Batch: 120; loss: 2.48; acc: 0.5
Batch: 140; loss: 4.55; acc: 0.41
Val Epoch over. val_loss: 2.1686717400884934; val_accuracy: 0.5904657643312102 

Epoch 12 start
Batch: 0; loss: 2.03; acc: 0.61
Batch: 20; loss: 2.28; acc: 0.58
Batch: 40; loss: 1.95; acc: 0.66
Batch: 60; loss: 3.96; acc: 0.53
Batch: 80; loss: 1.82; acc: 0.69
Batch: 100; loss: 2.29; acc: 0.66
Batch: 120; loss: 2.7; acc: 0.48
Batch: 140; loss: 1.55; acc: 0.61
Batch: 160; loss: 3.12; acc: 0.5
Batch: 180; loss: 2.15; acc: 0.55
Batch: 200; loss: 2.51; acc: 0.61
Batch: 220; loss: 2.53; acc: 0.5
Batch: 240; loss: 1.99; acc: 0.56
Batch: 260; loss: 1.75; acc: 0.67
Batch: 280; loss: 2.95; acc: 0.5
Batch: 300; loss: 2.45; acc: 0.58
Batch: 320; loss: 2.1; acc: 0.61
Batch: 340; loss: 1.99; acc: 0.64
Batch: 360; loss: 2.89; acc: 0.59
Batch: 380; loss: 3.93; acc: 0.45
Batch: 400; loss: 2.14; acc: 0.64
Batch: 420; loss: 2.34; acc: 0.58
Batch: 440; loss: 2.84; acc: 0.59
Batch: 460; loss: 2.38; acc: 0.53
Batch: 480; loss: 2.49; acc: 0.55
Batch: 500; loss: 2.02; acc: 0.67
Batch: 520; loss: 1.8; acc: 0.69
Batch: 540; loss: 1.54; acc: 0.66
Batch: 560; loss: 1.84; acc: 0.59
Batch: 580; loss: 2.46; acc: 0.58
Batch: 600; loss: 1.99; acc: 0.62
Batch: 620; loss: 3.25; acc: 0.48
Train Epoch over. train_loss: 2.38; train_accuracy: 0.58 

Batch: 0; loss: 2.3; acc: 0.59
Batch: 20; loss: 3.54; acc: 0.42
Batch: 40; loss: 2.15; acc: 0.59
Batch: 60; loss: 2.38; acc: 0.53
Batch: 80; loss: 2.26; acc: 0.61
Batch: 100; loss: 2.71; acc: 0.5
Batch: 120; loss: 2.62; acc: 0.58
Batch: 140; loss: 4.4; acc: 0.39
Val Epoch over. val_loss: 2.4009052750411306; val_accuracy: 0.5789211783439491 

Epoch 13 start
Batch: 0; loss: 1.16; acc: 0.64
Batch: 20; loss: 2.5; acc: 0.56
Batch: 40; loss: 2.62; acc: 0.52
Batch: 60; loss: 2.34; acc: 0.53
Batch: 80; loss: 2.1; acc: 0.58
Batch: 100; loss: 1.87; acc: 0.59
Batch: 120; loss: 1.76; acc: 0.67
Batch: 140; loss: 3.8; acc: 0.45
Batch: 160; loss: 2.54; acc: 0.61
Batch: 180; loss: 1.52; acc: 0.72
Batch: 200; loss: 2.47; acc: 0.48
Batch: 220; loss: 4.27; acc: 0.47
Batch: 240; loss: 2.56; acc: 0.52
Batch: 260; loss: 1.24; acc: 0.73
Batch: 280; loss: 2.28; acc: 0.55
Batch: 300; loss: 2.39; acc: 0.62
Batch: 320; loss: 3.02; acc: 0.58
Batch: 340; loss: 1.99; acc: 0.61
Batch: 360; loss: 2.85; acc: 0.53
Batch: 380; loss: 2.78; acc: 0.52
Batch: 400; loss: 2.85; acc: 0.53
Batch: 420; loss: 2.71; acc: 0.45
Batch: 440; loss: 2.22; acc: 0.61
Batch: 460; loss: 1.48; acc: 0.64
Batch: 480; loss: 2.6; acc: 0.56
Batch: 500; loss: 2.33; acc: 0.58
Batch: 520; loss: 2.14; acc: 0.55
Batch: 540; loss: 2.78; acc: 0.56
Batch: 560; loss: 2.6; acc: 0.56
Batch: 580; loss: 1.9; acc: 0.62
Batch: 600; loss: 2.53; acc: 0.53
Batch: 620; loss: 2.67; acc: 0.47
Train Epoch over. train_loss: 2.42; train_accuracy: 0.58 

Batch: 0; loss: 2.31; acc: 0.56
Batch: 20; loss: 3.71; acc: 0.44
Batch: 40; loss: 2.28; acc: 0.58
Batch: 60; loss: 2.12; acc: 0.59
Batch: 80; loss: 1.96; acc: 0.69
Batch: 100; loss: 2.66; acc: 0.52
Batch: 120; loss: 2.62; acc: 0.56
Batch: 140; loss: 4.45; acc: 0.41
Val Epoch over. val_loss: 2.376856882860706; val_accuracy: 0.5750398089171974 

Epoch 14 start
Batch: 0; loss: 3.02; acc: 0.44
Batch: 20; loss: 1.91; acc: 0.62
Batch: 40; loss: 1.84; acc: 0.61
Batch: 60; loss: 2.4; acc: 0.62
Batch: 80; loss: 2.56; acc: 0.59
Batch: 100; loss: 2.29; acc: 0.67
Batch: 120; loss: 2.67; acc: 0.64
Batch: 140; loss: 2.45; acc: 0.61
Batch: 160; loss: 2.67; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.67
Batch: 200; loss: 2.17; acc: 0.66
Batch: 220; loss: 2.64; acc: 0.5
Batch: 240; loss: 2.75; acc: 0.53
Batch: 260; loss: 2.66; acc: 0.58
Batch: 280; loss: 2.01; acc: 0.62
Batch: 300; loss: 2.73; acc: 0.5
Batch: 320; loss: 5.19; acc: 0.38
Batch: 340; loss: 2.64; acc: 0.58
Batch: 360; loss: 1.72; acc: 0.66
Batch: 380; loss: 2.33; acc: 0.59
Batch: 400; loss: 1.92; acc: 0.64
Batch: 420; loss: 1.97; acc: 0.59
Batch: 440; loss: 2.08; acc: 0.59
Batch: 460; loss: 1.77; acc: 0.62
Batch: 480; loss: 2.83; acc: 0.5
Batch: 500; loss: 1.7; acc: 0.69
Batch: 520; loss: 2.35; acc: 0.59
Batch: 540; loss: 2.16; acc: 0.62
Batch: 560; loss: 2.2; acc: 0.64
Batch: 580; loss: 2.06; acc: 0.59
Batch: 600; loss: 1.67; acc: 0.64
Batch: 620; loss: 2.37; acc: 0.59
Train Epoch over. train_loss: 2.36; train_accuracy: 0.58 

Batch: 0; loss: 1.82; acc: 0.62
Batch: 20; loss: 3.1; acc: 0.47
Batch: 40; loss: 1.94; acc: 0.62
Batch: 60; loss: 2.23; acc: 0.59
Batch: 80; loss: 2.04; acc: 0.61
Batch: 100; loss: 2.35; acc: 0.62
Batch: 120; loss: 2.81; acc: 0.53
Batch: 140; loss: 4.14; acc: 0.41
Val Epoch over. val_loss: 2.136881747822853; val_accuracy: 0.6067874203821656 

Epoch 15 start
Batch: 0; loss: 1.85; acc: 0.62
Batch: 20; loss: 2.38; acc: 0.55
Batch: 40; loss: 2.27; acc: 0.62
Batch: 60; loss: 2.27; acc: 0.52
Batch: 80; loss: 3.34; acc: 0.48
Batch: 100; loss: 2.02; acc: 0.55
Batch: 120; loss: 3.01; acc: 0.56
Batch: 140; loss: 3.19; acc: 0.56
Batch: 160; loss: 2.36; acc: 0.62
Batch: 180; loss: 1.75; acc: 0.62
Batch: 200; loss: 2.17; acc: 0.56
Batch: 220; loss: 1.98; acc: 0.64
Batch: 240; loss: 1.9; acc: 0.59
Batch: 260; loss: 1.89; acc: 0.66
Batch: 280; loss: 2.58; acc: 0.5
Batch: 300; loss: 2.84; acc: 0.52
Batch: 320; loss: 2.94; acc: 0.53
Batch: 340; loss: 2.34; acc: 0.53
Batch: 360; loss: 5.91; acc: 0.39
Batch: 380; loss: 4.46; acc: 0.41
Batch: 400; loss: 3.4; acc: 0.53
Batch: 420; loss: 1.92; acc: 0.62
Batch: 440; loss: 1.48; acc: 0.62
Batch: 460; loss: 2.28; acc: 0.59
Batch: 480; loss: 2.22; acc: 0.64
Batch: 500; loss: 2.07; acc: 0.56
Batch: 520; loss: 2.5; acc: 0.53
Batch: 540; loss: 2.47; acc: 0.58
Batch: 560; loss: 2.53; acc: 0.59
Batch: 580; loss: 2.91; acc: 0.56
Batch: 600; loss: 1.89; acc: 0.62
Batch: 620; loss: 1.48; acc: 0.61
Train Epoch over. train_loss: 2.4; train_accuracy: 0.58 

Batch: 0; loss: 2.17; acc: 0.53
Batch: 20; loss: 3.54; acc: 0.39
Batch: 40; loss: 1.89; acc: 0.59
Batch: 60; loss: 2.72; acc: 0.55
Batch: 80; loss: 1.74; acc: 0.64
Batch: 100; loss: 2.74; acc: 0.47
Batch: 120; loss: 2.55; acc: 0.5
Batch: 140; loss: 4.58; acc: 0.45
Val Epoch over. val_loss: 2.3063766242592196; val_accuracy: 0.5806130573248408 

Epoch 16 start
Batch: 0; loss: 2.09; acc: 0.64
Batch: 20; loss: 2.65; acc: 0.56
Batch: 40; loss: 2.15; acc: 0.58
Batch: 60; loss: 2.33; acc: 0.47
Batch: 80; loss: 2.53; acc: 0.53
Batch: 100; loss: 1.54; acc: 0.62
Batch: 120; loss: 2.11; acc: 0.56
Batch: 140; loss: 2.21; acc: 0.58
Batch: 160; loss: 2.45; acc: 0.62
Batch: 180; loss: 4.67; acc: 0.52
Batch: 200; loss: 1.83; acc: 0.58
Batch: 220; loss: 2.52; acc: 0.55
Batch: 240; loss: 2.59; acc: 0.59
Batch: 260; loss: 1.99; acc: 0.59
Batch: 280; loss: 2.83; acc: 0.56
Batch: 300; loss: 2.31; acc: 0.69
Batch: 320; loss: 1.98; acc: 0.59
Batch: 340; loss: 1.74; acc: 0.66
Batch: 360; loss: 1.77; acc: 0.66
Batch: 380; loss: 3.08; acc: 0.53
Batch: 400; loss: 2.31; acc: 0.58
Batch: 420; loss: 1.9; acc: 0.61
Batch: 440; loss: 1.96; acc: 0.58
Batch: 460; loss: 3.08; acc: 0.55
Batch: 480; loss: 2.16; acc: 0.56
Batch: 500; loss: 2.38; acc: 0.58
Batch: 520; loss: 2.15; acc: 0.58
Batch: 540; loss: 2.15; acc: 0.67
Batch: 560; loss: 3.31; acc: 0.44
Batch: 580; loss: 1.53; acc: 0.73
Batch: 600; loss: 3.5; acc: 0.42
Batch: 620; loss: 3.18; acc: 0.55
Train Epoch over. train_loss: 2.37; train_accuracy: 0.58 

Batch: 0; loss: 1.81; acc: 0.7
Batch: 20; loss: 3.34; acc: 0.45
Batch: 40; loss: 1.57; acc: 0.66
Batch: 60; loss: 2.21; acc: 0.59
Batch: 80; loss: 1.87; acc: 0.59
Batch: 100; loss: 2.32; acc: 0.55
Batch: 120; loss: 2.28; acc: 0.59
Batch: 140; loss: 4.49; acc: 0.41
Val Epoch over. val_loss: 2.1425533708493423; val_accuracy: 0.6024084394904459 

Epoch 17 start
Batch: 0; loss: 3.07; acc: 0.55
Batch: 20; loss: 3.25; acc: 0.5
Batch: 40; loss: 2.29; acc: 0.55
Batch: 60; loss: 2.24; acc: 0.56
Batch: 80; loss: 2.05; acc: 0.44
Batch: 100; loss: 2.21; acc: 0.72
Batch: 120; loss: 2.08; acc: 0.64
Batch: 140; loss: 2.9; acc: 0.52
Batch: 160; loss: 2.67; acc: 0.47
Batch: 180; loss: 2.42; acc: 0.58
Batch: 200; loss: 1.94; acc: 0.61
Batch: 220; loss: 2.09; acc: 0.69
Batch: 240; loss: 2.89; acc: 0.52
Batch: 260; loss: 1.47; acc: 0.59
Batch: 280; loss: 1.08; acc: 0.73
Batch: 300; loss: 2.15; acc: 0.56
Batch: 320; loss: 2.32; acc: 0.52
Batch: 340; loss: 1.83; acc: 0.61
Batch: 360; loss: 2.22; acc: 0.58
Batch: 380; loss: 1.49; acc: 0.7
Batch: 400; loss: 2.56; acc: 0.59
Batch: 420; loss: 1.92; acc: 0.59
Batch: 440; loss: 3.64; acc: 0.47
Batch: 460; loss: 2.77; acc: 0.59
Batch: 480; loss: 1.89; acc: 0.62
Batch: 500; loss: 2.26; acc: 0.56
Batch: 520; loss: 2.9; acc: 0.5
Batch: 540; loss: 1.6; acc: 0.62
Batch: 560; loss: 2.5; acc: 0.58
Batch: 580; loss: 3.58; acc: 0.44
Batch: 600; loss: 2.38; acc: 0.59
Batch: 620; loss: 2.05; acc: 0.61
Train Epoch over. train_loss: 2.38; train_accuracy: 0.58 

Batch: 0; loss: 2.05; acc: 0.56
Batch: 20; loss: 3.6; acc: 0.45
Batch: 40; loss: 1.79; acc: 0.69
Batch: 60; loss: 2.52; acc: 0.5
Batch: 80; loss: 2.01; acc: 0.66
Batch: 100; loss: 2.37; acc: 0.47
Batch: 120; loss: 2.36; acc: 0.58
Batch: 140; loss: 4.57; acc: 0.39
Val Epoch over. val_loss: 2.3785341211185336; val_accuracy: 0.5776273885350318 

Epoch 18 start
Batch: 0; loss: 2.39; acc: 0.59
Batch: 20; loss: 2.28; acc: 0.58
Batch: 40; loss: 2.36; acc: 0.61
Batch: 60; loss: 1.79; acc: 0.64
Batch: 80; loss: 2.1; acc: 0.58
Batch: 100; loss: 1.7; acc: 0.61
Batch: 120; loss: 2.02; acc: 0.64
Batch: 140; loss: 2.74; acc: 0.53
Batch: 160; loss: 3.08; acc: 0.56
Batch: 180; loss: 1.7; acc: 0.69
Batch: 200; loss: 2.19; acc: 0.61
Batch: 220; loss: 1.66; acc: 0.56
Batch: 240; loss: 2.5; acc: 0.62
Batch: 260; loss: 1.36; acc: 0.67
Batch: 280; loss: 2.89; acc: 0.59
Batch: 300; loss: 3.06; acc: 0.47
Batch: 320; loss: 1.62; acc: 0.62
Batch: 340; loss: 1.67; acc: 0.62
Batch: 360; loss: 1.99; acc: 0.58
Batch: 380; loss: 3.09; acc: 0.48
Batch: 400; loss: 2.78; acc: 0.45
Batch: 420; loss: 2.8; acc: 0.55
Batch: 440; loss: 3.7; acc: 0.53
Batch: 460; loss: 1.42; acc: 0.67
Batch: 480; loss: 2.28; acc: 0.62
Batch: 500; loss: 2.48; acc: 0.55
Batch: 520; loss: 2.81; acc: 0.56
Batch: 540; loss: 2.52; acc: 0.58
Batch: 560; loss: 3.07; acc: 0.48
Batch: 580; loss: 2.53; acc: 0.61
Batch: 600; loss: 1.83; acc: 0.7
Batch: 620; loss: 3.32; acc: 0.53
Train Epoch over. train_loss: 2.37; train_accuracy: 0.58 

Batch: 0; loss: 1.72; acc: 0.69
Batch: 20; loss: 2.96; acc: 0.48
Batch: 40; loss: 1.94; acc: 0.7
Batch: 60; loss: 2.39; acc: 0.55
Batch: 80; loss: 2.0; acc: 0.55
Batch: 100; loss: 2.63; acc: 0.53
Batch: 120; loss: 2.17; acc: 0.62
Batch: 140; loss: 4.17; acc: 0.42
Val Epoch over. val_loss: 2.217886499538543; val_accuracy: 0.59484474522293 

Epoch 19 start
Batch: 0; loss: 2.13; acc: 0.66
Batch: 20; loss: 1.68; acc: 0.61
Batch: 40; loss: 1.62; acc: 0.62
Batch: 60; loss: 2.43; acc: 0.56
Batch: 80; loss: 1.57; acc: 0.66
Batch: 100; loss: 2.3; acc: 0.58
Batch: 120; loss: 2.37; acc: 0.56
Batch: 140; loss: 2.31; acc: 0.55
Batch: 160; loss: 2.39; acc: 0.5
Batch: 180; loss: 2.19; acc: 0.58
Batch: 200; loss: 2.03; acc: 0.58
Batch: 220; loss: 2.29; acc: 0.64
Batch: 240; loss: 2.27; acc: 0.67
Batch: 260; loss: 2.38; acc: 0.56
Batch: 280; loss: 1.85; acc: 0.64
Batch: 300; loss: 2.03; acc: 0.58
Batch: 320; loss: 2.24; acc: 0.62
Batch: 340; loss: 2.47; acc: 0.61
Batch: 360; loss: 1.61; acc: 0.61
Batch: 380; loss: 2.45; acc: 0.58
Batch: 400; loss: 2.73; acc: 0.48
Batch: 420; loss: 1.83; acc: 0.56
Batch: 440; loss: 2.32; acc: 0.59
Batch: 460; loss: 2.35; acc: 0.64
Batch: 480; loss: 2.13; acc: 0.59
Batch: 500; loss: 2.11; acc: 0.67
Batch: 520; loss: 2.42; acc: 0.62
Batch: 540; loss: 1.81; acc: 0.59
Batch: 560; loss: 2.48; acc: 0.64
Batch: 580; loss: 1.8; acc: 0.7
Batch: 600; loss: 2.94; acc: 0.58
Batch: 620; loss: 1.73; acc: 0.61
Train Epoch over. train_loss: 2.37; train_accuracy: 0.58 

Batch: 0; loss: 1.9; acc: 0.62
Batch: 20; loss: 3.44; acc: 0.39
Batch: 40; loss: 1.81; acc: 0.61
Batch: 60; loss: 2.51; acc: 0.55
Batch: 80; loss: 1.95; acc: 0.69
Batch: 100; loss: 2.79; acc: 0.52
Batch: 120; loss: 2.58; acc: 0.55
Batch: 140; loss: 4.62; acc: 0.41
Val Epoch over. val_loss: 2.292960363588515; val_accuracy: 0.5940485668789809 

Epoch 20 start
Batch: 0; loss: 2.11; acc: 0.58
Batch: 20; loss: 3.26; acc: 0.47
Batch: 40; loss: 2.54; acc: 0.52
Batch: 60; loss: 3.56; acc: 0.5
Batch: 80; loss: 2.42; acc: 0.52
Batch: 100; loss: 2.05; acc: 0.66
Batch: 120; loss: 2.52; acc: 0.55
Batch: 140; loss: 2.45; acc: 0.58
Batch: 160; loss: 1.62; acc: 0.64
Batch: 180; loss: 1.6; acc: 0.72
Batch: 200; loss: 3.08; acc: 0.53
Batch: 220; loss: 1.97; acc: 0.64
Batch: 240; loss: 2.42; acc: 0.56
Batch: 260; loss: 0.92; acc: 0.77
Batch: 280; loss: 2.17; acc: 0.62
Batch: 300; loss: 2.49; acc: 0.52
Batch: 320; loss: 2.85; acc: 0.53
Batch: 340; loss: 2.59; acc: 0.62
Batch: 360; loss: 2.32; acc: 0.58
Batch: 380; loss: 2.61; acc: 0.41
Batch: 400; loss: 2.7; acc: 0.55
Batch: 420; loss: 3.35; acc: 0.55
Batch: 440; loss: 2.6; acc: 0.52
Batch: 460; loss: 3.15; acc: 0.52
Batch: 480; loss: 3.06; acc: 0.59
Batch: 500; loss: 2.88; acc: 0.47
Batch: 520; loss: 2.54; acc: 0.52
Batch: 540; loss: 2.04; acc: 0.58
Batch: 560; loss: 2.19; acc: 0.48
Batch: 580; loss: 1.91; acc: 0.64
Batch: 600; loss: 2.48; acc: 0.53
Batch: 620; loss: 2.3; acc: 0.48
Train Epoch over. train_loss: 2.35; train_accuracy: 0.58 

Batch: 0; loss: 1.86; acc: 0.59
Batch: 20; loss: 3.11; acc: 0.56
Batch: 40; loss: 1.63; acc: 0.67
Batch: 60; loss: 2.55; acc: 0.55
Batch: 80; loss: 1.88; acc: 0.66
Batch: 100; loss: 2.67; acc: 0.5
Batch: 120; loss: 2.24; acc: 0.55
Batch: 140; loss: 4.39; acc: 0.48
Val Epoch over. val_loss: 2.1804011084471537; val_accuracy: 0.6134554140127388 

Epoch 21 start
Batch: 0; loss: 2.14; acc: 0.59
Batch: 20; loss: 3.19; acc: 0.53
Batch: 40; loss: 2.06; acc: 0.66
Batch: 60; loss: 2.14; acc: 0.56
Batch: 80; loss: 2.35; acc: 0.58
Batch: 100; loss: 2.45; acc: 0.66
Batch: 120; loss: 2.75; acc: 0.56
Batch: 140; loss: 2.66; acc: 0.59
Batch: 160; loss: 2.34; acc: 0.58
Batch: 180; loss: 3.03; acc: 0.52
Batch: 200; loss: 2.05; acc: 0.55
Batch: 220; loss: 2.17; acc: 0.56
Batch: 240; loss: 3.31; acc: 0.55
Batch: 260; loss: 2.92; acc: 0.55
Batch: 280; loss: 1.83; acc: 0.66
Batch: 300; loss: 2.19; acc: 0.62
Batch: 320; loss: 1.95; acc: 0.58
Batch: 340; loss: 2.05; acc: 0.59
Batch: 360; loss: 1.63; acc: 0.69
Batch: 380; loss: 2.56; acc: 0.56
Batch: 400; loss: 2.24; acc: 0.66
Batch: 420; loss: 2.53; acc: 0.53
Batch: 440; loss: 1.94; acc: 0.64
Batch: 460; loss: 2.19; acc: 0.59
Batch: 480; loss: 2.17; acc: 0.62
Batch: 500; loss: 2.82; acc: 0.47
Batch: 520; loss: 2.85; acc: 0.59
Batch: 540; loss: 2.51; acc: 0.52
Batch: 560; loss: 2.63; acc: 0.62
Batch: 580; loss: 1.64; acc: 0.62
Batch: 600; loss: 2.68; acc: 0.47
Batch: 620; loss: 2.18; acc: 0.58
Train Epoch over. train_loss: 2.37; train_accuracy: 0.58 

Batch: 0; loss: 2.89; acc: 0.59
Batch: 20; loss: 4.14; acc: 0.42
Batch: 40; loss: 2.58; acc: 0.61
Batch: 60; loss: 3.38; acc: 0.53
Batch: 80; loss: 2.86; acc: 0.56
Batch: 100; loss: 3.64; acc: 0.42
Batch: 120; loss: 3.53; acc: 0.41
Batch: 140; loss: 5.35; acc: 0.38
Val Epoch over. val_loss: 3.227965364030972; val_accuracy: 0.5158240445859873 

Epoch 22 start
Batch: 0; loss: 2.62; acc: 0.47
Batch: 20; loss: 1.91; acc: 0.56
Batch: 40; loss: 1.42; acc: 0.7
Batch: 60; loss: 2.88; acc: 0.5
Batch: 80; loss: 2.24; acc: 0.61
Batch: 100; loss: 1.74; acc: 0.62
Batch: 120; loss: 2.44; acc: 0.53
Batch: 140; loss: 2.04; acc: 0.66
Batch: 160; loss: 2.7; acc: 0.55
Batch: 180; loss: 2.76; acc: 0.56
Batch: 200; loss: 2.51; acc: 0.61
Batch: 220; loss: 2.37; acc: 0.58
Batch: 240; loss: 2.29; acc: 0.62
Batch: 260; loss: 2.97; acc: 0.47
Batch: 280; loss: 1.06; acc: 0.73
Batch: 300; loss: 2.26; acc: 0.62
Batch: 320; loss: 1.8; acc: 0.56
Batch: 340; loss: 2.04; acc: 0.59
Batch: 360; loss: 2.96; acc: 0.55
Batch: 380; loss: 3.18; acc: 0.56
Batch: 400; loss: 1.66; acc: 0.72
Batch: 420; loss: 2.26; acc: 0.5
Batch: 440; loss: 1.84; acc: 0.67
Batch: 460; loss: 2.4; acc: 0.5
Batch: 480; loss: 2.42; acc: 0.52
Batch: 500; loss: 2.21; acc: 0.42
Batch: 520; loss: 2.89; acc: 0.53
Batch: 540; loss: 1.74; acc: 0.55
Batch: 560; loss: 1.63; acc: 0.66
Batch: 580; loss: 2.71; acc: 0.5
Batch: 600; loss: 1.88; acc: 0.67
Batch: 620; loss: 2.73; acc: 0.59
Train Epoch over. train_loss: 2.39; train_accuracy: 0.58 

Batch: 0; loss: 1.7; acc: 0.62
Batch: 20; loss: 3.22; acc: 0.48
Batch: 40; loss: 1.75; acc: 0.69
Batch: 60; loss: 2.03; acc: 0.52
Batch: 80; loss: 1.89; acc: 0.64
Batch: 100; loss: 2.36; acc: 0.59
Batch: 120; loss: 2.16; acc: 0.56
Batch: 140; loss: 4.34; acc: 0.41
Val Epoch over. val_loss: 2.0847974730904695; val_accuracy: 0.6111664012738853 

Epoch 23 start
Batch: 0; loss: 1.68; acc: 0.67
Batch: 20; loss: 2.83; acc: 0.56
Batch: 40; loss: 1.93; acc: 0.56
Batch: 60; loss: 1.15; acc: 0.75
Batch: 80; loss: 1.63; acc: 0.66
Batch: 100; loss: 2.27; acc: 0.61
Batch: 120; loss: 1.56; acc: 0.69
Batch: 140; loss: 1.95; acc: 0.69
Batch: 160; loss: 1.96; acc: 0.58
Batch: 180; loss: 2.62; acc: 0.47
Batch: 200; loss: 2.21; acc: 0.62
Batch: 220; loss: 2.14; acc: 0.59
Batch: 240; loss: 3.3; acc: 0.55
Batch: 260; loss: 2.43; acc: 0.56
Batch: 280; loss: 2.68; acc: 0.52
Batch: 300; loss: 2.53; acc: 0.55
Batch: 320; loss: 2.52; acc: 0.58
Batch: 340; loss: 2.48; acc: 0.61
Batch: 360; loss: 0.97; acc: 0.72
Batch: 380; loss: 1.85; acc: 0.66
Batch: 400; loss: 2.47; acc: 0.53
Batch: 420; loss: 2.24; acc: 0.52
Batch: 440; loss: 2.68; acc: 0.5
Batch: 460; loss: 2.5; acc: 0.55
Batch: 480; loss: 1.67; acc: 0.69
Batch: 500; loss: 1.92; acc: 0.62
Batch: 520; loss: 3.4; acc: 0.52
Batch: 540; loss: 2.03; acc: 0.58
Batch: 560; loss: 2.42; acc: 0.5
Batch: 580; loss: 2.07; acc: 0.64
Batch: 600; loss: 3.28; acc: 0.52
Batch: 620; loss: 1.92; acc: 0.59
Train Epoch over. train_loss: 2.38; train_accuracy: 0.58 

Batch: 0; loss: 2.22; acc: 0.61
Batch: 20; loss: 4.02; acc: 0.48
Batch: 40; loss: 2.59; acc: 0.5
Batch: 60; loss: 2.96; acc: 0.53
Batch: 80; loss: 2.7; acc: 0.59
Batch: 100; loss: 2.86; acc: 0.53
Batch: 120; loss: 2.92; acc: 0.47
Batch: 140; loss: 4.59; acc: 0.48
Val Epoch over. val_loss: 2.704692484466893; val_accuracy: 0.5558320063694268 

Epoch 24 start
Batch: 0; loss: 3.08; acc: 0.48
Batch: 20; loss: 2.34; acc: 0.56
Batch: 40; loss: 1.82; acc: 0.56
Batch: 60; loss: 2.71; acc: 0.58
Batch: 80; loss: 2.38; acc: 0.64
Batch: 100; loss: 3.14; acc: 0.47
Batch: 120; loss: 3.36; acc: 0.5
Batch: 140; loss: 1.92; acc: 0.66
Batch: 160; loss: 1.61; acc: 0.64
Batch: 180; loss: 2.23; acc: 0.67
Batch: 200; loss: 2.19; acc: 0.61
Batch: 220; loss: 1.83; acc: 0.61
Batch: 240; loss: 3.44; acc: 0.5
Batch: 260; loss: 2.97; acc: 0.47
Batch: 280; loss: 2.11; acc: 0.69
Batch: 300; loss: 2.81; acc: 0.56
Batch: 320; loss: 2.56; acc: 0.59
Batch: 340; loss: 2.38; acc: 0.52
Batch: 360; loss: 1.53; acc: 0.69
Batch: 380; loss: 2.13; acc: 0.55
Batch: 400; loss: 2.12; acc: 0.64
Batch: 420; loss: 2.47; acc: 0.47
Batch: 440; loss: 2.31; acc: 0.61
Batch: 460; loss: 1.31; acc: 0.69
Batch: 480; loss: 1.47; acc: 0.67
Batch: 500; loss: 1.96; acc: 0.58
Batch: 520; loss: 1.81; acc: 0.7
Batch: 540; loss: 3.09; acc: 0.45
Batch: 560; loss: 3.5; acc: 0.59
Batch: 580; loss: 3.54; acc: 0.52
Batch: 600; loss: 1.93; acc: 0.59
Batch: 620; loss: 1.93; acc: 0.56
Train Epoch over. train_loss: 2.37; train_accuracy: 0.58 

Batch: 0; loss: 1.88; acc: 0.62
Batch: 20; loss: 3.06; acc: 0.48
Batch: 40; loss: 1.78; acc: 0.66
Batch: 60; loss: 2.42; acc: 0.52
Batch: 80; loss: 2.1; acc: 0.55
Batch: 100; loss: 2.62; acc: 0.48
Batch: 120; loss: 2.59; acc: 0.52
Batch: 140; loss: 4.92; acc: 0.38
Val Epoch over. val_loss: 2.3522625371908688; val_accuracy: 0.5696656050955414 

Epoch 25 start
Batch: 0; loss: 2.23; acc: 0.62
Batch: 20; loss: 2.36; acc: 0.55
Batch: 40; loss: 1.81; acc: 0.62
Batch: 60; loss: 1.9; acc: 0.67
Batch: 80; loss: 2.26; acc: 0.59
Batch: 100; loss: 3.5; acc: 0.53
Batch: 120; loss: 2.64; acc: 0.55
Batch: 140; loss: 2.98; acc: 0.5
Batch: 160; loss: 2.26; acc: 0.62
Batch: 180; loss: 1.58; acc: 0.64
Batch: 200; loss: 2.3; acc: 0.56
Batch: 220; loss: 2.38; acc: 0.56
Batch: 240; loss: 2.78; acc: 0.53
Batch: 260; loss: 1.33; acc: 0.64
Batch: 280; loss: 1.35; acc: 0.58
Batch: 300; loss: 3.17; acc: 0.48
Batch: 320; loss: 2.13; acc: 0.62
Batch: 340; loss: 2.76; acc: 0.59
Batch: 360; loss: 2.91; acc: 0.52
Batch: 380; loss: 1.22; acc: 0.73
Batch: 400; loss: 2.15; acc: 0.64
Batch: 420; loss: 2.5; acc: 0.53
Batch: 440; loss: 2.28; acc: 0.55
Batch: 460; loss: 2.36; acc: 0.55
Batch: 480; loss: 2.47; acc: 0.61
Batch: 500; loss: 1.79; acc: 0.62
Batch: 520; loss: 1.65; acc: 0.66
Batch: 540; loss: 2.48; acc: 0.58
Batch: 560; loss: 1.97; acc: 0.66
Batch: 580; loss: 1.69; acc: 0.66
Batch: 600; loss: 3.37; acc: 0.53
Batch: 620; loss: 2.12; acc: 0.58
Train Epoch over. train_loss: 2.38; train_accuracy: 0.58 

Batch: 0; loss: 1.86; acc: 0.67
Batch: 20; loss: 3.36; acc: 0.47
Batch: 40; loss: 1.82; acc: 0.66
Batch: 60; loss: 2.41; acc: 0.53
Batch: 80; loss: 2.1; acc: 0.59
Batch: 100; loss: 2.59; acc: 0.59
Batch: 120; loss: 2.21; acc: 0.56
Batch: 140; loss: 4.39; acc: 0.41
Val Epoch over. val_loss: 2.2098247257007917; val_accuracy: 0.5928542993630573 

Epoch 26 start
Batch: 0; loss: 2.58; acc: 0.45
Batch: 20; loss: 3.05; acc: 0.53
Batch: 40; loss: 1.9; acc: 0.58
Batch: 60; loss: 1.98; acc: 0.59
Batch: 80; loss: 1.76; acc: 0.59
Batch: 100; loss: 2.12; acc: 0.58
Batch: 120; loss: 2.29; acc: 0.59
Batch: 140; loss: 2.29; acc: 0.58
Batch: 160; loss: 1.57; acc: 0.66
Batch: 180; loss: 3.03; acc: 0.47
Batch: 200; loss: 4.17; acc: 0.48
Batch: 220; loss: 2.34; acc: 0.62
Batch: 240; loss: 1.77; acc: 0.61
Batch: 260; loss: 3.07; acc: 0.53
Batch: 280; loss: 1.78; acc: 0.66
Batch: 300; loss: 3.15; acc: 0.52
Batch: 320; loss: 2.89; acc: 0.56
Batch: 340; loss: 3.02; acc: 0.61
Batch: 360; loss: 2.3; acc: 0.61
Batch: 380; loss: 1.88; acc: 0.53
Batch: 400; loss: 1.84; acc: 0.61
Batch: 420; loss: 2.13; acc: 0.61
Batch: 440; loss: 2.21; acc: 0.61
Batch: 460; loss: 1.71; acc: 0.66
Batch: 480; loss: 1.94; acc: 0.64
Batch: 500; loss: 1.91; acc: 0.69
Batch: 520; loss: 3.03; acc: 0.5
Batch: 540; loss: 2.84; acc: 0.61
Batch: 560; loss: 2.28; acc: 0.53
Batch: 580; loss: 1.58; acc: 0.62
Batch: 600; loss: 2.28; acc: 0.62
Batch: 620; loss: 2.65; acc: 0.55
Train Epoch over. train_loss: 2.34; train_accuracy: 0.58 

Batch: 0; loss: 1.71; acc: 0.58
Batch: 20; loss: 3.29; acc: 0.47
Batch: 40; loss: 1.95; acc: 0.58
Batch: 60; loss: 2.24; acc: 0.58
Batch: 80; loss: 2.06; acc: 0.69
Batch: 100; loss: 2.53; acc: 0.55
Batch: 120; loss: 2.31; acc: 0.53
Batch: 140; loss: 4.21; acc: 0.41
Val Epoch over. val_loss: 2.1472979191761867; val_accuracy: 0.5915605095541401 

Epoch 27 start
Batch: 0; loss: 1.32; acc: 0.64
Batch: 20; loss: 2.06; acc: 0.61
Batch: 40; loss: 2.12; acc: 0.59
Batch: 60; loss: 2.02; acc: 0.64
Batch: 80; loss: 3.52; acc: 0.5
Batch: 100; loss: 2.79; acc: 0.59
Batch: 120; loss: 2.55; acc: 0.61
Batch: 140; loss: 2.65; acc: 0.58
Batch: 160; loss: 2.45; acc: 0.62
Batch: 180; loss: 1.8; acc: 0.58
Batch: 200; loss: 1.48; acc: 0.75
Batch: 220; loss: 2.06; acc: 0.59
Batch: 240; loss: 2.12; acc: 0.56
Batch: 260; loss: 2.92; acc: 0.48
Batch: 280; loss: 1.82; acc: 0.66
Batch: 300; loss: 2.4; acc: 0.67
Batch: 320; loss: 2.26; acc: 0.56
Batch: 340; loss: 4.17; acc: 0.39
Batch: 360; loss: 2.22; acc: 0.61
Batch: 380; loss: 1.96; acc: 0.67
Batch: 400; loss: 2.67; acc: 0.53
Batch: 420; loss: 1.74; acc: 0.61
Batch: 440; loss: 2.78; acc: 0.5
Batch: 460; loss: 2.97; acc: 0.53
Batch: 480; loss: 2.8; acc: 0.47
Batch: 500; loss: 2.5; acc: 0.5
Batch: 520; loss: 1.68; acc: 0.66
Batch: 540; loss: 2.78; acc: 0.61
Batch: 560; loss: 3.38; acc: 0.42
Batch: 580; loss: 2.13; acc: 0.66
Batch: 600; loss: 2.86; acc: 0.52
Batch: 620; loss: 2.42; acc: 0.66
Train Epoch over. train_loss: 2.38; train_accuracy: 0.58 

Batch: 0; loss: 1.72; acc: 0.7
Batch: 20; loss: 3.06; acc: 0.48
Batch: 40; loss: 1.67; acc: 0.62
Batch: 60; loss: 2.32; acc: 0.58
Batch: 80; loss: 1.68; acc: 0.7
Batch: 100; loss: 2.55; acc: 0.52
Batch: 120; loss: 2.23; acc: 0.56
Batch: 140; loss: 4.39; acc: 0.45
Val Epoch over. val_loss: 2.0844801246740254; val_accuracy: 0.6103702229299363 

Epoch 28 start
Batch: 0; loss: 1.94; acc: 0.61
Batch: 20; loss: 2.25; acc: 0.66
Batch: 40; loss: 3.16; acc: 0.5
Batch: 60; loss: 1.65; acc: 0.64
Batch: 80; loss: 2.4; acc: 0.58
Batch: 100; loss: 1.94; acc: 0.61
Batch: 120; loss: 3.27; acc: 0.58
Batch: 140; loss: 1.61; acc: 0.73
Batch: 160; loss: 2.58; acc: 0.61
Batch: 180; loss: 2.38; acc: 0.58
Batch: 200; loss: 2.84; acc: 0.58
Batch: 220; loss: 3.37; acc: 0.5
Batch: 240; loss: 2.39; acc: 0.62
Batch: 260; loss: 2.11; acc: 0.58
Batch: 280; loss: 1.85; acc: 0.58
Batch: 300; loss: 1.65; acc: 0.62
Batch: 320; loss: 2.67; acc: 0.55
Batch: 340; loss: 1.78; acc: 0.64
Batch: 360; loss: 1.91; acc: 0.62
Batch: 380; loss: 2.08; acc: 0.56
Batch: 400; loss: 3.37; acc: 0.48
Batch: 420; loss: 1.99; acc: 0.59
Batch: 440; loss: 2.09; acc: 0.62
Batch: 460; loss: 2.08; acc: 0.59
Batch: 480; loss: 1.42; acc: 0.66
Batch: 500; loss: 2.1; acc: 0.64
Batch: 520; loss: 2.41; acc: 0.62
Batch: 540; loss: 3.65; acc: 0.52
Batch: 560; loss: 1.81; acc: 0.67
Batch: 580; loss: 2.53; acc: 0.56
Batch: 600; loss: 3.04; acc: 0.56
Batch: 620; loss: 3.36; acc: 0.53
Train Epoch over. train_loss: 2.43; train_accuracy: 0.58 

Batch: 0; loss: 2.22; acc: 0.59
Batch: 20; loss: 3.45; acc: 0.47
Batch: 40; loss: 2.35; acc: 0.52
Batch: 60; loss: 2.41; acc: 0.56
Batch: 80; loss: 2.68; acc: 0.56
Batch: 100; loss: 2.7; acc: 0.53
Batch: 120; loss: 2.93; acc: 0.52
Batch: 140; loss: 5.0; acc: 0.38
Val Epoch over. val_loss: 2.5853781373637497; val_accuracy: 0.5558320063694268 

Epoch 29 start
Batch: 0; loss: 2.31; acc: 0.59
Batch: 20; loss: 1.92; acc: 0.64
Batch: 40; loss: 1.83; acc: 0.56
Batch: 60; loss: 2.11; acc: 0.59
Batch: 80; loss: 2.56; acc: 0.52
Batch: 100; loss: 4.13; acc: 0.42
Batch: 120; loss: 2.66; acc: 0.5
Batch: 140; loss: 1.77; acc: 0.59
Batch: 160; loss: 2.45; acc: 0.58
Batch: 180; loss: 2.97; acc: 0.58
Batch: 200; loss: 2.94; acc: 0.53
Batch: 220; loss: 2.52; acc: 0.58
Batch: 240; loss: 2.47; acc: 0.55
Batch: 260; loss: 3.18; acc: 0.56
Batch: 280; loss: 3.29; acc: 0.47
Batch: 300; loss: 2.26; acc: 0.59
Batch: 320; loss: 2.3; acc: 0.64
Batch: 340; loss: 2.08; acc: 0.59
Batch: 360; loss: 2.43; acc: 0.62
Batch: 380; loss: 1.94; acc: 0.55
Batch: 400; loss: 1.69; acc: 0.61
Batch: 420; loss: 1.98; acc: 0.58
Batch: 440; loss: 2.41; acc: 0.55
Batch: 460; loss: 2.6; acc: 0.61
Batch: 480; loss: 2.67; acc: 0.53
Batch: 500; loss: 2.05; acc: 0.62
Batch: 520; loss: 2.59; acc: 0.61
Batch: 540; loss: 1.84; acc: 0.62
Batch: 560; loss: 2.86; acc: 0.45
Batch: 580; loss: 3.0; acc: 0.61
Batch: 600; loss: 2.07; acc: 0.59
Batch: 620; loss: 4.16; acc: 0.47
Train Epoch over. train_loss: 2.4; train_accuracy: 0.58 

Batch: 0; loss: 1.74; acc: 0.61
Batch: 20; loss: 3.28; acc: 0.45
Batch: 40; loss: 1.8; acc: 0.67
Batch: 60; loss: 2.4; acc: 0.53
Batch: 80; loss: 1.83; acc: 0.67
Batch: 100; loss: 2.47; acc: 0.55
Batch: 120; loss: 2.23; acc: 0.59
Batch: 140; loss: 4.56; acc: 0.44
Val Epoch over. val_loss: 2.3062160614949123; val_accuracy: 0.601015127388535 

Epoch 30 start
Batch: 0; loss: 2.43; acc: 0.59
Batch: 20; loss: 2.87; acc: 0.56
Batch: 40; loss: 2.9; acc: 0.62
Batch: 60; loss: 2.09; acc: 0.58
Batch: 80; loss: 2.61; acc: 0.61
Batch: 100; loss: 2.07; acc: 0.61
Batch: 120; loss: 2.95; acc: 0.48
Batch: 140; loss: 2.18; acc: 0.59
Batch: 160; loss: 4.07; acc: 0.44
Batch: 180; loss: 1.83; acc: 0.62
Batch: 200; loss: 2.26; acc: 0.5
Batch: 220; loss: 3.52; acc: 0.42
Batch: 240; loss: 2.56; acc: 0.55
Batch: 260; loss: 1.97; acc: 0.69
Batch: 280; loss: 2.62; acc: 0.47
Batch: 300; loss: 3.16; acc: 0.48
Batch: 320; loss: 3.03; acc: 0.53
Batch: 340; loss: 2.6; acc: 0.48
Batch: 360; loss: 2.64; acc: 0.55
Batch: 380; loss: 2.38; acc: 0.55
Batch: 400; loss: 2.33; acc: 0.56
Batch: 420; loss: 2.92; acc: 0.56
Batch: 440; loss: 2.41; acc: 0.61
Batch: 460; loss: 3.56; acc: 0.38
Batch: 480; loss: 2.55; acc: 0.53
Batch: 500; loss: 2.02; acc: 0.62
Batch: 520; loss: 2.42; acc: 0.56
Batch: 540; loss: 2.12; acc: 0.59
Batch: 560; loss: 3.59; acc: 0.62
Batch: 580; loss: 2.88; acc: 0.58
Batch: 600; loss: 2.5; acc: 0.56
Batch: 620; loss: 2.18; acc: 0.69
Train Epoch over. train_loss: 2.4; train_accuracy: 0.58 

Batch: 0; loss: 2.22; acc: 0.61
Batch: 20; loss: 3.32; acc: 0.5
Batch: 40; loss: 2.18; acc: 0.52
Batch: 60; loss: 2.0; acc: 0.56
Batch: 80; loss: 2.05; acc: 0.58
Batch: 100; loss: 2.45; acc: 0.56
Batch: 120; loss: 2.33; acc: 0.59
Batch: 140; loss: 4.24; acc: 0.41
Val Epoch over. val_loss: 2.2340253044845193; val_accuracy: 0.5902667197452229 

plots/subspace_training/lenet/2020-01-10 04:26:27/d_dim_100_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 14.81; acc: 0.08
Batch: 20; loss: 3.06; acc: 0.39
Batch: 40; loss: 1.6; acc: 0.53
Batch: 60; loss: 1.5; acc: 0.66
Batch: 80; loss: 1.29; acc: 0.66
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 2.2; acc: 0.59
Batch: 140; loss: 1.21; acc: 0.62
Batch: 160; loss: 1.38; acc: 0.64
Batch: 180; loss: 1.15; acc: 0.75
Batch: 200; loss: 1.56; acc: 0.58
Batch: 220; loss: 1.48; acc: 0.62
Batch: 240; loss: 1.04; acc: 0.69
Batch: 260; loss: 1.09; acc: 0.73
Batch: 280; loss: 1.04; acc: 0.69
Batch: 300; loss: 1.28; acc: 0.62
Batch: 320; loss: 1.33; acc: 0.67
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 1.06; acc: 0.73
Batch: 380; loss: 1.11; acc: 0.69
Batch: 400; loss: 1.43; acc: 0.67
Batch: 420; loss: 1.02; acc: 0.67
Batch: 440; loss: 0.85; acc: 0.7
Batch: 460; loss: 0.95; acc: 0.75
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 1.11; acc: 0.73
Batch: 520; loss: 1.32; acc: 0.61
Batch: 540; loss: 1.0; acc: 0.8
Batch: 560; loss: 1.16; acc: 0.69
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 1.01; acc: 0.78
Batch: 620; loss: 0.62; acc: 0.78
Train Epoch over. train_loss: 1.35; train_accuracy: 0.66 

Batch: 0; loss: 0.8; acc: 0.72
Batch: 20; loss: 1.68; acc: 0.59
Batch: 40; loss: 1.53; acc: 0.56
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.46; acc: 0.62
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 1.76; acc: 0.59
Val Epoch over. val_loss: 1.1895078099836969; val_accuracy: 0.7017316878980892 

Epoch 2 start
Batch: 0; loss: 1.41; acc: 0.77
Batch: 20; loss: 1.31; acc: 0.77
Batch: 40; loss: 0.73; acc: 0.78
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 1.09; acc: 0.67
Batch: 120; loss: 1.1; acc: 0.73
Batch: 140; loss: 1.19; acc: 0.75
Batch: 160; loss: 0.59; acc: 0.8
Batch: 180; loss: 0.9; acc: 0.7
Batch: 200; loss: 0.59; acc: 0.8
Batch: 220; loss: 0.63; acc: 0.77
Batch: 240; loss: 1.0; acc: 0.73
Batch: 260; loss: 0.97; acc: 0.78
Batch: 280; loss: 1.0; acc: 0.73
Batch: 300; loss: 1.18; acc: 0.64
Batch: 320; loss: 0.95; acc: 0.8
Batch: 340; loss: 0.99; acc: 0.69
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 1.13; acc: 0.73
Batch: 400; loss: 1.06; acc: 0.7
Batch: 420; loss: 0.79; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.8
Batch: 460; loss: 1.0; acc: 0.67
Batch: 480; loss: 0.84; acc: 0.77
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 1.0; acc: 0.81
Batch: 540; loss: 1.18; acc: 0.73
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.57; acc: 0.8
Batch: 600; loss: 1.12; acc: 0.73
Batch: 620; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 0.97; train_accuracy: 0.75 

Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 1.34; acc: 0.69
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 1.38; acc: 0.67
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 1.34; acc: 0.66
Val Epoch over. val_loss: 0.941976392345064; val_accuracy: 0.7455214968152867 

Epoch 3 start
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.76; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.36; acc: 0.72
Batch: 140; loss: 0.89; acc: 0.78
Batch: 160; loss: 0.9; acc: 0.78
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.97; acc: 0.77
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 1.11; acc: 0.73
Batch: 280; loss: 1.4; acc: 0.7
Batch: 300; loss: 1.38; acc: 0.67
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.8
Batch: 360; loss: 0.88; acc: 0.81
Batch: 380; loss: 0.62; acc: 0.77
Batch: 400; loss: 0.99; acc: 0.73
Batch: 420; loss: 1.05; acc: 0.75
Batch: 440; loss: 0.79; acc: 0.81
Batch: 460; loss: 0.82; acc: 0.69
Batch: 480; loss: 0.87; acc: 0.72
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 1.1; acc: 0.75
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.98; acc: 0.7
Batch: 600; loss: 1.28; acc: 0.72
Batch: 620; loss: 0.9; acc: 0.78
Train Epoch over. train_loss: 0.91; train_accuracy: 0.76 

Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 1.31; acc: 0.69
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 1.57; acc: 0.56
Val Epoch over. val_loss: 0.9110817127167039; val_accuracy: 0.7559713375796179 

Epoch 4 start
Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 1.15; acc: 0.73
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.96; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.78
Batch: 140; loss: 0.79; acc: 0.75
Batch: 160; loss: 1.0; acc: 0.67
Batch: 180; loss: 0.91; acc: 0.75
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.79; acc: 0.77
Batch: 260; loss: 1.14; acc: 0.69
Batch: 280; loss: 0.56; acc: 0.81
Batch: 300; loss: 0.63; acc: 0.77
Batch: 320; loss: 0.81; acc: 0.75
Batch: 340; loss: 0.71; acc: 0.8
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.84; acc: 0.75
Batch: 400; loss: 0.85; acc: 0.83
Batch: 420; loss: 0.8; acc: 0.78
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 1.11; acc: 0.73
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.9; acc: 0.78
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 0.8; acc: 0.78
Batch: 600; loss: 0.88; acc: 0.78
Batch: 620; loss: 0.66; acc: 0.77
Train Epoch over. train_loss: 0.85; train_accuracy: 0.78 

Batch: 0; loss: 0.74; acc: 0.72
Batch: 20; loss: 1.72; acc: 0.62
Batch: 40; loss: 0.85; acc: 0.7
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.88; acc: 0.8
Batch: 100; loss: 1.16; acc: 0.69
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 1.22; acc: 0.67
Val Epoch over. val_loss: 0.8722826151331519; val_accuracy: 0.7727906050955414 

Epoch 5 start
Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 1.27; acc: 0.72
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.99; acc: 0.75
Batch: 160; loss: 1.3; acc: 0.78
Batch: 180; loss: 1.16; acc: 0.73
Batch: 200; loss: 0.58; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.86
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 1.08; acc: 0.77
Batch: 280; loss: 0.61; acc: 0.75
Batch: 300; loss: 0.85; acc: 0.78
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.91; acc: 0.73
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.81; acc: 0.78
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 1.04; acc: 0.73
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.84
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 0.76; acc: 0.88
Batch: 540; loss: 0.85; acc: 0.77
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.91; acc: 0.78
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.85; train_accuracy: 0.78 

Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 1.48; acc: 0.61
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 1.3; acc: 0.73
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 1.32; acc: 0.62
Val Epoch over. val_loss: 0.8239535450176069; val_accuracy: 0.7883160828025477 

Epoch 6 start
Batch: 0; loss: 1.1; acc: 0.78
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 1.19; acc: 0.69
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 1.63; acc: 0.64
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.93; acc: 0.8
Batch: 160; loss: 1.15; acc: 0.78
Batch: 180; loss: 0.81; acc: 0.75
Batch: 200; loss: 0.97; acc: 0.72
Batch: 220; loss: 1.27; acc: 0.7
Batch: 240; loss: 0.94; acc: 0.73
Batch: 260; loss: 0.79; acc: 0.75
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 1.06; acc: 0.78
Batch: 340; loss: 0.89; acc: 0.78
Batch: 360; loss: 0.72; acc: 0.8
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 1.15; acc: 0.73
Batch: 420; loss: 0.81; acc: 0.78
Batch: 440; loss: 1.07; acc: 0.66
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.73; acc: 0.75
Batch: 500; loss: 0.67; acc: 0.83
Batch: 520; loss: 1.05; acc: 0.69
Batch: 540; loss: 1.01; acc: 0.77
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 1.0; acc: 0.7
Batch: 600; loss: 1.3; acc: 0.72
Batch: 620; loss: 1.07; acc: 0.73
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 1.99; acc: 0.66
Batch: 20; loss: 2.65; acc: 0.58
Batch: 40; loss: 2.42; acc: 0.64
Batch: 60; loss: 1.35; acc: 0.75
Batch: 80; loss: 1.32; acc: 0.77
Batch: 100; loss: 2.64; acc: 0.61
Batch: 120; loss: 1.36; acc: 0.73
Batch: 140; loss: 2.21; acc: 0.58
Val Epoch over. val_loss: 1.7354740864911657; val_accuracy: 0.6765525477707006 

Epoch 7 start
Batch: 0; loss: 1.88; acc: 0.66
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 1.33; acc: 0.72
Batch: 60; loss: 0.96; acc: 0.73
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 0.83; acc: 0.77
Batch: 200; loss: 0.81; acc: 0.78
Batch: 220; loss: 1.05; acc: 0.8
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 1.5; acc: 0.62
Batch: 280; loss: 0.84; acc: 0.81
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 1.07; acc: 0.8
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 0.97; acc: 0.8
Batch: 440; loss: 0.78; acc: 0.84
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.73; acc: 0.8
Batch: 500; loss: 0.99; acc: 0.73
Batch: 520; loss: 0.59; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 1.18; acc: 0.72
Batch: 580; loss: 0.7; acc: 0.78
Batch: 600; loss: 1.05; acc: 0.77
Batch: 620; loss: 0.99; acc: 0.78
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 1.47; acc: 0.62
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 1.23; acc: 0.69
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 1.66; acc: 0.59
Val Epoch over. val_loss: 0.8331822249919746; val_accuracy: 0.7825437898089171 

Epoch 8 start
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 1.03; acc: 0.77
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 1.11; acc: 0.72
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.91; acc: 0.73
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.73
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 1.06; acc: 0.73
Batch: 240; loss: 0.9; acc: 0.72
Batch: 260; loss: 0.86; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 1.14; acc: 0.78
Batch: 340; loss: 0.94; acc: 0.78
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 1.14; acc: 0.7
Batch: 400; loss: 0.91; acc: 0.77
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.83; acc: 0.75
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.84
Batch: 520; loss: 0.87; acc: 0.77
Batch: 540; loss: 1.55; acc: 0.7
Batch: 560; loss: 0.92; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.75
Batch: 600; loss: 0.91; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.78
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 1.42; acc: 0.67
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.81; acc: 0.81
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 1.26; acc: 0.69
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 1.42; acc: 0.62
Val Epoch over. val_loss: 0.8550631580459085; val_accuracy: 0.7705015923566879 

Epoch 9 start
Batch: 0; loss: 0.83; acc: 0.81
Batch: 20; loss: 0.76; acc: 0.83
Batch: 40; loss: 1.12; acc: 0.7
Batch: 60; loss: 0.94; acc: 0.77
Batch: 80; loss: 1.01; acc: 0.75
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.98; acc: 0.75
Batch: 160; loss: 0.9; acc: 0.73
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.78; acc: 0.77
Batch: 240; loss: 0.58; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.82; acc: 0.83
Batch: 300; loss: 1.29; acc: 0.72
Batch: 320; loss: 0.77; acc: 0.75
Batch: 340; loss: 1.25; acc: 0.75
Batch: 360; loss: 0.84; acc: 0.8
Batch: 380; loss: 1.52; acc: 0.78
Batch: 400; loss: 1.09; acc: 0.77
Batch: 420; loss: 1.06; acc: 0.8
Batch: 440; loss: 0.79; acc: 0.77
Batch: 460; loss: 0.93; acc: 0.73
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.87; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.78
Batch: 540; loss: 0.82; acc: 0.77
Batch: 560; loss: 1.28; acc: 0.66
Batch: 580; loss: 0.46; acc: 0.75
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 1.49; acc: 0.69
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 1.48; acc: 0.7
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.98; acc: 0.77
Batch: 100; loss: 1.14; acc: 0.69
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 1.47; acc: 0.62
Val Epoch over. val_loss: 0.8637572720551946; val_accuracy: 0.7731886942675159 

Epoch 10 start
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.78; acc: 0.72
Batch: 60; loss: 0.79; acc: 0.81
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.89; acc: 0.77
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.91; acc: 0.81
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 1.19; acc: 0.77
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.81; acc: 0.8
Batch: 260; loss: 0.75; acc: 0.77
Batch: 280; loss: 0.8; acc: 0.78
Batch: 300; loss: 0.91; acc: 0.78
Batch: 320; loss: 0.78; acc: 0.83
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 1.04; acc: 0.78
Batch: 420; loss: 0.97; acc: 0.77
Batch: 440; loss: 0.83; acc: 0.77
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 1.12; acc: 0.69
Batch: 520; loss: 0.83; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 1.11; acc: 0.72
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.65; acc: 0.8
Batch: 620; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.84; train_accuracy: 0.79 

Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 1.35; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.75; acc: 0.83
Batch: 100; loss: 1.2; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 1.46; acc: 0.64
Val Epoch over. val_loss: 0.8125356458554602; val_accuracy: 0.7935907643312102 

Epoch 11 start
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 1.4; acc: 0.75
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.97; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.77
Batch: 140; loss: 0.98; acc: 0.73
Batch: 160; loss: 0.79; acc: 0.83
Batch: 180; loss: 0.96; acc: 0.7
Batch: 200; loss: 0.91; acc: 0.73
Batch: 220; loss: 0.97; acc: 0.75
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 1.25; acc: 0.72
Batch: 280; loss: 1.16; acc: 0.7
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.57; acc: 0.77
Batch: 340; loss: 1.23; acc: 0.73
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 1.18; acc: 0.69
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.96; acc: 0.77
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.83; acc: 0.73
Batch: 480; loss: 0.68; acc: 0.8
Batch: 500; loss: 0.81; acc: 0.7
Batch: 520; loss: 0.96; acc: 0.75
Batch: 540; loss: 0.82; acc: 0.83
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 1.32; acc: 0.66
Batch: 600; loss: 1.04; acc: 0.75
Batch: 620; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.78 

Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 1.57; acc: 0.64
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.94; acc: 0.78
Batch: 80; loss: 0.92; acc: 0.75
Batch: 100; loss: 1.61; acc: 0.72
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 1.79; acc: 0.58
Val Epoch over. val_loss: 0.8998962563883727; val_accuracy: 0.7700039808917197 

Epoch 12 start
Batch: 0; loss: 1.4; acc: 0.75
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.75
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.65; acc: 0.81
Batch: 180; loss: 1.13; acc: 0.73
Batch: 200; loss: 0.67; acc: 0.8
Batch: 220; loss: 1.08; acc: 0.8
Batch: 240; loss: 1.04; acc: 0.75
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.93; acc: 0.77
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 1.12; acc: 0.77
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.86; acc: 0.8
Batch: 400; loss: 0.62; acc: 0.8
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.73
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 1.16; acc: 0.7
Batch: 500; loss: 1.04; acc: 0.78
Batch: 520; loss: 0.94; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 1.12; acc: 0.73
Batch: 600; loss: 0.76; acc: 0.81
Batch: 620; loss: 1.37; acc: 0.73
Train Epoch over. train_loss: 0.83; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 1.51; acc: 0.62
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 1.44; acc: 0.69
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 1.55; acc: 0.61
Val Epoch over. val_loss: 0.7981685006124958; val_accuracy: 0.793093152866242 

Epoch 13 start
Batch: 0; loss: 1.05; acc: 0.77
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 1.16; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 1.27; acc: 0.7
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.95; acc: 0.8
Batch: 160; loss: 1.03; acc: 0.77
Batch: 180; loss: 1.12; acc: 0.72
Batch: 200; loss: 0.88; acc: 0.72
Batch: 220; loss: 0.93; acc: 0.78
Batch: 240; loss: 0.87; acc: 0.77
Batch: 260; loss: 0.65; acc: 0.81
Batch: 280; loss: 0.89; acc: 0.8
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 1.3; acc: 0.77
Batch: 380; loss: 0.8; acc: 0.8
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 1.23; acc: 0.66
Batch: 440; loss: 1.06; acc: 0.8
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.75; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.73; acc: 0.84
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 1.33; acc: 0.75
Batch: 600; loss: 1.84; acc: 0.64
Batch: 620; loss: 1.04; acc: 0.7
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 1.44; acc: 0.7
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 1.24; acc: 0.7
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 1.7; acc: 0.59
Val Epoch over. val_loss: 0.8970827877901162; val_accuracy: 0.7699044585987261 

Epoch 14 start
Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 1.06; acc: 0.72
Batch: 80; loss: 0.91; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.89
Batch: 120; loss: 1.12; acc: 0.77
Batch: 140; loss: 0.56; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.79; acc: 0.72
Batch: 200; loss: 0.94; acc: 0.75
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.78; acc: 0.77
Batch: 260; loss: 0.65; acc: 0.75
Batch: 280; loss: 1.1; acc: 0.73
Batch: 300; loss: 0.83; acc: 0.81
Batch: 320; loss: 0.66; acc: 0.78
Batch: 340; loss: 1.12; acc: 0.7
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 0.91; acc: 0.8
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.85; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.8
Batch: 500; loss: 1.37; acc: 0.69
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 1.09; acc: 0.75
Batch: 620; loss: 0.87; acc: 0.77
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.86; acc: 0.78
Batch: 20; loss: 1.41; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.73
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 1.3; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 1.57; acc: 0.61
Val Epoch over. val_loss: 0.8848949321516001; val_accuracy: 0.7776671974522293 

Epoch 15 start
Batch: 0; loss: 1.09; acc: 0.69
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 1.41; acc: 0.66
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 1.49; acc: 0.67
Batch: 100; loss: 1.09; acc: 0.77
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.88; acc: 0.78
Batch: 180; loss: 0.81; acc: 0.78
Batch: 200; loss: 0.72; acc: 0.86
Batch: 220; loss: 0.74; acc: 0.84
Batch: 240; loss: 1.25; acc: 0.7
Batch: 260; loss: 1.02; acc: 0.75
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.92; acc: 0.78
Batch: 320; loss: 0.85; acc: 0.8
Batch: 340; loss: 1.29; acc: 0.7
Batch: 360; loss: 0.96; acc: 0.75
Batch: 380; loss: 0.94; acc: 0.75
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.83
Batch: 460; loss: 1.05; acc: 0.69
Batch: 480; loss: 0.93; acc: 0.73
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.8; acc: 0.83
Batch: 600; loss: 0.95; acc: 0.73
Batch: 620; loss: 0.86; acc: 0.8
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 1.7; acc: 0.59
Batch: 40; loss: 0.79; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 0.85; acc: 0.8
Batch: 100; loss: 1.58; acc: 0.69
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 1.55; acc: 0.62
Val Epoch over. val_loss: 0.878743296785719; val_accuracy: 0.7799562101910829 

Epoch 16 start
Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.88; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 0.84; acc: 0.83
Batch: 80; loss: 0.87; acc: 0.78
Batch: 100; loss: 0.94; acc: 0.75
Batch: 120; loss: 0.9; acc: 0.81
Batch: 140; loss: 0.9; acc: 0.73
Batch: 160; loss: 0.87; acc: 0.81
Batch: 180; loss: 1.04; acc: 0.73
Batch: 200; loss: 1.16; acc: 0.67
Batch: 220; loss: 0.51; acc: 0.8
Batch: 240; loss: 0.97; acc: 0.72
Batch: 260; loss: 0.79; acc: 0.73
Batch: 280; loss: 0.81; acc: 0.77
Batch: 300; loss: 1.17; acc: 0.73
Batch: 320; loss: 0.94; acc: 0.75
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 1.13; acc: 0.72
Batch: 380; loss: 1.16; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.81
Batch: 420; loss: 1.12; acc: 0.73
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 0.68; acc: 0.77
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 1.08; acc: 0.7
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.99; acc: 0.73
Batch: 600; loss: 0.98; acc: 0.78
Batch: 620; loss: 0.86; acc: 0.77
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 1.27; acc: 0.67
Batch: 40; loss: 0.84; acc: 0.78
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.94; acc: 0.77
Batch: 100; loss: 1.37; acc: 0.67
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 1.54; acc: 0.59
Val Epoch over. val_loss: 0.8193070278236061; val_accuracy: 0.7898089171974523 

Epoch 17 start
Batch: 0; loss: 0.92; acc: 0.75
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 0.83; acc: 0.83
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.74; acc: 0.84
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.81; acc: 0.8
Batch: 220; loss: 0.79; acc: 0.84
Batch: 240; loss: 0.94; acc: 0.77
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.67; acc: 0.75
Batch: 300; loss: 0.75; acc: 0.83
Batch: 320; loss: 0.59; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.94; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.78
Batch: 440; loss: 0.91; acc: 0.73
Batch: 460; loss: 1.01; acc: 0.75
Batch: 480; loss: 0.78; acc: 0.73
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.94; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.9; acc: 0.75
Batch: 600; loss: 1.17; acc: 0.73
Batch: 620; loss: 0.55; acc: 0.89
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 1.49; acc: 0.7
Batch: 40; loss: 1.13; acc: 0.75
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.78
Batch: 100; loss: 1.31; acc: 0.7
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 1.65; acc: 0.59
Val Epoch over. val_loss: 0.9388526541412257; val_accuracy: 0.7713972929936306 

Epoch 18 start
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 1.39; acc: 0.7
Batch: 60; loss: 0.73; acc: 0.84
Batch: 80; loss: 0.85; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.78
Batch: 180; loss: 0.97; acc: 0.8
Batch: 200; loss: 1.6; acc: 0.72
Batch: 220; loss: 1.3; acc: 0.69
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 1.07; acc: 0.7
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 1.32; acc: 0.7
Batch: 340; loss: 0.95; acc: 0.73
Batch: 360; loss: 0.95; acc: 0.73
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.81; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.8
Batch: 440; loss: 1.31; acc: 0.72
Batch: 460; loss: 0.88; acc: 0.78
Batch: 480; loss: 0.79; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.73; acc: 0.81
Batch: 540; loss: 1.34; acc: 0.72
Batch: 560; loss: 0.66; acc: 0.78
Batch: 580; loss: 0.84; acc: 0.75
Batch: 600; loss: 1.16; acc: 0.73
Batch: 620; loss: 0.76; acc: 0.83
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.89; acc: 0.83
Batch: 20; loss: 1.46; acc: 0.64
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.75; acc: 0.78
Batch: 80; loss: 0.83; acc: 0.8
Batch: 100; loss: 1.67; acc: 0.67
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 1.73; acc: 0.61
Val Epoch over. val_loss: 0.9602147627408337; val_accuracy: 0.7651273885350318 

Epoch 19 start
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.54; acc: 0.64
Batch: 40; loss: 0.81; acc: 0.78
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 1.14; acc: 0.73
Batch: 120; loss: 0.92; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 1.26; acc: 0.77
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.9; acc: 0.73
Batch: 240; loss: 0.93; acc: 0.73
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 1.38; acc: 0.64
Batch: 300; loss: 0.52; acc: 0.81
Batch: 320; loss: 0.91; acc: 0.8
Batch: 340; loss: 0.83; acc: 0.84
Batch: 360; loss: 0.87; acc: 0.72
Batch: 380; loss: 0.96; acc: 0.78
Batch: 400; loss: 0.72; acc: 0.8
Batch: 420; loss: 0.91; acc: 0.8
Batch: 440; loss: 0.81; acc: 0.81
Batch: 460; loss: 1.15; acc: 0.73
Batch: 480; loss: 1.04; acc: 0.78
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 1.02; acc: 0.78
Batch: 540; loss: 0.98; acc: 0.72
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.78; acc: 0.81
Batch: 600; loss: 0.8; acc: 0.77
Batch: 620; loss: 1.39; acc: 0.72
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 1.44; acc: 0.61
Batch: 40; loss: 0.74; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.83
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 1.37; acc: 0.67
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 1.85; acc: 0.56
Val Epoch over. val_loss: 0.8966936116005965; val_accuracy: 0.7727906050955414 

Epoch 20 start
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.85; acc: 0.81
Batch: 60; loss: 1.21; acc: 0.72
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.91; acc: 0.75
Batch: 160; loss: 0.8; acc: 0.8
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.84; acc: 0.8
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 1.5; acc: 0.69
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 1.43; acc: 0.66
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.72; acc: 0.83
Batch: 360; loss: 0.99; acc: 0.8
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 1.07; acc: 0.77
Batch: 420; loss: 1.81; acc: 0.72
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 1.07; acc: 0.75
Batch: 480; loss: 0.73; acc: 0.72
Batch: 500; loss: 0.61; acc: 0.88
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 1.13; acc: 0.72
Batch: 580; loss: 0.74; acc: 0.75
Batch: 600; loss: 0.6; acc: 0.8
Batch: 620; loss: 0.6; acc: 0.78
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

Batch: 0; loss: 0.62; acc: 0.86
Batch: 20; loss: 1.36; acc: 0.61
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.67; acc: 0.78
Batch: 100; loss: 1.29; acc: 0.7
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 1.38; acc: 0.61
Val Epoch over. val_loss: 0.8117385307315049; val_accuracy: 0.7826433121019108 

Epoch 21 start
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.79; acc: 0.75
Batch: 60; loss: 0.61; acc: 0.75
Batch: 80; loss: 1.33; acc: 0.75
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.9; acc: 0.78
Batch: 160; loss: 1.03; acc: 0.73
Batch: 180; loss: 0.99; acc: 0.78
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.94
Batch: 240; loss: 1.04; acc: 0.69
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.8; acc: 0.8
Batch: 320; loss: 0.66; acc: 0.77
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.9; acc: 0.78
Batch: 380; loss: 1.35; acc: 0.7
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.85; acc: 0.77
Batch: 460; loss: 0.9; acc: 0.75
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.75
Batch: 540; loss: 0.82; acc: 0.83
Batch: 560; loss: 0.82; acc: 0.8
Batch: 580; loss: 1.09; acc: 0.81
Batch: 600; loss: 0.94; acc: 0.75
Batch: 620; loss: 1.23; acc: 0.72
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 1.47; acc: 0.67
Batch: 40; loss: 0.69; acc: 0.88
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 1.25; acc: 0.69
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 1.74; acc: 0.56
Val Epoch over. val_loss: 0.8419121218145273; val_accuracy: 0.7820461783439491 

Epoch 22 start
Batch: 0; loss: 1.23; acc: 0.78
Batch: 20; loss: 0.87; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 0.77; acc: 0.86
Batch: 140; loss: 0.68; acc: 0.83
Batch: 160; loss: 1.15; acc: 0.72
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 1.06; acc: 0.78
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.82; acc: 0.75
Batch: 260; loss: 1.03; acc: 0.78
Batch: 280; loss: 0.93; acc: 0.78
Batch: 300; loss: 1.08; acc: 0.83
Batch: 320; loss: 0.96; acc: 0.84
Batch: 340; loss: 1.01; acc: 0.73
Batch: 360; loss: 0.81; acc: 0.69
Batch: 380; loss: 1.02; acc: 0.7
Batch: 400; loss: 0.87; acc: 0.75
Batch: 420; loss: 0.94; acc: 0.78
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.81
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.74; acc: 0.72
Batch: 520; loss: 1.03; acc: 0.77
Batch: 540; loss: 0.84; acc: 0.75
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 1.1; acc: 0.73
Batch: 600; loss: 0.64; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.8
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.74; acc: 0.83
Batch: 20; loss: 1.24; acc: 0.66
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 1.53; acc: 0.67
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 1.81; acc: 0.59
Val Epoch over. val_loss: 0.8251280671661827; val_accuracy: 0.7851313694267515 

Epoch 23 start
Batch: 0; loss: 0.78; acc: 0.84
Batch: 20; loss: 0.89; acc: 0.81
Batch: 40; loss: 0.91; acc: 0.75
Batch: 60; loss: 0.77; acc: 0.81
Batch: 80; loss: 1.45; acc: 0.75
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.87; acc: 0.78
Batch: 160; loss: 1.03; acc: 0.77
Batch: 180; loss: 0.79; acc: 0.72
Batch: 200; loss: 1.28; acc: 0.78
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.84; acc: 0.75
Batch: 260; loss: 0.75; acc: 0.83
Batch: 280; loss: 1.03; acc: 0.75
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 1.13; acc: 0.77
Batch: 340; loss: 0.96; acc: 0.72
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 1.24; acc: 0.7
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 1.17; acc: 0.78
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.89; acc: 0.8
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 1.07; acc: 0.67
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.1; acc: 0.69
Batch: 620; loss: 1.02; acc: 0.8
Train Epoch over. train_loss: 0.85; train_accuracy: 0.78 

Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 1.32; acc: 0.69
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 1.29; acc: 0.7
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 1.06; acc: 0.72
Val Epoch over. val_loss: 0.8207998702860182; val_accuracy: 0.7961783439490446 

Epoch 24 start
Batch: 0; loss: 1.03; acc: 0.77
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.91; acc: 0.8
Batch: 60; loss: 1.0; acc: 0.77
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.83; acc: 0.75
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 1.04; acc: 0.72
Batch: 160; loss: 0.78; acc: 0.75
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.93; acc: 0.77
Batch: 220; loss: 0.71; acc: 0.8
Batch: 240; loss: 0.59; acc: 0.8
Batch: 260; loss: 0.69; acc: 0.86
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.97; acc: 0.7
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.78
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 1.16; acc: 0.73
Batch: 400; loss: 0.88; acc: 0.8
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.75; acc: 0.86
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 0.9; acc: 0.78
Batch: 500; loss: 1.09; acc: 0.73
Batch: 520; loss: 1.06; acc: 0.69
Batch: 540; loss: 1.16; acc: 0.73
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.98; acc: 0.73
Batch: 620; loss: 1.11; acc: 0.69
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 1.22; acc: 0.69
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 1.28; acc: 0.69
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 1.46; acc: 0.69
Val Epoch over. val_loss: 0.7858814435779669; val_accuracy: 0.7955812101910829 

Epoch 25 start
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 1.0; acc: 0.75
Batch: 80; loss: 0.88; acc: 0.83
Batch: 100; loss: 0.97; acc: 0.77
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 1.08; acc: 0.8
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.98; acc: 0.81
Batch: 200; loss: 0.79; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 1.11; acc: 0.67
Batch: 260; loss: 0.87; acc: 0.77
Batch: 280; loss: 1.07; acc: 0.73
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 1.04; acc: 0.81
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 1.4; acc: 0.61
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.72; acc: 0.73
Batch: 420; loss: 0.91; acc: 0.75
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.96; acc: 0.75
Batch: 480; loss: 0.81; acc: 0.77
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.75
Batch: 540; loss: 0.66; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.99; acc: 0.75
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.91; acc: 0.78
Train Epoch over. train_loss: 0.83; train_accuracy: 0.78 

Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 1.61; acc: 0.66
Batch: 40; loss: 1.04; acc: 0.75
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.79; acc: 0.81
Batch: 100; loss: 1.45; acc: 0.64
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 1.76; acc: 0.61
Val Epoch over. val_loss: 1.0126506143314824; val_accuracy: 0.7521894904458599 

Epoch 26 start
Batch: 0; loss: 1.17; acc: 0.8
Batch: 20; loss: 0.97; acc: 0.77
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 0.78; acc: 0.83
Batch: 80; loss: 0.95; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 1.17; acc: 0.77
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.79; acc: 0.78
Batch: 200; loss: 0.81; acc: 0.78
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.91; acc: 0.72
Batch: 260; loss: 1.09; acc: 0.72
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.97; acc: 0.73
Batch: 320; loss: 0.72; acc: 0.84
Batch: 340; loss: 0.99; acc: 0.72
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 1.11; acc: 0.73
Batch: 440; loss: 0.5; acc: 0.81
Batch: 460; loss: 1.03; acc: 0.86
Batch: 480; loss: 0.97; acc: 0.8
Batch: 500; loss: 0.68; acc: 0.77
Batch: 520; loss: 0.91; acc: 0.8
Batch: 540; loss: 1.03; acc: 0.77
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 1.29; acc: 0.73
Batch: 620; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 1.22; acc: 0.64
Batch: 40; loss: 0.82; acc: 0.8
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.77
Batch: 100; loss: 1.24; acc: 0.72
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 1.66; acc: 0.61
Val Epoch over. val_loss: 0.8260616675304; val_accuracy: 0.7883160828025477 

Epoch 27 start
Batch: 0; loss: 0.85; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.91; acc: 0.77
Batch: 60; loss: 0.99; acc: 0.78
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 1.44; acc: 0.72
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.67; acc: 0.77
Batch: 220; loss: 0.85; acc: 0.77
Batch: 240; loss: 1.04; acc: 0.69
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.69; acc: 0.78
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.92; acc: 0.77
Batch: 340; loss: 1.2; acc: 0.77
Batch: 360; loss: 1.75; acc: 0.66
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 0.71; acc: 0.77
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 1.01; acc: 0.77
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.84; acc: 0.77
Batch: 580; loss: 1.3; acc: 0.66
Batch: 600; loss: 0.84; acc: 0.77
Batch: 620; loss: 0.73; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.79 

Batch: 0; loss: 0.93; acc: 0.77
Batch: 20; loss: 1.62; acc: 0.69
Batch: 40; loss: 1.02; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.77
Batch: 80; loss: 1.33; acc: 0.69
Batch: 100; loss: 1.3; acc: 0.64
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 2.25; acc: 0.58
Val Epoch over. val_loss: 1.1229922418381757; val_accuracy: 0.7338773885350318 

Epoch 28 start
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.74; acc: 0.75
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 1.14; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.81; acc: 0.78
Batch: 160; loss: 1.19; acc: 0.72
Batch: 180; loss: 0.96; acc: 0.78
Batch: 200; loss: 1.08; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.81
Batch: 240; loss: 1.59; acc: 0.66
Batch: 260; loss: 0.94; acc: 0.75
Batch: 280; loss: 1.02; acc: 0.8
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 1.14; acc: 0.73
Batch: 340; loss: 0.95; acc: 0.78
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.96; acc: 0.77
Batch: 400; loss: 0.91; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.75
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 1.55; acc: 0.7
Batch: 500; loss: 0.48; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 1.33; acc: 0.72
Batch: 600; loss: 0.84; acc: 0.78
Batch: 620; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.79; acc: 0.81
Batch: 20; loss: 1.24; acc: 0.67
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 1.41; acc: 0.67
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 1.26; acc: 0.64
Val Epoch over. val_loss: 0.828591215192892; val_accuracy: 0.7847332802547771 

Epoch 29 start
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.91; acc: 0.75
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 1.17; acc: 0.72
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.84; acc: 0.78
Batch: 160; loss: 1.13; acc: 0.7
Batch: 180; loss: 1.04; acc: 0.77
Batch: 200; loss: 0.89; acc: 0.77
Batch: 220; loss: 0.6; acc: 0.86
Batch: 240; loss: 0.8; acc: 0.8
Batch: 260; loss: 0.82; acc: 0.77
Batch: 280; loss: 0.88; acc: 0.8
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.57; acc: 0.81
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.78; acc: 0.78
Batch: 420; loss: 0.97; acc: 0.72
Batch: 440; loss: 1.43; acc: 0.73
Batch: 460; loss: 1.31; acc: 0.7
Batch: 480; loss: 1.12; acc: 0.73
Batch: 500; loss: 1.24; acc: 0.75
Batch: 520; loss: 0.78; acc: 0.77
Batch: 540; loss: 0.93; acc: 0.75
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.8
Batch: 620; loss: 0.99; acc: 0.75
Train Epoch over. train_loss: 0.85; train_accuracy: 0.78 

Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 1.43; acc: 0.62
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.87; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.69
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 1.61; acc: 0.56
Val Epoch over. val_loss: 0.8737288878601828; val_accuracy: 0.7752786624203821 

Epoch 30 start
Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.77
Batch: 40; loss: 0.82; acc: 0.81
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 1.03; acc: 0.75
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.81; acc: 0.77
Batch: 200; loss: 1.35; acc: 0.66
Batch: 220; loss: 0.79; acc: 0.81
Batch: 240; loss: 0.58; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.78
Batch: 280; loss: 0.93; acc: 0.77
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.48; acc: 0.81
Batch: 360; loss: 1.4; acc: 0.69
Batch: 380; loss: 0.96; acc: 0.78
Batch: 400; loss: 1.41; acc: 0.8
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.78; acc: 0.78
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 0.86; acc: 0.84
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.46; acc: 0.8
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.76; acc: 0.73
Batch: 620; loss: 1.22; acc: 0.75
Train Epoch over. train_loss: 0.84; train_accuracy: 0.78 

Batch: 0; loss: 0.76; acc: 0.84
Batch: 20; loss: 1.34; acc: 0.64
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.86; acc: 0.73
Batch: 100; loss: 1.24; acc: 0.72
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 1.67; acc: 0.59
Val Epoch over. val_loss: 0.8256575799292061; val_accuracy: 0.7770700636942676 

plots/subspace_training/lenet/2020-01-10 04:26:27/d_dim_200_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 14.88; acc: 0.16
Batch: 20; loss: 2.4; acc: 0.3
Batch: 40; loss: 1.69; acc: 0.5
Batch: 60; loss: 1.21; acc: 0.62
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 1.11; acc: 0.53
Batch: 120; loss: 1.38; acc: 0.56
Batch: 140; loss: 0.85; acc: 0.67
Batch: 160; loss: 1.05; acc: 0.64
Batch: 180; loss: 1.08; acc: 0.7
Batch: 200; loss: 1.24; acc: 0.67
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 1.07; acc: 0.73
Batch: 280; loss: 0.96; acc: 0.75
Batch: 300; loss: 0.86; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.98; acc: 0.77
Batch: 380; loss: 0.88; acc: 0.73
Batch: 400; loss: 0.96; acc: 0.73
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.95; acc: 0.75
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 0.8; acc: 0.78
Batch: 520; loss: 0.92; acc: 0.77
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.93; acc: 0.78
Batch: 580; loss: 0.91; acc: 0.77
Batch: 600; loss: 1.12; acc: 0.75
Batch: 620; loss: 0.52; acc: 0.8
Train Epoch over. train_loss: 1.09; train_accuracy: 0.7 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 1.0; acc: 0.67
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.81; acc: 0.77
Batch: 120; loss: 0.74; acc: 0.84
Batch: 140; loss: 1.05; acc: 0.72
Val Epoch over. val_loss: 0.7735528609934886; val_accuracy: 0.7913017515923567 

Epoch 2 start
Batch: 0; loss: 1.09; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.78
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.77; acc: 0.81
Batch: 200; loss: 1.05; acc: 0.75
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.9; acc: 0.73
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.77
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.75; acc: 0.78
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 1.27; acc: 0.7
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.72; train_accuracy: 0.8 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 1.07; acc: 0.7
Val Epoch over. val_loss: 0.6783142917475123; val_accuracy: 0.8163813694267515 

Epoch 3 start
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.88; acc: 0.8
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.98; acc: 0.78
Batch: 220; loss: 0.83; acc: 0.81
Batch: 240; loss: 0.88; acc: 0.84
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.85; acc: 0.77
Batch: 380; loss: 1.02; acc: 0.77
Batch: 400; loss: 1.27; acc: 0.69
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.86; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 1.02; acc: 0.81
Batch: 560; loss: 0.86; acc: 0.77
Batch: 580; loss: 0.76; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.78
Batch: 620; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.67; train_accuracy: 0.82 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.47; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.93; acc: 0.73
Val Epoch over. val_loss: 0.6568387323504041; val_accuracy: 0.8212579617834395 

Epoch 4 start
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 1.05; acc: 0.73
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.56; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 1.09; acc: 0.8
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 1.04; acc: 0.75
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.85; acc: 0.83
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.84; acc: 0.77
Batch: 620; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.88; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.94; acc: 0.78
Val Epoch over. val_loss: 0.6832728442872406; val_accuracy: 0.8153861464968153 

Epoch 5 start
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.78
Batch: 160; loss: 0.73; acc: 0.83
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.69; acc: 0.81
Batch: 260; loss: 0.95; acc: 0.7
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.63; acc: 0.77
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.75; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.8
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.83 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.95; acc: 0.73
Val Epoch over. val_loss: 0.6136872196083616; val_accuracy: 0.8350915605095541 

Epoch 6 start
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.79; acc: 0.77
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.64; acc: 0.81
Batch: 260; loss: 0.87; acc: 0.77
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.79; acc: 0.73
Batch: 320; loss: 0.56; acc: 0.91
Batch: 340; loss: 1.09; acc: 0.81
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.73; acc: 0.88
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.79; acc: 0.86
Batch: 560; loss: 0.69; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.78
Batch: 600; loss: 1.17; acc: 0.73
Batch: 620; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.63; train_accuracy: 0.83 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 1.17; acc: 0.72
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.88; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.7; acc: 0.75
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.99; acc: 0.73
Val Epoch over. val_loss: 0.6674302528808072; val_accuracy: 0.8213574840764332 

Epoch 7 start
Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.83; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.85; acc: 0.8
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.76; acc: 0.83
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.86; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.61; acc: 0.75
Batch: 360; loss: 0.77; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.88; acc: 0.78
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.78; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.7; acc: 0.88
Batch: 500; loss: 0.82; acc: 0.69
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 1.17; acc: 0.66
Batch: 620; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.62; train_accuracy: 0.83 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 1.0; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.89; acc: 0.81
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.84; acc: 0.78
Val Epoch over. val_loss: 0.6179147664528744; val_accuracy: 0.8332006369426752 

Epoch 8 start
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.85; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.9; acc: 0.84
Batch: 160; loss: 1.02; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.84; acc: 0.73
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.84; acc: 0.77
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.85; acc: 0.78
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.65; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.96; acc: 0.77
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.62; train_accuracy: 0.83 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.94; acc: 0.77
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 0.6308339379585473; val_accuracy: 0.8241441082802548 

Epoch 9 start
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.84
Batch: 60; loss: 0.96; acc: 0.73
Batch: 80; loss: 1.24; acc: 0.7
Batch: 100; loss: 0.77; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 1.43; acc: 0.77
Batch: 220; loss: 0.25; acc: 0.88
Batch: 240; loss: 0.92; acc: 0.81
Batch: 260; loss: 0.89; acc: 0.81
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.62; acc: 0.78
Batch: 320; loss: 0.62; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.86
Batch: 360; loss: 0.94; acc: 0.86
Batch: 380; loss: 0.71; acc: 0.81
Batch: 400; loss: 0.85; acc: 0.8
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.83; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.66; acc: 0.83
Batch: 560; loss: 1.2; acc: 0.75
Batch: 580; loss: 0.42; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.63; train_accuracy: 0.83 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 1.16; acc: 0.77
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.9; acc: 0.78
Val Epoch over. val_loss: 0.6301666401374112; val_accuracy: 0.8376791401273885 

Epoch 10 start
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.74; acc: 0.81
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 1.36; acc: 0.77
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.75; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.84
Train Epoch over. train_loss: 0.62; train_accuracy: 0.84 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 1.35; acc: 0.69
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.88; acc: 0.81
Val Epoch over. val_loss: 0.6101668697253914; val_accuracy: 0.8371815286624203 

Epoch 11 start
Batch: 0; loss: 0.99; acc: 0.8
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 1.03; acc: 0.78
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 1.18; acc: 0.72
Batch: 280; loss: 0.75; acc: 0.84
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.8; acc: 0.72
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.81
Batch: 400; loss: 0.66; acc: 0.8
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.87; acc: 0.81
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.81
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.88
Train Epoch over. train_loss: 0.63; train_accuracy: 0.83 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.73
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.95; acc: 0.78
Val Epoch over. val_loss: 0.7105330267244843; val_accuracy: 0.8153861464968153 

Epoch 12 start
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 1.18; acc: 0.73
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.53; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.75
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.8; acc: 0.8
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.78; acc: 0.78
Batch: 280; loss: 0.42; acc: 0.81
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.81; acc: 0.83
Batch: 380; loss: 0.58; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.88; acc: 0.78
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.83; acc: 0.81
Batch: 580; loss: 0.69; acc: 0.89
Batch: 600; loss: 0.8; acc: 0.81
Batch: 620; loss: 0.53; acc: 0.91
Train Epoch over. train_loss: 0.62; train_accuracy: 0.84 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 1.2; acc: 0.73
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.94; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 1.03; acc: 0.73
Val Epoch over. val_loss: 0.6252196710200826; val_accuracy: 0.8335987261146497 

Epoch 13 start
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.73; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.84
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.81
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.58; acc: 0.8
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.73
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.95; acc: 0.8
Batch: 460; loss: 0.94; acc: 0.81
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.82; acc: 0.8
Batch: 520; loss: 0.83; acc: 0.83
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.84
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.62; train_accuracy: 0.84 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 1.28; acc: 0.69
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.4; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.72
Val Epoch over. val_loss: 0.628580991819406; val_accuracy: 0.8382762738853503 

Epoch 14 start
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.91; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 1.13; acc: 0.78
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.93; acc: 0.75
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.38; acc: 0.83
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.89; acc: 0.86
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.66; acc: 0.83
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.88
Batch: 600; loss: 0.95; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 1.17; acc: 0.73
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.76; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.99; acc: 0.67
Val Epoch over. val_loss: 0.6397566411905228; val_accuracy: 0.8305135350318471 

Epoch 15 start
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.75; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.65; acc: 0.8
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.82; acc: 0.81
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.79; acc: 0.83
Batch: 400; loss: 0.79; acc: 0.77
Batch: 420; loss: 0.84; acc: 0.77
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.67; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.75; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.9; acc: 0.81
Batch: 600; loss: 0.86; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.6; train_accuracy: 0.84 

Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 1.33; acc: 0.67
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 1.01; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 1.23; acc: 0.64
Val Epoch over. val_loss: 0.7393704853050268; val_accuracy: 0.8077229299363057 

Epoch 16 start
Batch: 0; loss: 1.28; acc: 0.69
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.81; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.87; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.8
Batch: 180; loss: 0.81; acc: 0.81
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.85; acc: 0.81
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.7; acc: 0.84
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.81
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 1.28; acc: 0.67
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.62; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.92; acc: 0.72
Val Epoch over. val_loss: 0.6406943911010292; val_accuracy: 0.8242436305732485 

Epoch 17 start
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 1.14; acc: 0.77
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.74; acc: 0.73
Batch: 160; loss: 0.66; acc: 0.75
Batch: 180; loss: 0.81; acc: 0.83
Batch: 200; loss: 0.75; acc: 0.8
Batch: 220; loss: 0.88; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.83
Batch: 260; loss: 0.92; acc: 0.81
Batch: 280; loss: 0.92; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.75; acc: 0.77
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.78; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.81
Batch: 560; loss: 0.82; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.68; acc: 0.78
Batch: 620; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.6; train_accuracy: 0.84 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 1.12; acc: 0.8
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.79; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.88
Batch: 140; loss: 0.85; acc: 0.67
Val Epoch over. val_loss: 0.5976880196553127; val_accuracy: 0.8423566878980892 

Epoch 18 start
Batch: 0; loss: 0.91; acc: 0.8
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.77
Batch: 300; loss: 0.81; acc: 0.78
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.86; acc: 0.75
Batch: 360; loss: 1.13; acc: 0.83
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.62; acc: 0.8
Batch: 420; loss: 0.52; acc: 0.78
Batch: 440; loss: 0.63; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.86
Batch: 480; loss: 0.99; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.36; acc: 0.84
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 1.56; acc: 0.7
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.79; acc: 0.81
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.83
Batch: 140; loss: 1.08; acc: 0.73
Val Epoch over. val_loss: 0.7390642447076785; val_accuracy: 0.8215565286624203 

Epoch 19 start
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.8
Batch: 160; loss: 0.72; acc: 0.86
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.92; acc: 0.77
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.95
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.94; acc: 0.72
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 1.18; acc: 0.66
Batch: 380; loss: 0.68; acc: 0.81
Batch: 400; loss: 0.83; acc: 0.78
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.63; acc: 0.8
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.69; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.42; acc: 0.81
Batch: 20; loss: 1.45; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.77; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.92; acc: 0.77
Val Epoch over. val_loss: 0.6760148961263098; val_accuracy: 0.8269307324840764 

Epoch 20 start
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.8
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.69; acc: 0.83
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.79; acc: 0.83
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.9; acc: 0.8
Batch: 340; loss: 0.47; acc: 0.78
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.77; acc: 0.77
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.73; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.9; acc: 0.8
Batch: 540; loss: 0.72; acc: 0.86
Batch: 560; loss: 0.87; acc: 0.8
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.92
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 1.08; acc: 0.77
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.88; acc: 0.8
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.73
Val Epoch over. val_loss: 0.6060869505830632; val_accuracy: 0.8381767515923567 

Epoch 21 start
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.84
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.95; acc: 0.81
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 1.1; acc: 0.72
Batch: 320; loss: 0.67; acc: 0.84
Batch: 340; loss: 1.05; acc: 0.77
Batch: 360; loss: 0.57; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.81
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.69; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.75; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 1.04; acc: 0.75
Batch: 580; loss: 0.61; acc: 0.77
Batch: 600; loss: 1.08; acc: 0.84
Batch: 620; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 1.3; acc: 0.72
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.78
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.73
Val Epoch over. val_loss: 0.6225934196619471; val_accuracy: 0.8332006369426752 

Epoch 22 start
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.76; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.77
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.81; acc: 0.75
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 1.41; acc: 0.7
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.86; acc: 0.81
Batch: 320; loss: 0.97; acc: 0.75
Batch: 340; loss: 0.49; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.94; acc: 0.81
Batch: 440; loss: 0.94; acc: 0.77
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.83
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.6; acc: 0.92
Batch: 540; loss: 1.26; acc: 0.73
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.93; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 1.16; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.69; acc: 0.77
Val Epoch over. val_loss: 0.6428284032899103; val_accuracy: 0.833797770700637 

Epoch 23 start
Batch: 0; loss: 0.36; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.83
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.78; acc: 0.77
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.91
Batch: 380; loss: 0.72; acc: 0.73
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.89; acc: 0.75
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.79; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.95; acc: 0.77
Batch: 620; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.84 

Batch: 0; loss: 0.44; acc: 0.91
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.79; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 1.04; acc: 0.75
Val Epoch over. val_loss: 0.636889014464275; val_accuracy: 0.8412619426751592 

Epoch 24 start
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.88
Batch: 40; loss: 0.9; acc: 0.78
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.75; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.8
Batch: 200; loss: 0.93; acc: 0.73
Batch: 220; loss: 1.36; acc: 0.75
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.96; acc: 0.77
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.59; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.77
Batch: 460; loss: 0.85; acc: 0.83
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.28; acc: 0.86
Batch: 560; loss: 0.59; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.6; train_accuracy: 0.84 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 1.36; acc: 0.7
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.92; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.78
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.8; acc: 0.73
Val Epoch over. val_loss: 0.6199183316936918; val_accuracy: 0.8378781847133758 

Epoch 25 start
Batch: 0; loss: 0.91; acc: 0.72
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.83; acc: 0.81
Batch: 200; loss: 0.87; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.56; acc: 0.81
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.91
Batch: 400; loss: 0.81; acc: 0.84
Batch: 420; loss: 0.8; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.97; acc: 0.77
Batch: 480; loss: 0.64; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.79; acc: 0.81
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.74; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 1.07; acc: 0.72
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.77
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.88; acc: 0.7
Val Epoch over. val_loss: 0.629276609249935; val_accuracy: 0.8383757961783439 

Epoch 26 start
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 1.61; acc: 0.7
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 1.24; acc: 0.78
Batch: 140; loss: 0.7; acc: 0.86
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 0.63; acc: 0.89
Batch: 200; loss: 0.83; acc: 0.75
Batch: 220; loss: 0.81; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.83
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.92; acc: 0.78
Batch: 360; loss: 0.38; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.76; acc: 0.8
Batch: 420; loss: 0.61; acc: 0.91
Batch: 440; loss: 1.05; acc: 0.78
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.79; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.89; acc: 0.77
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.8; acc: 0.83
Batch: 600; loss: 0.87; acc: 0.81
Batch: 620; loss: 0.88; acc: 0.84
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.82; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.88
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 1.03; acc: 0.72
Val Epoch over. val_loss: 0.717030886252215; val_accuracy: 0.8199641719745223 

Epoch 27 start
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.81; acc: 0.8
Batch: 220; loss: 1.09; acc: 0.78
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.7; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.79; acc: 0.8
Batch: 360; loss: 0.95; acc: 0.78
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.86
Batch: 420; loss: 0.99; acc: 0.8
Batch: 440; loss: 0.97; acc: 0.73
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.52; acc: 0.83
Batch: 540; loss: 0.71; acc: 0.77
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 1.08; acc: 0.78
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.86
Batch: 140; loss: 0.83; acc: 0.8
Val Epoch over. val_loss: 0.6142679169109673; val_accuracy: 0.839171974522293 

Epoch 28 start
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 1.22; acc: 0.81
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.83; acc: 0.8
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.95; acc: 0.77
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.71; acc: 0.83
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.79; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.77; acc: 0.75
Batch: 400; loss: 0.73; acc: 0.77
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.77; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.81; acc: 0.84
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.96; acc: 0.77
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.58; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.9; acc: 0.77
Train Epoch over. train_loss: 0.61; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 1.41; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.78
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.73
Val Epoch over. val_loss: 0.6010301225599209; val_accuracy: 0.8445461783439491 

Epoch 29 start
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.68; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.75; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.81
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.86; acc: 0.75
Batch: 560; loss: 1.26; acc: 0.77
Batch: 580; loss: 0.82; acc: 0.83
Batch: 600; loss: 0.94; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.62; train_accuracy: 0.84 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 1.02; acc: 0.73
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.73; acc: 0.77
Val Epoch over. val_loss: 0.5651700333899753; val_accuracy: 0.854796974522293 

Epoch 30 start
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 0.97; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.86
Batch: 140; loss: 0.81; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.81; acc: 0.78
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.81
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.9; acc: 0.77
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.92
Batch: 420; loss: 1.04; acc: 0.81
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 1.3; acc: 0.78
Batch: 520; loss: 1.25; acc: 0.7
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.62; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 1.15; acc: 0.73
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.79; acc: 0.75
Val Epoch over. val_loss: 0.5621245013680428; val_accuracy: 0.8531050955414012 

plots/subspace_training/lenet/2020-01-10 04:26:27/d_dim_300_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 16.06; acc: 0.08
Batch: 20; loss: 1.72; acc: 0.42
Batch: 40; loss: 0.97; acc: 0.7
Batch: 60; loss: 1.02; acc: 0.64
Batch: 80; loss: 0.84; acc: 0.77
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.8
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.76; acc: 0.77
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.81
Batch: 300; loss: 0.48; acc: 0.84
Batch: 320; loss: 0.87; acc: 0.73
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.77; acc: 0.81
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.77
Batch: 440; loss: 0.73; acc: 0.8
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.46; acc: 0.83
Batch: 520; loss: 0.47; acc: 0.8
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.8
Batch: 620; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.88; train_accuracy: 0.76 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.77
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.78; acc: 0.77
Val Epoch over. val_loss: 0.5502200422772936; val_accuracy: 0.8420581210191083 

Epoch 2 start
Batch: 0; loss: 0.72; acc: 0.83
Batch: 20; loss: 1.15; acc: 0.8
Batch: 40; loss: 0.44; acc: 0.81
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.98; acc: 0.77
Batch: 180; loss: 0.82; acc: 0.78
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.83
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.8
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.85 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.94
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 0.4768991127230559; val_accuracy: 0.8643511146496815 

Epoch 3 start
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.78
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.7; acc: 0.8
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.86
Batch: 320; loss: 0.74; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.78
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.9; acc: 0.81
Batch: 620; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.47; train_accuracy: 0.87 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.91
Batch: 140; loss: 0.72; acc: 0.83
Val Epoch over. val_loss: 0.4482951974792845; val_accuracy: 0.8771894904458599 

Epoch 4 start
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.8; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.63; acc: 0.8
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.83
Batch: 460; loss: 0.79; acc: 0.83
Batch: 480; loss: 1.14; acc: 0.69
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.72; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.92
Batch: 140; loss: 0.86; acc: 0.77
Val Epoch over. val_loss: 0.44283872059765894; val_accuracy: 0.876890923566879 

Epoch 5 start
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.87; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.9; acc: 0.8
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.92; acc: 0.81
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 1.06; acc: 0.73
Val Epoch over. val_loss: 0.42860643703276946; val_accuracy: 0.8805732484076433 

Epoch 6 start
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.7; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.92
Batch: 140; loss: 0.96; acc: 0.75
Val Epoch over. val_loss: 0.39144148392851946; val_accuracy: 0.8902269108280255 

Epoch 7 start
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.81
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.59; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.9; acc: 0.77
Val Epoch over. val_loss: 0.4123326960927362; val_accuracy: 0.8852507961783439 

Epoch 8 start
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.68; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.83
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.57; acc: 0.92
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.4144193073556681; val_accuracy: 0.8812699044585988 

Epoch 9 start
Batch: 0; loss: 0.6; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.66; acc: 0.86
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.73; acc: 0.77
Batch: 620; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 1.05; acc: 0.81
Val Epoch over. val_loss: 0.447294903788597; val_accuracy: 0.8785828025477707 

Epoch 10 start
Batch: 0; loss: 0.82; acc: 0.83
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.64; acc: 0.81
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.61; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.72; acc: 0.81
Val Epoch over. val_loss: 0.44082068248539213; val_accuracy: 0.8770899681528662 

Epoch 11 start
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.73; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.69; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.88; acc: 0.8
Batch: 500; loss: 0.63; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.6; acc: 0.91
Batch: 140; loss: 0.72; acc: 0.83
Val Epoch over. val_loss: 0.47840365193262224; val_accuracy: 0.8751990445859873 

Epoch 12 start
Batch: 0; loss: 0.54; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.83
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.88
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.83
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.89
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.56; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.81
Val Epoch over. val_loss: 0.3945044418618937; val_accuracy: 0.8909235668789809 

Epoch 13 start
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.71; acc: 0.86
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.77; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.83; acc: 0.83
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.91
Batch: 140; loss: 0.72; acc: 0.83
Val Epoch over. val_loss: 0.4011014131413903; val_accuracy: 0.8899283439490446 

Epoch 14 start
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.86
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.51; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.91; acc: 0.83
Val Epoch over. val_loss: 0.3816425835915432; val_accuracy: 0.8947054140127388 

Epoch 15 start
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.75; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.94
Batch: 140; loss: 0.81; acc: 0.8
Val Epoch over. val_loss: 0.42268149478799977; val_accuracy: 0.8824641719745223 

Epoch 16 start
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.07; acc: 0.95
Batch: 320; loss: 0.54; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.92
Batch: 620; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 1.03; acc: 0.81
Val Epoch over. val_loss: 0.4193186572023258; val_accuracy: 0.8893312101910829 

Epoch 17 start
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.81
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.76; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.85; acc: 0.83
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 0.37403996113189464; val_accuracy: 0.8990843949044586 

Epoch 18 start
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.74; acc: 0.78
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.88; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 1.21; acc: 0.81
Val Epoch over. val_loss: 0.3887911746456365; val_accuracy: 0.8965963375796179 

Epoch 19 start
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.65; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.65; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.75; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.85; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.91
Batch: 140; loss: 0.96; acc: 0.83
Val Epoch over. val_loss: 0.38442620169964564; val_accuracy: 0.8980891719745223 

Epoch 20 start
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.73; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.91 

Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.89
Batch: 140; loss: 0.86; acc: 0.83
Val Epoch over. val_loss: 0.39909580686859264; val_accuracy: 0.8943073248407644 

Epoch 21 start
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.96; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.63; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.98; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.43; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.64; acc: 0.88
Batch: 120; loss: 0.62; acc: 0.91
Batch: 140; loss: 0.74; acc: 0.83
Val Epoch over. val_loss: 0.3903468130690277; val_accuracy: 0.9014729299363057 

Epoch 22 start
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.54; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.91
Batch: 140; loss: 0.83; acc: 0.78
Val Epoch over. val_loss: 0.38912182718895044; val_accuracy: 0.9000796178343949 

Epoch 23 start
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.94
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.61; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 1.0; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 1.13; acc: 0.83
Val Epoch over. val_loss: 0.40500000081244547; val_accuracy: 0.8940087579617835 

Epoch 24 start
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.65; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.62; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.86
Batch: 620; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.53; acc: 0.92
Batch: 140; loss: 1.15; acc: 0.8
Val Epoch over. val_loss: 0.4399223142084043; val_accuracy: 0.8777866242038217 

Epoch 25 start
Batch: 0; loss: 0.73; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.74; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.94
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.77; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.93; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.98; acc: 0.78
Val Epoch over. val_loss: 0.3823339903050927; val_accuracy: 0.8937101910828026 

Epoch 26 start
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.61; acc: 0.86
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.67; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.75; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.94
Batch: 140; loss: 0.93; acc: 0.83
Val Epoch over. val_loss: 0.338577370782187; val_accuracy: 0.9069466560509554 

Epoch 27 start
Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.6; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.95
Batch: 160; loss: 0.43; acc: 0.94
Batch: 180; loss: 0.5; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.97
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.94
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 0.36766050732249667; val_accuracy: 0.9019705414012739 

Epoch 28 start
Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.6; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.95; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.91; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 1.22; acc: 0.77
Val Epoch over. val_loss: 0.4168680651932006; val_accuracy: 0.8877388535031847 

Epoch 29 start
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.61; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.84
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.86; acc: 0.81
Val Epoch over. val_loss: 0.3615987294228973; val_accuracy: 0.8999800955414012 

Epoch 30 start
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.88; acc: 0.8
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 1.03; acc: 0.8
Val Epoch over. val_loss: 0.36710936884591533; val_accuracy: 0.9031648089171974 

plots/subspace_training/lenet/2020-01-10 04:26:27/d_dim_400_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 17.36; acc: 0.08
Batch: 20; loss: 1.89; acc: 0.44
Batch: 40; loss: 1.29; acc: 0.53
Batch: 60; loss: 1.53; acc: 0.59
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.77; acc: 0.72
Batch: 120; loss: 0.86; acc: 0.7
Batch: 140; loss: 0.79; acc: 0.75
Batch: 160; loss: 0.65; acc: 0.78
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.53; acc: 0.81
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.56; acc: 0.77
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.7; acc: 0.77
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.81
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.73
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.6; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.84; train_accuracy: 0.76 

Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 1.07; acc: 0.7
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.75
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 1.04; acc: 0.7
Val Epoch over. val_loss: 0.5850270279463688; val_accuracy: 0.8244426751592356 

Epoch 2 start
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.62; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.78; acc: 0.83
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.81
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.99; acc: 0.75
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.77; acc: 0.77
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 1.04; acc: 0.67
Val Epoch over. val_loss: 0.46676381069953277; val_accuracy: 0.8638535031847133 

Epoch 3 start
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.86
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.81
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.54; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 1.21; acc: 0.73
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.88; acc: 0.77
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 1.0; acc: 0.77
Val Epoch over. val_loss: 0.42147397994995117; val_accuracy: 0.8840565286624203 

Epoch 4 start
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.6; acc: 0.78
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.94; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.92; acc: 0.77
Val Epoch over. val_loss: 0.4442470812114181; val_accuracy: 0.8750995222929936 

Epoch 5 start
Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.87; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.71; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.81
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.77; acc: 0.81
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.24; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.88
Batch: 140; loss: 1.33; acc: 0.7
Val Epoch over. val_loss: 0.49306915994662387; val_accuracy: 0.857484076433121 

Epoch 6 start
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.94
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.8
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.83; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 0.43173620377661315; val_accuracy: 0.881468949044586 

Epoch 7 start
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.47; acc: 0.81
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.92; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.95; acc: 0.75
Val Epoch over. val_loss: 0.4335471540214909; val_accuracy: 0.884156050955414 

Epoch 8 start
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.73; acc: 0.8
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.7; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 1.02; acc: 0.78
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.84
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 1.02; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.76; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.79; acc: 0.77
Val Epoch over. val_loss: 0.43769917352374194; val_accuracy: 0.8788813694267515 

Epoch 9 start
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.84
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.67; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.65; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.2; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.88; acc: 0.78
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.83; acc: 0.72
Val Epoch over. val_loss: 0.4017008330411972; val_accuracy: 0.8905254777070064 

Epoch 10 start
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.63; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.62; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.24; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.79; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.82; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.81; acc: 0.8
Val Epoch over. val_loss: 0.37690987105771995; val_accuracy: 0.8948049363057324 

Epoch 11 start
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.73; acc: 0.83
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.73; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.67; acc: 0.81
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.84
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.96; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.79; acc: 0.78
Val Epoch over. val_loss: 0.4499825052679724; val_accuracy: 0.8821656050955414 

Epoch 12 start
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.66; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.76; acc: 0.86
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.81; acc: 0.75
Val Epoch over. val_loss: 0.40249242192241036; val_accuracy: 0.8904259554140127 

Epoch 13 start
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.74; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.67; acc: 0.78
Val Epoch over. val_loss: 0.41484158851538494; val_accuracy: 0.8908240445859873 

Epoch 14 start
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.86
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 1.05; acc: 0.83
Val Epoch over. val_loss: 0.3870233830752646; val_accuracy: 0.8960987261146497 

Epoch 15 start
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.64; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.84
Batch: 220; loss: 0.48; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.8
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.71; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.59; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.65; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.91
Batch: 140; loss: 0.86; acc: 0.77
Val Epoch over. val_loss: 0.37690462433039; val_accuracy: 0.9023686305732485 

Epoch 16 start
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.75; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.88
Batch: 380; loss: 0.83; acc: 0.78
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.63; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.81; acc: 0.77
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 1.04; acc: 0.8
Val Epoch over. val_loss: 0.45832317502825126; val_accuracy: 0.880672770700637 

Epoch 17 start
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.57; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.88
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.39808564887020237; val_accuracy: 0.894406847133758 

Epoch 18 start
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.88; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.76; acc: 0.81
Val Epoch over. val_loss: 0.39893385649296886; val_accuracy: 0.892515923566879 

Epoch 19 start
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.73; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.71; acc: 0.81
Val Epoch over. val_loss: 0.38129021891742754; val_accuracy: 0.8987858280254777 

Epoch 20 start
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.87; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.61; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.7; acc: 0.88
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.84; acc: 0.83
Val Epoch over. val_loss: 0.39612005515747767; val_accuracy: 0.8927149681528662 

Epoch 21 start
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.7; acc: 0.84
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.91
Batch: 620; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.88; acc: 0.81
Val Epoch over. val_loss: 0.40151908922537116; val_accuracy: 0.8958996815286624 

Epoch 22 start
Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.83; acc: 0.8
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 1.12; acc: 0.77
Val Epoch over. val_loss: 0.4249310095788567; val_accuracy: 0.8813694267515924 

Epoch 23 start
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.89
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.81; acc: 0.81
Val Epoch over. val_loss: 0.383995438220015; val_accuracy: 0.9031648089171974 

Epoch 24 start
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.65; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.95; acc: 0.77
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.73; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 0.85; acc: 0.8
Val Epoch over. val_loss: 0.3775627344467078; val_accuracy: 0.8960987261146497 

Epoch 25 start
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.63; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.48; acc: 0.83
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.65; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.62; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.84
Val Epoch over. val_loss: 0.3751821396457162; val_accuracy: 0.8965963375796179 

Epoch 26 start
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.82; acc: 0.83
Val Epoch over. val_loss: 0.3824979692792437; val_accuracy: 0.8959992038216561 

Epoch 27 start
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.73; acc: 0.84
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.51; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.94; acc: 0.83
Val Epoch over. val_loss: 0.3685708814508216; val_accuracy: 0.9021695859872612 

Epoch 28 start
Batch: 0; loss: 0.62; acc: 0.88
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.35; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.74; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.63; acc: 0.83
Val Epoch over. val_loss: 0.3716942493323308; val_accuracy: 0.8946058917197452 

Epoch 29 start
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.88
Batch: 100; loss: 0.87; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.92; acc: 0.81
Val Epoch over. val_loss: 0.4520875707173803; val_accuracy: 0.8815684713375797 

Epoch 30 start
Batch: 0; loss: 0.82; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.83
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.58; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 1.04; acc: 0.81
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.48; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.81; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.66; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.89; acc: 0.8
Val Epoch over. val_loss: 0.3506892678464294; val_accuracy: 0.9071457006369427 

plots/subspace_training/lenet/2020-01-10 04:26:27/d_dim_500_lr_0.1_seed_1_epochs_30_batchsize_64
plots/subspace_training/lenet/2020-01-10 04:26:27/d_dim_XXXXX_lr_0.1_seed_1_epochs_30_batchsize_64
